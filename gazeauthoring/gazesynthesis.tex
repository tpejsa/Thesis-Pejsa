Given a gaze behavior specified as a sequence of instances, the gaze synthesis component produces a gaze motion and adds it to the character's body motion. To synthesize plausible gaze shift movements toward each target, we adapt a procedural gaze controller~\cite{pejsa2015gaze}. The controller is driven by neurophysiology-derived kinematic laws and has been empirically shown to produce natural-looking gaze shifts, while affording significant parametric control over their kinematic properties---target position, head and torso posture, and velocity. In this section, we give an overview of gaze shift kinematics and our modifications required to support highly varied body motions. We also describe a blending scheme for combining the pose output by the controller with the original body motion, that helps preserve the expressive motion content and ensure smooth transitions between constrained gaze (gaze instances) and unconstrained gaze (gaps between instances).
%The original controller is designed to synthesize gaze shifts toward targets that are relatively static to the character, while our gaze authoring approach must support scenarios involving locomotion and large shifts in stance. We modify the controller's kinematics computation to synthesize correct gaze shift movements in such scenarios. Furthermore, we describe a blending scheme for combining the posture output by the gaze controller with the original body motion, that helps preserve the expressive motion content and ensure smooth transitions between constrained gaze (gaze instances) and unconstrained gaze (gaps between instances).

\subsection{Synthesis of Gaze Kinematics}

The gaze controller is a feed-forward system generating rotational motion of the eyes, head, and torso joints toward a gaze target $T$. A more detailed description of the controller's operation and equations describing its kinematics are given in~\cite{pejsa2015gaze}---here we highlight the key functionality and our extensions.

Kinematics of eye, head, and torso movements are described by peaked velocity profiles parametrized by their peak velocities, $v_{\mathrm{max},E}$, $v_{\mathrm{max},H}$, and $v_{\mathrm{max},T}$. Peak velocities depend linearly on gaze shift amplitude---the further the joints need to rotate to reach the target, the faster they move. In general, peak velocities are computed using equations of the form $v_{\mathrm{max},*} = (a_* D_* + b_*) v_{0,*}$, where $*$ denotes the body part (eye, head, or torso), while $a_*$, $b_*$, and $v_{0,*}$ are constants specific to the body part (values given in ~\cite{pejsa2015gaze}). $D_*$ is the body part's rotational amplitude, which brings it into alignment with the target. Two other important kinematic properties are head and torso latency times, $\tau_H$ and $\tau_T$. The head and torso usually do not begin moving toward the target at the same time as the eyes, but lag behind by their respective latency. Latencies also linearly depend on gaze shift amplitude: $\tau_* = c_* D_* +  + d_*$, where $c_*$ and $d_*$ are per-body-part constants.

To synthesize the pose at a given frame $f$, at which there is an active gaze instance $G = (f_s, f_x, f_e, T, \alpha_{H}, \alpha_{T})$ ($f_s \leq f \leq f_e$), we invoke the gaze controller and supply it the gaze target position, $\mathbf{p}_T$, and head and torso alignments, $\alpha_H$ and $\alpha_T$. $\mathbf{p}_T$ determines the direction in which the body parts will rotate, while the alignment parameters determine \emph{how much} the head and torso contribute to the overall movement (as described in Section~\ref{sec:GazeAlignmentInference}). From these parameters, the controller computes the target posture needed to bring each body part into alignment with the given target. Let $\mathbf{q}_H^s$ and $\mathbf{q}_T^s$ be the orientations of the topmost head and torso joints at the start of the gaze shift. The controller computes the target posture as orientations $\mathbf{q}_H^x$ and $\mathbf{q}_T^x$. From these orientations it computes the rotational amplitudes: $D_* = \angle(\mathbf{q}_*^s, \mathbf{q}_*^x)$, which it uses to calculate peak velocities and latencies. Having computed the required kinematic parameters, the controller synthesizes the gaze shift as rotational movements from $\mathbf{q}_*^s$ to $\mathbf{q}_*^x$, as described in~\cite{pejsa2015gaze}.

\noindent\textbf{Root Movement Adaptation} -- Since key kinematic parameters (velocities and latencies) are computed from amplitude, kinematics of the gaze shift very much depend on the target position relative to the character. When adding a gaze shift to a character that is moving relative to the target (e.g., walking and turning), we must compute these parameters based on the target's projected position at the \emph{end} of the gaze shift, otherwise we will get implausible gaze shift kinematics. Our solution is to supply the gaze controller with an additional parameter $\Delta \mathbf{p}_T$---the \emph{relative target translation} due to root movement over the course of the gaze shift. We apply this translational offset to the target position: $\mathbf{p}_T' + \mathbf{p}_T + \Delta \mathbf{p}_T$. The adjusted target position $\mathbf{p}_T'$ is used for computing initial kinematic parameters---amplitudes, peak velocities, and latencies---resulting in more plausible kinematics.

Computing $\Delta \mathbf{p}_T$ is straightforward. Let $\mathbf{p}_{R,0}$ and $\mathbf{q}_{R,0}$ be the world-space position and orientation of the character's root at the start of the gaze shift (frame $f_s$). Let $\mathbf{p}_{R,1}$ and $\mathbf{q}_{R,1}$ be the root position and orientation at the end of the gaze shift (frame $f_x$). Finally, let $\mathbf{p}_T$ be the gaze target position. Relative target translation is then: $\Delta \mathbf{p}_T = \mathbf{p}_{R,0} + (\mathbf{q}_{R,1}^{-1} \mathbf{q}_{R,0}) (\mathbf{p}_T - \mathbf{p}_{R,1}) - \mathbf{p}_T$.

\subsection{Gaze Motion Blending}
\label{sec:GazeMotionBlending}

The gaze controller outputs at each frame $f$ a head and torso posture as a set of joint orientations, $\mathbf{q}_j^f$, where $j$ is the joint index. We blend this posture with the character's current pose (encoded in the original motion), $\mathbf{q}_{0,j}^f$. Thus we get the blended pose: $\mathbf{q}_{\mathrm{blend},j}^f = \mathop{slerp}(\mathbf{q}_{0,j}^f, \mathbf{q}_j^f, w^f)$. To compute the blend weight, $w^f$, we devised a scheme that achieves two properties: (1) it tries to preserve as much of the original body motion as possible, and (2) ensures smooth transitions from unconstrained gaze to constrained gaze and vice versa. Therefore, the blend weight has two components: $w^f = w_1^f w_2^f$.

We compute the first component using the cosine of the angle between the original and gaze orientation at the current frame: $w_1^f = 1 - \cos \phi^f$, where $\phi^f = \angle(\mathbf{q}_{0,j}^f, \mathbf{q}_j^f)$. We clamp this value to 1 for $\phi^f > 90 \deg$. If a gaze instance is unedited, then the output gaze direction from the controller will be the same as in the original motion, the blend weight will be close to 0, and the final pose will be identical the original pose. On the other hand, if we change the gaze target for the instance, gaze controller output will diverge from the original motion and its influence on the final pose will increase, allowing the character's gaze to reach the new target. This scheme allows us to automatically control the trade-off between using procedurally synthesis and the more expressive original motion data. Alternatively, we also allow the animator to manually fix or keyframe the value of $w_1$ and thus exercise more control over the result.

The second blend weight component, $w_2^f$, ensures smooth transition at the boundaries between constrained and unconstrained gaze. Gaze instance specification can contain gaps between two adjacent instances. During these gaps no gaze is synthesized and the original motion is unchanged. When we have a gaze instance, $G = (f_s, f_x, f_e, \ldots)$ following such a gap, we need to smoothly blend in gaze controller output, like so: $w_2^f = -2 t^3 + 3 t^2$, where $t = (f - f_s)/(f_e - f_s)$. Similarly, we employ the inverse scheme to blend out of a gaze instance preceding a gap.

The blended pose $\mathbf{q}_{\mathrm{blend},j}^f$ is also the final pose, unless the body part is the torso and there are active end-effector constraints. If torso movements introduced by the controller violate the constraints, we correct the torso pose using an IK solver adapted from~\cite{shin2001puppetry}. The solver ensures the end-effector constraints are satisfied while minimally changing the input pose.