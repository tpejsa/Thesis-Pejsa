Given a gaze behavior specified as a sequence of instances, the gaze synthesis component produces a gaze motion and adds it to the character's body motion. To synthesize plausible gaze shift movements toward each target, we adapt the gaze shift synthesis model described in Chapter~\ref{cha:GazeShiftModel}. In this section, I give an overview of the modifications to the model required to support highly varied body motions. I also describe a blending scheme for combining the pose output by the model with the original body motion, that helps preserve the expressive motion content and ensure smooth transitions between constrained gaze (gaze instances) and unconstrained gaze (gaps between instances).

\subsection{Synthesis of Gaze Kinematics}

The gaze shift model is implemented as a feed-forward system generating rotational motion of the eyes, head, and torso joints toward a gaze target $T$. Kinematics of the eye, head, and torso movements are described by velocity profiles parametrized by their peak velocities, $v_{\mathrm{max},E}$, $v_{\mathrm{max},H}$, and $v_{\mathrm{max},T}$. Peak velocities depend linearly on gaze shift amplitude---the further the body parts need to rotate to align with the target, the faster they move. Two other important kinematic properties are the head and torso latency times, $\tau_H$ and $\tau_T$. The head and torso usually do not begin moving toward the target at the same time as the eyes, but lag behind by their respective latency. Latencies also linearly depend on gaze shift amplitude.

To synthesize the pose at a given frame $f$, at which there is an active gaze instance $G = (f_s, f_x, f_e, T, \alpha_{H}, \alpha_{T})$ ($f_s \leq f \leq f_e$), we invoke the gaze shift model and supply it the gaze target position, $\mathbf{p}_T$, and head and torso alignments, $\alpha_H$ and $\alpha_T$. $\mathbf{p}_T$ determines the direction in which the body parts will rotate, while the alignment parameters determine \emph{how much} the head and torso contribute to the overall movement. From these parameters, the model computes for each body part: the \emph{target direction} vector needed to bring the body part into alignment with the given target; the rotational \emph{amplitude} of the gaze shift for that body part; peak \emph{velocity}; body part \emph{latency}. Having computed the required kinematic parameters, the model synthesizes the gaze shift as rotational movements from toward the target.

Since the key kinematic parameters (velocities and latencies) are computed from amplitude, kinematics of the gaze shift very much depend on the target position relative to the character. When adding a gaze shift to a character that is moving relative to the target (e.g., walking and turning), we must compute these parameters based on the target's projected position at the \emph{end} of the gaze shift, otherwise we may get an implausible fast or slow gaze shift. Our solution is to supply the gaze model with an additional parameter $\Delta \mathbf{p}_T$---the \emph{relative target translation} due to root movement over the course of the gaze shift. We apply this translational offset to the target position: $\mathbf{p}_T' + \mathbf{p}_T + \Delta \mathbf{p}_T$. The adjusted target position $\mathbf{p}_T'$ is used for computing the initial kinematic parameters---amplitudes, peak velocities, and latencies---resulting in more plausible kinematics.

Computing $\Delta \mathbf{p}_T$ is straightforward. Let $\mathbf{p}_{R,0}$ and $\mathbf{q}_{R,0}$ be the world-space position and orientation of the character's root at the start of the gaze shift (frame $f_s$). Let $\mathbf{p}_{R,1}$ and $\mathbf{q}_{R,1}$ be the root position and orientation at the end of the gaze shift (frame $f_x$). Finally, let $\mathbf{p}_T$ be the gaze target position. Relative target translation is then:

\begin{align} \label{eq:GazeShiftTargetTrans}
\Delta \mathbf{p}_T = \mathbf{p}_{R,0} + (\mathbf{q}_{R,1}^{-1} \mathbf{q}_{R,0}) (\mathbf{p}_T - \mathbf{p}_{R,1}) - \mathbf{p}_T
\end{align}

\subsection{Gaze Motion Blending}
\label{sec:GazeMotionBlending}

The gaze shift model outputs at each frame $f$ a head and torso pose as a set of joint orientations, $\mathbf{q}_j^f$, where $j$ is the joint index. We blend this pose with the character's current pose (encoded in the original motion), $\mathbf{q}_{0,j}^f$. Thus we get the blended pose:

\begin{align} \label{eq:GazeShiftPoseBlend}
\mathbf{q}_{\mathrm{blend},j}^f = \mathop{slerp}(\mathbf{q}_{0,j}^f, \mathbf{q}_j^f, w^f)
\end{align}

To compute the blend weight, $w^f$, we devised a scheme that achieves two properties: (1) it tries to preserve as much of the original body motion as possible, and (2) it ensures smooth transitions from unconstrained gaze to constrained gaze and vice versa. Therefore, the blend weight has two components: $w^f = w_1^f w_2^f$.

We compute the first component using the cosine of the angle between the original and gaze orientation at the current frame: $w_1^f = 1 - \cos \phi^f$, where $\phi^f = \angle(\mathbf{q}_{0,j}^f, \mathbf{q}_j^f)$. We clamp this value to 1 for $\phi^f > 90 \deg$. If a gaze instance is unedited, then the output gaze direction from the model will be the same as in the original motion, the blend weight will be close to 0, and the final pose will be identical the original pose. On the other hand, if we change the gaze target for the instance, the gaze model output will diverge from the original motion and its influence on the final pose will increase, allowing the character's gaze to reach the new target. This scheme allows us to automatically control the trade-off between using procedurally synthesis and the more expressive original motion data. Alternatively, we also allow the animator to manually fix or keyframe the value of $w_1$ and thus exercise more control over the result.

The second blend weight component, $w_2^f$, ensures smooth transitions at the boundaries between constrained and unconstrained gaze. Gaze instance specification can contain gaps between two adjacent instances. During these gaps no gaze is synthesized and the original motion is unchanged. When we have a gaze instance, $G = (f_s, f_x, f_e, \ldots)$ following such a gap, we need to smoothly blend in the gaze model output, like so: $w_2^f = -2 t^3 + 3 t^2$, where $t = (f - f_s)/(f_e - f_s)$. Similarly, we employ the inverse scheme to blend out of a gaze instance preceding a gap.

The blended pose $\mathbf{q}_{\mathrm{blend},j}^f$ is also the final pose, unless the body part is the torso and there are active end-effector constraints. If torso movements introduced by the gaze shift model violate the constraints, we correct the torso pose using an IK solver adapted from~\citep{shin2001puppetry}. The solver ensures the end-effector constraints are satisfied while minimally changing the input pose. 