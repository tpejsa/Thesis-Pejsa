The current chapter has proposed a novel approach for scalable authoring of gaze animation in non-interactive scenarios The approach is based on the idea of modeling the gaze behavior as a sequence of gaze instances, representing gaze shifts toward targets in the scene. We have shown how this representation can be automatically inferred from the body motion and scene geometry, producing an initial gaze animation that can be refined further through manual editing. We have also described a convenient gaze editing approach which takes advantage of the representation's two key properties: abstraction of gaze pose and timing details, and anchoring the gaze behavior in the scene via target handles. Finally, we have described a method for synthesizing plausible gaze animation from this representation and adding it to the original motion. As shown in the evaluations, the proposed approach can substantially reduce the labor and skill required to author gaze animation compared to traditional tools. Even a novice animator can endow characters with gaze at an acceptable level of quality, and it can also serve as a labor-saving tool for skilled animators by providing them with a first-guess animation they can edit further. In particular, the approach has potential applications in domains where quantity and speed of animation production are important considerations, such as television animation or animation of background and midground characters in film and games.

The gaze authoring approach could be extended in several ways.
For one, the accuracy and sensitivity of our gaze inference model could be further improved by replacing the current set of heuristics with models learned from recorded motion and gaze data.
In addition, the approach currently complements saliency-based methods for idle gaze, such as~\citep{peters2003bottomup}, but such models could be integrated with our own to refine the inference output.
Another property of the gaze inference model is that it produces a relatively conservative estimate of the gaze behavior. While it matches the body motion and scene, it does not have much expressiveness in the form of random saccades and idle gaze. Such aesthetic flourishes can mean the difference between adequate, yet uninteresting and truly lifelike animation, and professional animators spend much time getting them right. While probabilistic models for gaze behavior synthesis (e.g., ~\citep{lee2002eyes}) are no substitute for the skill of human artists, they can nonetheless be integrated with our gaze authoring approach to obtain a richer, more lifelike gaze behavior without the need for manual intervention.
Finally, the purpose of the gaze authoring approach is adding editable gaze animation to non-interactive scenarios. However, the gaze inference model could be adapted to operate on interactive inputs, allowing its use for predicting user attention and intent in virtual reality and other interactive contexts.

The current implementation of the system in Unity serves as an adequate proof of concept, but its utility to artists would be increased by integration with a 3D animation software such as MotionBuilder or Maya. Furthermore, the system uses a feed-forward gaze controller for motion synthesis. Due to its design, the entire motion needs to be synthesized from start to end to obtain the final editing result. To make the system more immediate, alternative motion synthesis methods might need to be explored. While the complexity of the gaze shift model (Chapter~\ref{cha:GazeShiftModel}) precludes a closed-form formulation, a simplified version of the model could be designed that would allow exact pose computation on a per-frame basis.
