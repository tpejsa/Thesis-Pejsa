The current chapter has proposed a novel approach for scalable authoring of gaze animation in non-interactive scenarios. The approach is based on the idea of modeling the gaze behavior as a sequence of gaze instances, representing gaze shifts toward targets in the scene. We have shown how this representation can be automatically inferred from the body motion and scene geometry, producing an initial gaze motion that can be refined further through manual editing. We have also described a convenient gaze editing approach which takes advantage of the representation's two key properties: abstraction of gaze pose and timing details, and anchoring the gaze behavior in the scene via target handles. Finally, we have described how plausible gaze motion can be synthesized from this representation and added to the original motion.

As shown in the evaluations, the proposed approach can substantially reduce the labor and skill required to author gaze animation compared to traditional tools, allowing much better scalability of authoring effort in relation to scenario complexity and scene changes. Even a novice animator can endow characters with gaze at an acceptable level of quality, and it can also serve as a labor-saving tool for skilled animators by providing them with a first-guess animation they can edit further. In particular, the approach has potential applications in domains where quantity and speed of animation production are important considerations, such as television animation or animation of background and midground characters in film and games.

The gaze authoring approach could be extended in several ways.
First, we focus on modeling only directed gaze. Although other human gaze movements such as smooth pursuit, saccades, and conversational aversions are not explicitly simulated in our system, they can be reasonably approximated as directed gaze. Others, such as eye blinks, co-occur with directed gaze and are important for expressiveness.
In the current implementation of our approach, we add probabilistically generated eye blinks and saccades to the synthesis output in a post-processing step in order to achieve more expressive results. Blinks are generated following the model proposed by~\citet{peters2010animating}, and saccadic gaze is based on the Eyes Alive model~\citep{lee2002eyes}.

Second, while our gaze inference approach is sufficient to obtain a plausible gaze behavior that can serve as a starting point for editing, its accuracy and sensitivity could be improved further. Probabilistic formulations used for gaze-event and -target inference can be extended in many ways; for example, heuristic terms can be replaced by priors learned from ground-truth data. Saliency-based methods for idle gaze (e.g.,~\citet{peters2003bottomup}) could be utilized to refine the output of target inference. Recorded eye movements---if available---could be utilized for the same purpose.
Support for tasks that involve object interactions could be enhanced by introducing target inference terms for other significant kinematic events besides hand contact start~\citep{johansson2001eyehead}.
Moreover, the gaze-inference approach need not be restricted to non-interactive scenarios and could be adapted to operate on interactive inputs, allowing its use for predicting user attention and intent in virtual reality and other interactive contexts.

Third, the current editing approach lacks support for specifying whole-body reorientation. Editing whole-body orientation in conjunction with eye gaze is challenging since it requires editing the entire underlying body motion, which involves root movements and complex patterns of end-effector constraints. Existing methods such as motion path editing~\citep{gleicher2001path} may provide a good starting point for implementing such features.

The current implementation of the system in Unity serves as an adequate proof of concept, but its utility to artists would be increased by integration with commonly used 3D-animation software such as MotionBuilder or Maya. Furthermore, the system does not provide truly interactive editing due to the limitations of the synthesis approach. While our gaze shift model (Chapter~\ref{cha:GazeShiftModel}) generates biologically plausible movements, its feed-forward design and parametrization based on relative head and torso alignments necessitate re-synthesis of the entire motion from the beginning to the end to obtain the exact editing result. To achieve interactive performance, alternative methods for motion synthesis must be explored. While the complexity of the gaze shift model precludes a closed-form formulation, a simplified version of the model could be designed that would allow exact pose computation on a per-frame basis.
