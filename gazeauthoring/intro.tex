Directed gaze is an important component of good character animation, as it can situate characters in the scene, signal the focus of their attention, and convey their personality and intent. Directed gaze involves coordinated movements of the eyes, head, and torso. In animation practice, these movements are usually hand-authored, which requires considerable effort and skill due to their intricate spatial and timing relationships. Performance capture is seldom a viable option; even if the capture rig can record eye movements, the recorded gaze typically does not match the virtual scene and needs to be edited by hand.

Chapter~\ref{cha:GazeShiftModel} has introduced methods for synthesis of directed gaze shifts as building blocks of effective gaze behaviors, while Chapter~\ref{cha:StylizedGaze} has introduced motion adaptation methods for gaze shifts, facilitating their application to characters with varied designs.
In the current chapter, the focus is on using directed gaze shifts as primitives for authoring gaze motion in non-interactive scenarios. We introduce an abstract representation of gaze motion as a sequence of \emph{gaze instances}, where each instance specifies a gaze shift toward a target in the scene. This representation serves as the foundation of the authoring process. The workflow in our approach begins with an automatic \emph{inference} of the gaze instance sequence from the body motion and scene layout. The body motion implicitly contains the head and torso components of the actor's gaze, and we can infer a plausible reconstruction of the original gaze behavior by analyzing the motion's kinematic properties. From this sequence, our approach can synthesize the eye movements that match the body motion and scene.

The inferred sequence also serves as a starting point for the \emph{editing} process. The gaze instance representation has two key properties that enable convenient editing. First, it abstracts the details of gaze shift timing and pose. An animator can specify \emph{when} and \emph{where} to look by adding, removing, and modifying gaze instances, while the kinematics of eye, head, and body movements are synthesized automatically. Second, the model anchors the gaze motion in the virtual scene for easy spatial editing. Gaze targets serve as editing handles attached to objects and characters. When animators manipulate these handles, synthesized gaze motion automatically adapts to the new spatial constraints. We provide a graphical tool which exposes the operations for editing gaze instances and targets. The inferred and (potentially) edited gaze motion is \emph{synthesized} automatically by a procedural, neurophysiology-based gaze controller, the output of which is blended with the original body motion.

In the remainder of this chapter, I describe the gaze \textit{inference} approach (Section~\ref{sec:GazeInference}), which infers an editable representation of the character's gaze behavior from motion capture data and scene properties. This is followed by an explanation of methods for \emph{synthesis} of gaze motion from this representation (Section~\ref{sec:GazeSynthesis}). Next, I describe how gaze motion \emph{editing} is performed with respect to the representation (Section~\ref{sec:GazeEditing}). Finally, I conclude with an evaluation of the approach with respect to authoring cost, as well as quality and communicative effectiveness of authored animation (Section~\ref{sec:GazeAuthoringEvaluation})\footnote{The research presented in this chapter was conducted in collaboration with Daniel Rakita. This chapter will be published in~\citet{pejsa2016authoring}.}. 