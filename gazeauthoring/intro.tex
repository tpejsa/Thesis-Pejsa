Directed gaze is the intentional movement of a character's line of sight toward targets in space. It is an important component of good character animation, as it can situate characters in the scene, signal the focus of their attention, and convey their personality and intent. Directed gaze involves coordinated movements of the eyes, head, and torso. In animation practice, these movements are typically hand-authored, which requires considerable effort and skill due to their intricate spatial and timing relationships. Performance capture is seldom a viable option, and still requires editing; while some performance capture rigs may record eye movements, the quality of the data is seldom adequate for animation.
% these often do not match the virtual scene or narrative requirements, requiring a professional animator to edit the animation or author it from scratch.

% tomislav's version - which mike replaced, but then it somehow came back
%Our goal is to make the animator's work easier by (1) automatically adding directed gaze animation to a given body motion and scene, and (2) providing convenient controls for editing the gaze animation. We introduce a \emph{model-based} approach to gaze animation authoring, with an abstracted, conveniently editable representation of directed gaze utilized throughout the authoring process. We represent gaze as a sequence of \emph{gaze instances}, where each instance specifies a gaze shift toward a target in the scene. From this model, we can automatically synthesize biologically plausible gaze movements and add them to the body motion. Moreover, this model has two key properties that enable expedient editing. First, it abstracts kinematic details of eye, head, and body movements during gaze shifts, which are synthesized automatically by a neurophysiology-based gaze controller. Rather than worry about the intricacies of timing and pose, the animator only needs to specify \emph{when} and \emph{where} the character should look. Second, it anchors the gaze animation in the virtual scene for easy spatial editing. Gaze targets serve as editing handles attached to virtual objects and characters; animators can move these around and synthesized gaze animation automatically adapts to new spatial constraints.
%We introduce a novel approach for adding editable directed gaze animation to scenes animated using motion capture data. Given a captured body motion and virtual scene, our approach can automatically infer a description of the character's gaze behavior and synthesize corresponding eye animation. The approach also allows easy manual editing of the gaze behavior to correct errors in the performance or alter the character's personality and intent.
% Mike's version - 2 paragraphs
In this paper, we introduce an approach for adding directed gaze movements to a captured body motion. We propose a \emph{model-based} approach to gaze authoring, where an abstracted representation of gaze movements is used throughout the authoring process. Specifically, we represent gaze behavior as a sequence of \emph{gaze instances}, where each instance specifies a gaze shift toward a target in the scene. Our approach uses this representation to: (1) automatically \emph{infer} plausible gaze that matches the given body motion and scene layout, thus providing a starting point for editing; (2) afford convenient \emph{editing} controls allowing an animator to easily refine the gaze behavior; and (3) \emph{synthesize} neurophysiologically plausible gaze animation that expresses the author's intent. Two key properties of our model enable convenient editing. First, it abstracts details of gaze shift timing and pose, allowing an animator to specify \emph{when} and \emph{where} to look, while kinematics of eye, head, and body movements are synthesized automatically by a neurophysiology-based gaze controller. Second, it anchors the gaze animation in the virtual scene for easy spatial editing. Gaze targets serve as editing handles attached to virtual objects and characters; animators can move these around and synthesized gaze animation automatically adapts to new spatial constraints.%Timing can be adjusted with similar interactions.

Our main contribution is a comprehensive approach for gaze animation authoring based on a model of directed gaze instances. The approach involves the following specific contributions:

\begin{itemize}
\item Methods for automatic \emph{inference} of a virtual character's gaze movements from motion capture data and scene properties.
\item Methods and a graphical tool for \emph{editing} of the behavior by modifying the gaze instances and scene layout.
\item Methods for \emph{synthesis} of neurophysiologically plausible gaze animation from a descriptive set of gaze instances, and its layering onto the character's body motion.
\end{itemize}

\noindent\textbf{Approach Overview} -- Our work focuses on \emph{directed gaze}, which we model as a temporal sequence of gaze shifts and fixations toward target locations in the scene. Each gaze shift is a coordinated movement of the eyes, head, and torso toward the target, followed by gaze fixation on that target. We refer to every such gaze shift-fixation pair as a \emph{gaze instance} specifying the gaze shift start time, target location, and head and torso posture relative to the target. Given a sequence of gaze instances, our system synthesizes the corresponding gaze movements and applies them to the body motion. A gaze instance sequence can also contain gaps, during which no gaze motion is applied to the character---we refer to such gaps as \emph{unconstrained gaze}.
Directed gaze does not include all human gaze movements; e.g., smooth pursuit, saccades, and conversational aversions are not explicitly modeled. Other gaze types are approximated with our model or can be modeled separately.
%However, many of these movements can still be approximated as directed gaze, such as the numerous gaze aversions in the ChatWithFriend scene (Section~\ref{sec:GazeEditingResults}).

Our workflow has three main components: (1) inference, (2) synthesis, and (3) editing. The gaze inference model (Section~\ref{sec:GazeInference}) analyzes the captured body motion and the virtual scene, and it produces a set of editable gaze instances. The idea of gaze inference from body motion is that the motion implicitly contains the head and torso components of the actor's gaze, so by analyzing its kinematic properties we can infer an approximate reconstruction of the original gaze behavior. This reconstruction can be used to automatically add eye animation to the original motion. Moreover, it serves as a starting point for manual editing (Section~\ref{sec:GazeEditing}). We provide a graphical tool allowing animators to hand-edit the gaze instances, e.g., by changing the spatial layout of gaze targets, adjusting gaze instance timings, fine-tuning head and torso postures, and adding or removing gaze instances. Given an inferred and (potentially) edited set of gaze instances, the synthesis component (Section~\ref{sec:GazeSynthesis}) produces the final gaze animation. 