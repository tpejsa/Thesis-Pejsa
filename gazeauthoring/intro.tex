Directed gaze is an important component of good character animation, as it can situate characters in the scene, signal the focus of their attention, and convey their personality and intent. Directed gaze involves coordinated movements of the eyes, head, and body. In animation practice, these movements are typically hand-authored, which requires considerable effort and skill due to their intricate spatial and timing relationships. Performance capture is seldom a viable option for obtaining high-quality gaze animation: while some performance capture rigs may record eye movements, the recorded gaze seldom matches the virtual scene and needs to be edited by hand or authored from scratch.

Chapter~\ref{cha:GazeShiftModel} has introduced methods for synthesis of directed gaze shifts as building blocks of effective gaze behaviors, while Chapter~\ref{cha:StylizedGaze} has introduced motion adaptation methods for gaze shifts, facilitating their application to characters with varied designs. The latter approach makes the process of creating character animation more scalable from the standpoint of character design, as it removes the need to re-author or re-implement gaze behaviors for characters that depart from realistic, humanlike design. In the current chapter, I focus on a different dimension of scalability---making the creation of gaze animation more scalable with respect to the scenario. The problem tackled this chapter can be stated as follows: given a non-interactive scenario, consisting of a virtual scene (a set of 3D objects) with one or more virtual characters engaging in activities encoded in their captured body motion, can we automatically add plausible gaze animation to the characters and facilitate its convenient editing by novices?

To solve this problem, we introduce an approach for adding directed gaze movements to characters animated using full-body motion capture, which uses an abstracted representation of gaze movements as its foundation. Specifically, we represent the gaze behavior as a sequence of \emph{gaze instances}, where each instance specifies a gaze shift toward a target in the scene. Our approach uses this representation to: (1) automatically \emph{infer} plausible gaze that matches the given body motion and scene layout, thus providing a starting point for editing; (2) afford convenient \emph{editing} controls allowing an animator to easily refine the gaze behavior; and (3) \emph{synthesize} biologically plausible gaze animation that expresses the author's intent. Two key properties of our model enable convenient editing. First, it abstracts details of gaze shift timing and pose, allowing an animator to specify \emph{when} and \emph{where} to look, while kinematics of eye, head, and body movements are synthesized automatically by the gaze shift model introduced in Chapter~\ref{cha:GazeShiftModel}. Second, it anchors the gaze animation in the virtual scene for easy spatial editing. Gaze targets serve as editing handles attached to virtual objects and characters; animators can move these around and synthesized gaze animation automatically adapts to the new spatial constraints.

\subsection{Approach Overview}

In the proposed approach, we model the gaze behavior as a temporal sequence of gaze shifts and fixations toward target locations in the scene. Each gaze shift is a coordinated movement of the eyes, head, and torso toward the target, followed by a gaze fixation of that target. We refer to every such gaze shift-fixation pair as a \emph{gaze instance} specifying the gaze shift start time, target location, and head and torso alignment with the target. Given a sequence of gaze instances, our system synthesizes the corresponding gaze movements and applies them to the body motion. A gaze instance sequence can also contain gaps, during which no gaze motion is applied to the character---we refer to such gaps as \emph{unconstrained gaze}.
While directed gaze does not include all human gaze movements---e.g., smooth pursuit, saccades, and conversational aversions---many are well-approximated, e.g., the numerous gaze aversions in the ChatWithFriend scene (Section~\ref{sec:GazeEditingResults}).

Our workflow has three main components: (1) inference, (2) synthesis, and (3) editing. The gaze inference model (Section~\ref{sec:GazeInference}) analyzes the captured body motion and the virtual scene, and it produces a set of editable gaze instances. The idea of gaze inference from body motion is that the motion implicitly contains the head and torso components of the actor's gaze, so by analyzing its kinematic properties we can infer an approximate reconstruction of the original gaze behavior. This reconstruction can be used to automatically add eye animation to the original motion. Moreover, it serves as a starting point for manual editing (Section~\ref{sec:GazeEditing}). We provide a graphical tool allowing animators to hand-edit the gaze instances, e.g., by changing the spatial layout of gaze targets, adjusting gaze instance timings, fine-tuning head and torso poses, and adding or removing gaze instances. Given an inferred and (potentially) edited set of gaze instances, the synthesis component (Section~\ref{sec:GazeSynthesis}) produces the final gaze animation.