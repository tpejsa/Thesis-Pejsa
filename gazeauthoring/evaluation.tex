The goal of this work is to reduce the effort required to produce high-quality gaze animation modeling the gaze behavior as an automatically inferrable, easily editable sequence of gaze instances.
We conducted a three-part evaluation of our approach. The first part assessed the amount of effort required to author gaze animation to confirm that our approach saves animator time and effort. The second part assessed whether this reduction in effort came at the expense of animation quality. The third part evaluated the effects of gaze editing on viewers' understanding of the scenario to show that the our approach allows animators to tell a new story through changes in the characters' attending behaviors.

\subsection{Authoring Effort}
\label{sec:GazeAuthoringEffortEvaluation}

We evaluated authoring effort in two ways. First, we investigated the effort required to add eye animation to a captured body motion without modifying the motion itself, which our tool can do automatically. We asked two experienced animators to add eye movements to seven scenes, originally captured for the evaluation of the gaze inference approach (Section ~\ref{sec:GazeInferenceEvaluation}), with a cumulative length of 2:36 minutes. Both animators used Autodesk MotionBuilder, where they set up a simple eye animation rig that they keyed to create the desired eye movements. We measured authoring effort using two metrics: (1) time taken and (2) number of keys set. We found that the task required on average 25 minutes and 86 keys for each minute of animation. By contrast, our system can synthesize an equivalent amount of eye animation automatically, requiring on average 1.50 minutes of computation per minute of animation. This does not include the time needed to manually label important objects in the scene, which typically takes less than a minute.
We note that the labeling effort scales much better with scene complexity than does specifying the target of each gaze shift directly, because objects are often gazed at multiple times. Moreover, large objects---such as other characters---often have multiple parts that get looked at, and they require multiple target handles as a result.

Second, we assessed the effort required to edit gaze animation in a scene that already contained eye movements. An experienced animator was instructed to make the following edits: (1) introducing gaze aversion in a two-character interaction scene (HandShake), (2) removing gaze toward an object in a scene that involved interaction with the environment (StealGem), (3) introducing gaze toward a new character in a multi-character interaction scene (DrinkSoda), and (4) introducing gaze toward the camera in a scene that involved interaction with the environment (MakeSandwich). All of the edits involved changing not only eye movements, but also head and (in some cases) torso posture. To accomplish the edits, the animator used layering and keyframing features in MotionBuilder. Additionally, a novice animator authored the same edits using our gaze-editing tool. As before, we measured the time taken and number of operations required. For MotionBuilder, the latter is equivalent to the number of keys set, while for our tool it refers to the editing operations discussed in Section~\ref{sec:GazeEditing}. Our measurements, reported in Table~\ref{tab:GazeEditingEffortResults}, indicate that our approach requires 1/3 of the time and number of operations required to achieve equivalent edits using traditional motion editing.

\begin{table}
\small
\centering
\def\arraystretch{1.5}
\begin{tabular}{lrrrr}
\hline
\textbf{Scenario} & \multicolumn{2}{r}{\textbf{Traditional}} & \multicolumn{2}{r}{\textbf{Our Approach}} \\
\cline{2-5}
& Time & \# keys & Time & \# ops  \\
\hline
HandShake & 10:40 & 19 & 3:11 & 8 \\
StealGem & 1:29 & 9 & 1:30 & 4 \\
DrinkSoda & 9:05 & 43 & 3:51 & 9 \\
MakeSandwich & 10:43 & 33 & 4:05 & 13 \\
\hline
\end{tabular}
\caption{Gaze editing effort: traditional vs. our approach.}
\label{tab:GazeEditingEffortResults}
\end{table}

\subsection{Animation Quality}
\label{sec:GazeAnimationQualityEvaluation}

To evaluate the effect of our authoring approach on perceived quality of produced animation, we conducted a human-subjects experiment. We compared our approach to alternative methods for animating character gaze, including no eye animation, eye movements recorded using an eye tracker, and hand-authored gaze. We showed participants pairs of videos of a motion-captured character performing various actions and asked them to choose the one they thought had superior animation. The videos in each pair were identical in every way except the gaze animation method. Different methods corresponded to the following study conditions: (1) \emph{no gaze}, (2) \emph{recorded gaze} (using an eye tracker), (3) \emph{hand-authored gaze}, and (4) \emph{synthesized gaze} (our approach).

\subsubsection{Hypotheses}

We hypothesized that (1) \emph{synthesized gaze} would be preferred over \emph{no gaze}; (2) \emph{synthesized gaze} would be preferred over \emph{recorded gaze}; and (3) \emph{synthesized gaze} would be seen as non-inferior to \emph{hand-authored gaze}.
Hypothesis 2 is motivated by the consideration that raw eye-tracking data is noisy, non-trivial to map to an animated character, and often incorrect with respect to scene requirements. Hypothesis 3 reflects the expectation that our approach can reduce authoring effort without compromising quality.

\subsubsection{Design}

The experiment involved three separate studies, each of which tested one of the hypotheses. Each study consisted of five task trials and followed a within-participants design, wherein the participants were asked to choose between videos in \emph{synthesized gaze} and one of the other conditions. Video pairs were presented to the participants in a randomized order.

\subsubsection{Stimuli}

Experiment stimuli included $5 \times 4$ video clips (five scenes, each in four conditions) of a character animated using motion capture. The animations were 9 to 14 seconds in length. We created the \emph{no gaze} condition videos by extracting segments from original scenes created for the evaluation of our gaze inference method (Section~\ref{sec:GazeInferenceEvaluation}.) The scenes included ChatWithFriend, MakeSandwich, StackBoxes, StealGem, and WalkCones. The videos in the \emph{hand-authored gaze} condition were created by extracting the same segments from the scenes created for our evaluation of authoring effort (Section~\ref{sec:GazeAuthoringEffortEvaluation}). The videos in the \emph{synthesized gaze} condition resulted from applying gaze inference to the segments. To create the \emph{recorded gaze} condition, we generated eye animations directly from eye-tracking data.

\subsubsection{Measures}

Our experiment had the following subjective measures: (1) perceived \emph{animator competence}, (2) \emph{realism}, and (3) \emph{communicative clarity}. The measures were implemented as binary-choice questions presented at each task trial as follows: (1) ``On which video did the animator do a better job?'' (2) ``In which video were the character's movements more realistic?'' (3) ``In which video did the character communicate more clearly what they were doing?''

\subsubsection{Participants}

We conducted the experiment online using Amazon Mechanical Turk crowdsourcing service. We recruited 24 participants for each study---a total of 72 participants. The participants were paid at the rate of \$6/hour.
% TODO: Did you collect any demographics? I think people will at least expect gender breakdown.

\subsubsection{Results}

Upon collecting our data, we conducted separate analyses for each study of the experiment. We aggregated all participants' binary answers across all the scenes and obtained, for each condition, the count of how many times it was chosen over its counterpart in each measure. Then we compared the counts using chi-square tests of goodness-of-fit. The results are described in the following paragraphs and shown in Figure~\ref{fig:StudyResults}. ($^*$) marks significant differences.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{gazeauthoring/Figures/StudyResults.pdf}
\caption{Results of the study comparing gaze animation produced using our approach (synthesized Gaze) to (1) no gaze animation, (2) gaze recorded using an eye tracker, and (3) hand-authored gaze. The y-axis shows the percentage of trials in which one condition was chosen over the other.}
\end{figure}

The analysis of data from study one found a significant preference for \emph{synthesized gaze} over \emph{no gaze} on all measures, $\chi^2(1, 120) = 6.41, p = .011^*$ (animator competence), $\chi^2(1, 120) = 3.99, p = .046^*$ (realism), and $\chi^2(1, 120) = 5.54, p = .019^*$ (communicative clarity). These results support Hypothesis 1.

Data from the second study showed that \emph{synthesized gaze} was not significantly preferred over \emph{recorded gaze} in any of the measures, $\chi^2(1, 120) = 0.83, p = .362$ (animator competence), $\chi^2(1, 120) = 2.12, p = .145$ (realism), and $\chi^2(1, 120) = 0.53, p = .466$ (communicative clarity). These results do not support Hypothesis 2.

In study three, we tested Hypothesis 3 using the Wald test of non-inferiority with the margin $\sigma$ set to 0.07, following guidelines from literature~\citep{ng2001choice}.
\emph{Synthesized gaze} was not seen as non-inferior to \emph{hand-authored gaze} on either animator competence ($\chi^2(1, 120) = 0.000, p = 1.000$), realism ($\chi^2(1, 120) = 0.048, p = .827$), or communicative clarity ($\chi^2(1, 120) = 0.256, p = .513$). These results do not provide support for Hypothesis 3.

In a post-hoc exploratory analysis, we investigated the potential reasons for the lack of support for Hypothesis 2, as the findings contrasted our qualitative observations. This analysis focused on how recorded and synthesized gaze differed across different scenarios in order to determine whether or not our approach had a differential effect on different scenes. We found that scene type had a significant effect on participants' choice of \emph{recorded gaze} in all measures, e.g., $\chi^2(4, 55) = 25.04, p < .0001^*$ for the ``animator competence'' measure. Further inspection showed that scenes with a close-up of the character benefited the most from our synthesized gaze approach, likely due to greater saliency of gaze cues. The differences between approaches were indiscernible in scenes where the character's eyes were small on the screen and other motion components dominated. An illustrative example is the ChatWithFriend scene, which was always chosen in \emph{synthesized gaze} condition over \emph{recorded gaze} in animator competence and realism measures, and 23 times out of 24 in communicative clarity.

Study results suggest that adding eye animation using our approach leads to improvement in the perceived quality of the animation over having no eye animation at all. Our approach also seems to afford benefits over recorded eye motion data in scenes where gaze behavior is perceptually dominant. However, evidence suggests that the use of our approach incurs a small loss of quality compared to expert-crafted animation.

\subsection{Editing Effectiveness}
\label{sec:GazeEditingEffectEvaluation}

We conducted another study with human participants to evaluate the effectiveness of our gaze editing approach. We used our gaze editing tool (Section~\ref{sec:GazeEditing}) to edit the inferred gaze behavior in four scenarios. In the original scenarios, the character spent a substantial portion of time focusing on a scene object or another character. We edited their gaze so they would focus more on another object or character instead. We showed the videos of each scenario to participants and had them estimate the amount of time the character focused on each object. The two versions of the video were identical in every way except the character's gaze behavior. We hypothesized that participants' estimates of the character's focus time would be significantly different in the edited scenario compared to the original scenario.

\subsubsection{Design}

The experiment followed a between-participants design to minimize transfer effects. There were two conditions: \emph{original gaze}, which showed the scenarios with inferred, unedited gaze behavior, and \emph{edited gaze}, which showed the scenarios with edited gaze behavior. In each condition, the participant experienced four trials of the task, one for each scenario. The scenarios were presented to the participants in a randomized order.

\subsubsection{Stimuli}

Experiment stimuli were $4 \times 2$ video clips (four scenarios, each in two conditions) of one or more characters animated using motion capture, 1 to 15 seconds in length. We created the \emph{original gaze} condition videos by extracting segments from the scenarios created for the gaze inference evaluation (Section~\ref{sec:GazeInference}.) The scenarios were: HandShake, MakeSandwich, DrinkSoda, StealGem. We added eye gaze animation to the scenarios using our gaze inference approach. Next, we created the \emph{edited gaze} condition videos by editing the gaze behavior of a character in each scenario. The edits involved introducing additional gaze shifts and fixations of another object or character, which is looked at infrequently or not at all in the original scenario. The following edits were performed:

\begin{enumerate}
\item \emph{HandShake} -- In the original scenario, the green character mostly looks at the blue character. In the edited scenario, we have the green character look down at the floor instead.
\item \emph{MakeSandwich} -- In the original scenario, the character looks either at the tabletop or off to the side. In the edited scenario, we have them sporadically look at the camera.
\item \emph{DrinkSoda} -- In the original scenario, the blue character mostly looks at the green character. In the edited scenario, we have the blue character look at the pink character as well.
\item \emph{StealGem} -- We introduced a second, yellow gem into this scenario for the purposes of the current experiment. In the original scenario, the character only looks down at the red gem. In the edited scenario, the character looks at the yellow gem instead of the red gem.
\end{enumerate}

\subsubsection{Measures}

The experiment had two objective measures per each scenario, for a total of 8 measures. The per-scenario measures were participants' estimates of the time spent by the character focusing on one of two objects/characters in the scene. Each focus time was measured as the percentage of total time in the scenario. The participants specified their estimates on an 11-point scale, with items ranging from 0\% to 100\%.

For example, after viewing the DrinkSoda scenario (Figure~\ref{fig:GazeEditResults}), the participants answered the following two questions (each on an 11-point scale): ``How much was the blue character focusing on the green character?'' and ''How much was the blue character focusing on the pink character?''

\subsubsection{Participants}

We conducted the experiment online using Amazon Mechanical Turk crowdsourcing service. We recruited 24 participants or 12 per condition. The participants were paid at the rate of \$6/hour.

\subsubsection{Results}

We analyzed the data from each of our eight measures using a two-tailed Student's t-test. The results, shown in~\ref{fig:GazeEditEffectResults}, were as follows.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{gazeauthoring/Figures/StudyResults2.pdf}
\caption{Results of the study evaluating the effectiveness of gaze edits performed using our tool. Each chart shows the participants' estimates of one character's focus times on two objects/other characters in the scenario, contrasted among the two conditions: \emph{Original Gaze} (gaze inference output) and \emph{Edited Gaze} (character's gaze edited using our tool.)}
\label{fig:GazeEditEffectResults}
\end{figure}

In the HandShake scenario, there was a significant difference in participants' estimates of focus time on the blue character, $t(13) = -5.10, p = .0002$. Likewise, there was a significant difference in their estimates of focus time on the floor, $t(20) = -5.49, p < .0001$.

In the MakeSandwich scenario, there was no significant difference in participants' estimates of focus time on the tabletop, $t(21) = -0.81, p = .4227$. However, participants in the \emph{edited gaze} condition did give marginally higher estimates for focus time on the camera than those in the \emph{original gaze} condition, $t(22) = 1.94, p = .0065$.

In the DrinkSoda scenario, there was a significant difference in participants' estimates of focus time on the green character, $t(13) = -8.27, p < .0001$, as well as their estimates of focus time on the pink character, $t(14) = 9.46, p < .0001$.

In the StealGem scenario, participants' estimates of focus time on the red gem were marginally lower in the \emph{edited gaze} condition than in the \emph{original gaze} condition, $t(18) = -2.06, p = .054$. There was no significant difference in their estimates of focus time on the yellow gem, $t(22) = 0, p = 1$.

Overall, these results suggest that our gaze editing approach is effective in that it allows a novice to modify the gaze animation of a character and obtain a scenario that is interpreted differently by na{\"i}ve viewers.
