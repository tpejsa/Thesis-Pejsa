\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Discussion}
\label{cha:Discussion}

Human attending behaviors involve movements of the eyes, head, and body, and they play a number of important communicative roles in human perception, social interaction, and cognition. The communicative effectiveness of these behaviors comes from their remarkable kinematic variability: quick eye saccades, eye-head shifts, upper-body and whole-body reorientations are movements with similar kinematic profiles, yet they appear very different and yield very different effects in communication. Producing convincing animation of such movements on virtual characters is a challenging problem, both in interactive and non-interactive contexts.
In interactive contexts, we require methods that can synthesize the highly varied gaze movements in a controllable fashion. These movements then need to be utilized as primitives for building more complex gaze behaviors that trigger specific effects in people observing or interacting with the character.
Moreover, since virtual characters can differ morphologically from realistic humans, we require methods that can adapt the kinematics of humanlike gaze movements to account for morphological differences.
In non-interactive, authoring contexts, the challenge also lies in animator skill and effort required to author high-quality gaze movements. This challenge is compounded by the poor scalability of manual authoring with respect to scenario complexity and scene changes. Traditional animation authoring methods, such as keyframing and IK, do not ensure good scalability on any of those dimensions.

This works posits that the problem of creating effective attending behaviors on animated characters can be solved more easily by modeling those behaviors as sequences of \emph{directed gaze shifts}. To prove this thesis, we contributed several methods for synthesis of directed gaze shifts, a new conversational gaze mechanism for virtual agents, a workflow and tool for authoring directed gaze animation, and a set of empirical insights about directed gaze as a model for interactive and non-interactive gaze behaviors. An overview of the contributions is given in Table~\ref{tab:Contributions}.

\begin{table}
\small
\centering
\def\arraystretch{1.5}
\begin{tabularx}{\textwidth}{lp{10.8cm}r}
\hline
\textbf{Category} & \textbf{Contribution} & \textbf{Chapter} \\
\hline
\multirow{5}{*}{Technical} & Gaze shift synthesis model & \ref{cha:GazeShiftModel} \\
& Stylized gaze & \ref{cha:StylizedGaze} \\
& Performative gaze & \ref{cha:StylizedGaze} \\
& Gaze inference from body motion & \ref{cha:GazeAuthoring} \\
& Gaze motion layering & \ref{cha:GazeAuthoring} \\
\hdashline
\multirow{3}{*}{Design} & Gaze editing approach & \ref{cha:GazeAuthoring} \\
& Gaze authoring workflow & \ref{cha:GazeAuthoring} \\
& Nonverbal signals of conversational footing & \ref{cha:GazeFooting} \\
\hdashline
\multirow{10}{*}{Empirical} & Gaze shift model synthesizes plausible, communicatively accurate gaze shifts & \ref{cha:GazeShiftModel} \\
& Gaze shifts with more torso reorientation are stronger attention cues & \ref{cha:GazeShiftModel} \\
& Virtual agents can use gaze and spatial orientation to manage footing & \ref{cha:GazeFooting} \\
& Nonverbal footing signals are effective only in immersive VR & \ref{cha:GazeFooting} \\
& Gaze adaptation to stylized characters reduces animation artifacts & \ref{cha:StylizedGaze} \\
& Stylized and performative gaze adaptations do not impair communicative accuracy or plausibility & \ref{cha:StylizedGaze} \\
& Human actor's gaze can be reconstructed from their body motion and scene layout & \ref{cha:GazeAuthoring} \\
& Gaze authoring approach allows novices to create and edit gaze animation with less effort & \ref{cha:GazeAuthoring} \\
& Animation obtained from the gaze authoring approach looks plausible & \ref{cha:GazeAuthoring} \\
& Edited gaze animation communicates a different attention distribution & \ref{cha:GazeAuthoring} \\
\hline
\end{tabularx}
\caption{Research contributions from this dissertation.}
\label{tab:Contributions}
\end{table}

In the remainder of this chapter, I discuss potential applications of the methods and findings contributed in this work, as well as their limitations and possible directions for future research.

\section{Applications}

Gaze synthesis and authoring methods introduced in this dissertation can be applied to animated characters in a wide range of applications. The gaze shift synthesis model (Chapter~\ref{cha:GazeShiftModel}) is designed primarily for interactive applications, such as non-player characters in games, player avatars, and embodied conversational agents, but we have also integrated it into an animation authoring workflow (Chapter~\ref{cha:GazeAuthoring}).
With its parametric control over head, torso, and whole-body orientation, the gaze shift model is also uniquely applicable to synthesis of nuanced attending behaviors that occur in multiparty interactions. One example are the nonverbal footing signals introduced in Chapter~\ref{cha:GazeFooting}, which can be utilized by virtual agents engaging in multiparty interactions with users in order to influence users' conversational contributions. Such behaviors may become an essential component in social experiences built for virtual and mixed reality media, where users are more sensitive to the spatial arrangement and nonverbal cues of interaction participants.

Stylized gaze methods (Chapter~\ref{cha:StylizedGaze}) have the potential to expand the range of character designs that can be animated by existing gaze models. There are many reasons why authors might want to employ characters with stylized features: such characters might be seen as more appealing than realistic, humanlike characters; they might connect better with the target demographic (e.g., children); their design might be exaggerated to emphasize specific personality traits, etc. While online motion retargeting to such characters is still an open problem, the methods introduced in the current work allow transfer of a specific subset of humanlike motion---directed gaze.

Finally, one application domain that has hitherto received almost no attention in computer animation research on gaze has been animation authoring. Even though much of the facial and body animation seen in today's computer-generated films is captured, eye movements still tend to be hand-authored from scratch. Moreover, editing the characters' gaze animation is still accomplished using labor- and skill-intensive tools. Chapter~\ref{cha:GazeAuthoring} introduces an authoring approach designed to assist in the production of gaze animation in non-interactive contexts, such as game cutscenes, television, and film animation. In addition to the ability to automatically add plausible eye animation to captured scenes, the approach enables convenient authoring and editing of gaze animation by novices. As such, it can serve as a labor-saving tool for experienced animators to give them a starting point for gaze animation editing, but it can also enable novices to author gaze animation at a scale and level of quality that would be beyond their reach using traditional methods.

\section{Limitations and Future Directions}

\subsection{Supporting Other Gaze Movements}

Directed gaze is not an all-encompassing model of human gaze movements. For example, it excludes gaze aversions, smooth pursuit, and expressive eye gestures such as eye-rolls. Some of these movements cannot be even approximately synthesized using our methods---e.g., our gaze shift model can synthesize only shortest-arc movements and as such cannot produce an eye-roll. On the other hand, some gaze movements are acceptably approximated by our synthesis methods. For example, in Chapter~\ref{cha:GazeFooting}, we realize conversational gaze aversions as directed gaze shifts toward probabilistically selected targets in the environment. Our gaze shift model can also fixate the character's gaze onto a moving target as an approximation of smooth pursuit, which looks plausible when the target is slow-moving. However, that is not a realistic simulation of human smooth pursuit. Neuroscience research has shown that smooth pursuit actually consists of a quick succession of submovements along the moving object's trajectory. Each submovement resembles a directed gaze shift and could be synthesized using our model, but a dedicated model is required to compute its direction and timing---one such model is introduced by~\citet{yeo2012eyecatch}.

The challenges of supporting additional gaze movements on top of directed gaze go beyond computational modeling of individual movements. Integrating these movements into our gaze authoring approach would also pose difficulties, as it would require not just new synthesis methods, but also new movement parametrizations for editing and a gaze inference model that can automatically detect the occurrence of these movements in the original motion data (e.g., the actor's gaze following a moving object). If the goal of future work is to achieve a more complete simulation of human gaze behaviors on virtual characters, it will need to tackle these challenges in a principled fashion.

\subsection{Increasing Gaze Expressiveness}

Major advantages of directed gaze as a model of humanlike attending behaviors are its simple parametrization and uniform nature---the entire behavior is modeled as a sequence of gaze shifts parametrized by timing, target, and head and torso alignments. These advantages come with a downside---the resulting behavior is relatively predictable and lacks expressive variation in eye, head, and body movements. The lack of variation in head and body movements is less of a problem in gaze authoring (Chapter~\ref{cha:GazeAuthoring}), where much of the expressiveness is already encoded in the body motion, but it becomes more noticeable when animating virtual agents, which can appear static and robot-like as a result.
On the other hand, the often sparse and predictable eye movements yielded by our gaze inference model, while plausible, do not approach the rich expressiveness seen in professionally animated characters, where ``each new thought triggers an eye movement''~\citep{maestri2001digital}.

To improve the expressiveness of the gaze behaviors generated by our models, we enrich them with probabilistically generated eye blinks and saccades added in a post-process. We generate the blinks as described by~\citet{peters2010animating}, while for saccadic gaze we use the Eyes Alive model~\citep{lee2002eyes}. These behaviors make the character look more lively, but they do not improve its communicative expressiveness, as they are generated without regard for context or high-level state. Saccades in humans reflect changes in cognitive state and speech content, while blinks correlate with emotion, personality, and alertness. Our models do not capture these high-level contingencies; they introduce saccades and blinks based only on the low-level gaze shift sequence. As I discuss further below, more sophisticated modeling may be required to capture the communicative variety of real human gaze.

\subsection{Modeling Gaze Contingencies}

The focus of this dissertation is on enabling the use of directed gaze shifts as building blocks of more sophisticated gaze behaviors and as a primitive for motion editing. Automatic production of communicatively effective gaze patterns is the task of high-level gaze models. Although such models are not the primary focus of the current work, two are nonetheless contributed: a gaze model for virtual agents engaging in multiparty interactions (Chapter~\ref{cha:GazeFooting}) and a model for gaze behavior inference from a captured body motion and scene (Chapter~\ref{cha:GazeAuthoring}). Future work can explore building other models that use directed gaze shifts as units of behavior. Example use cases where directed gaze is a good abstraction are various conversational mechanisms (e.g., configuring the conversational formation, turn-taking, intimacy regulation), attention in visually-guided tasks, attending behaviors in crowds, and idle gaze.

Many prior gaze models---including the ones in this dissertation---generate gaze sequences using relatively simple stochastic-heuristic methods. These methods do not always adequately capture the complexity of human attending behaviors and may give rise to gaze behaviors that look implausible or communicate the wrong thing. Part of the problem is that humans do not produce gaze cues in isolation, but in coordination with other behaviors, such as speech, facial expressions, hand gestures, proxemics, locomotion, etc. In order to look plausible and communicate effectively, an animated character must display gaze cues that match other aspects of its visible and audible behavior. The multitude of behavioral contingencies are not adequately modeled by heuristic rules and low-order statistics utilized in many current models.

To achieve more plausible and effective gaze behaviors, future work should investigate more sophisticated probabilistic models learned from human behavior data. Markov models, Dynamic Bayesian Networks (DBN), and reinforcement learning can capture temporal contingencies among different behavioral modalities, as well as their relationships to the current communicative context, which makes them potentially well-suited for gaze behavior synthesis. In particular, Markov Decision Processes (MDP) and reinforcement learning may be suitable when the character's high-level state is fully or partially known (observable). This is usually the case with autonomous agents, whose attention, communicative intent, and cognitive state are computationally modeled. Models such as Hidden Markov Models (HMMs) and Dynamic Bayesian Networks (DBN) are appropriate when the high-level state is unknown, such as when the character is animated using motion capture data or controlled by a human player using various tracking devices.

\subsection{Accounting for User Attributes, Character Design, and Scenario}

This dissertation has introduced directed gaze as a model of attending behaviors that generalizes well across different contexts. We have used directed gaze shifts as building blocks of joint attention signals, multiparty-conversational gaze behaviors, and an abstraction for gaze authoring. However, there are many factors within those contexts that our models do not account for, which should affect the gaze patterns produced by the models. One set of factors are user attributes, such as gender, personality, and culture. There are significant cross-gender differences in gaze perception, which we also detected in our own studies, e.g., in Chapter~\ref{cha:GazeFooting}. Personality is also known to affect gaze patterns---e.g., extroverts engage in significantly more eye contact than introverts~\citep{rutter1972visual}. Finally, multiple studies suggest that people's gaze usage varies across cultures---e.g., people in Western cultures initiate more eye contact than those in Eastern cultures~\citep{mccarthy2006cultural,mccarthy2008gaze}.

Similarly, attributes of the virtual character, such as gender, realism, desired personality, and even species, may affect how people respond to its gaze. Studies have shown that an embodied agent can motivate a user better by matching its gaze patterns to the user's personality~\citep{andrist2015look}. Moreover, while we have introduced methods that account for variations in character design, we did not extensively evaluate how these variations might modify the social and cognitive effects of gaze, even though prior research indicates that effects of embodiment design on the user may be substantial~\citep{parise1996my,baylor2009promoting}.

Finally, our models do not account for scenario variations. For example, the gaze model in Chapter~\ref{cha:GazeFooting} is not a universal model for signalling footing in multiparty interactions. It is based on data from scenarios involving a lot of verbal interaction and mutual gaze~\citep{mutlu2012conversational} and it may not generalize to scenarios involving (for example) more referential gaze and object manipulation.

To optimize the effects of the character's gaze behavior on the user, high-level gaze models should incorporate as parameters the user's attributes, character attributes, and scenario type, and they should adapt the output behavior based on their values. Since these effects are often subtle, accounting for them may require extensive data collection from an appropriately chosen participant pool. However, the benefits could be significant. For example, the agent in Chapter~\ref{cha:GazeFooting} could decrease the amount of mutual gaze with female users to avoid making them uncomfortable, while the gaze inference model (Chapter~\ref{cha:GazeAuthoring}) could make a character appear more introverted by decreasing the amount of gaze toward other characters.
% TODO: discuss other limitations:
% - Simplicity of model tasks/scenarios in our evaluations

\subsection{Adapting Gaze to User Behavior}

Chapter~\ref{cha:GazeFooting} has described a set of gaze and spatial reorientation cues for virtual agents allowing them to manage conversational footing in interactions with one or more users. While still preliminary in many ways, this work motivates the need for virtual agent systems that can effectively engage with users in multiparty interactions and exposes a number of research challenges. To be effective at managing multiparty interactions, agent systems need to use the information about the users' engagement intents, attention, and environment layout, to make decisions about the agent's positioning, reorientation, and gaze patterns, such that desired communicative goals are achieved. Possible goals include: (1) establish and reconfigure the conversational formation to make optimal use of available space; (2) use appropriate gaze behaviors and proxemics to maximize users' comfort level; (3) manage conversational footing to optimize users' level of participation and sustain their interest; (4) manage turn-taking to avoid ambiguities and breakdowns in communication.

A particularly salient lesson from our project is one of the need to make inferences about users' intents in order to avoid communication failures and keep them engaged. Some of the most egregious mistakes committed by the agent involved cutting off the participant due to an erroneous belief that they were finished with their utterance. This could have been avoided if the system possessed a more comprehensive model of turn-taking behaviors, including the ability to sense the participant's floor-holding cues such as averted gaze. Similarly, the experience of female participants may have been negatively affected when the agent engaged in too much mutual gaze. To avoid such effects, embodied agent systems should be more sensitive to users' intimacy-regulating signals---gaze aversions and interpersonal distance---and adjust their own behavior in response. Future work will need to take better advantage of the sensing capabilities afforded by modern VR devices---e.g., head and eye tracking---to characterize user behavior and make appropriate decisions about the agent's behavior based on those inferences.

\subsection{Directed Gaze as a Motion Authoring Abstraction}

Chapter~\ref{cha:GazeAuthoring} has introduced the first animation authoring approach designed specifically for gaze, built around directed gaze as the motion abstraction for authoring.
While the approach succeeds in its two-fold goal of reducing authoring cost and producing motion at an acceptable level of quality, it can still be significantly improved upon in both regards.
% TODO

Time domain vs. frequency domain
Statistical models vs. deterministic gaze shift sequences
Adjusting gaze patterns to target type (human, task-relevant object, or a salient feature in the environment), scenario type (conversation, locomotion, environment interaction), and governing attention mechanism (top-down vs. bottom-up attention)
Incorporating character attributes, such as gender and personality
Technical and design challenges, such as building computational models for gaze behavior synthesis that incorporate those variables, and exposing editing controls that intuitively map to them in the way that gaze target and alignment handles map to properties of directed gaze shifts


Modeling character intents
Go beyond adding gaze to motion data
Understand the activity encoded in the motion, the actions that comprise it, and the intents behind those actions
Gaze that expresses those intents could then be added automatically, as in~\citep{bai2012synthesis}
Gaze could also adapt automatically as the character's actions get modified and rearranged

\subsection{Authoring Multi-Character Attending Behaviors}

% TODO
The current authoring approach is not yet comprehensive enough to allow universal editing of characters' attending behaviors, as it lacks support for whole-body orientation. Editing whole-body orientation as well as eye gaze is challenging since it requires editing the entire underlying body motion, which involves root movements and complex patterns of end-effector constraints. Existing methods such as motion path editing~\citep{gleicher2001path} may provide a good starting point for implementing such features.

With improved computational efficiency and ability to edit whole-body movements, the approach could be used for editing attending behaviors of groups and crowds of characters in a scalable manner. Attending behaviors in such contexts are more complex than simple gaze and spatial orientation shifts. Groups of interacting people arrange themselves into conversational formations~\citep{kendon2010spacing}, they maintain mutual gaze and interpersonal distance appropriate to their situation and intimacy level~\citep{argyle1965eyecontact}, and their gaze patterns reflect a number of joint perceptual, attentional, and social-cognitive processes. To synthesize such behaviors automatically and enable their convenient editing entails a slew of research challenges. These include computational modeling of the principles that govern human social behavior, designing editing handles that map intuitively to parameters of large-scale social interactions, and developing new motion synthesis and editing methods that can handle complex, multi-character, spatial and temporal constraints. The methods and designs presented in the current work represent but the first steps toward a more comprehensive editing approach. 