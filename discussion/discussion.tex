\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Discussion}
\label{cha:Discussion}

Human attending behaviors involve movements of the eyes, head, and body, and they play a number of important communicative roles in human perception, social interaction, and cognition. The communicative effectiveness of these behaviors comes from their remarkable kinematic variability: quick eye saccades, eye-head shifts, upper-body and whole-body reorientations are movements that occur in tight coodination with one another and yield very different effects in communication. Producing convincing animation of such movements on virtual characters is a challenging problem, both in interactive and non-interactive contexts. In a non-interactive context, the challenge lies in animator skill and effort required to author such movements, due to the inherent complexity of their spatial and timing relationships. A further challenge is posed by the poor scalability of manual authoring with respect to scenario complexity, scene changes, and character design. Traditional methods used for gaze animation authoring, such as keyframing and IK, do not ensure good scalability on any of those dimensions. In an interactive context, gaze motion synthesis is accomplished by automatic methods, that can dynamically adapt to changes in scenario state and user inputs. To realize the full range of human attending behaviors, the methods must be capable of synthesizing biologically plausible, coordinated movements of the eyes, head, torso, and whole body toward targets in the environment. Moreover, the methods must afford parametric control over how these movements coordinate, such that a subtle saccade can be produced just as easily as a whole-body reorientation.

This works posits that the problem of creating effective attending behaviors on animated characters in a scalable way can be solved more easily by modeling those behaviors as \emph{directed gaze}---sequences of coordinated, intentional movements of the eyes, head, torso, and the whole body toward target objects and information in the environment. To prove this thesis, we contributed several methods for synthesis of directed gaze movements, a new conversational gaze mechanism for virtual agents, a workflow and tool for authoring directed gaze animation, and a set of empirical insights about directed gaze as a model for interactive and non-interactive gaze behaviors. An overview of the contributions is given in Table~\ref{tab:Contributions}.

\begin{table}
\small
\centering
\def\arraystretch{1.5}
\begin{tabularx}{\textwidth}{lp{10.8cm}r}
\hline
\textbf{Category} & \textbf{Contribution} & \textbf{Chapter} \\
\hline
\multirow{5}{*}{Technical} & Gaze shift synthesis model & \ref{cha:GazeShiftModel} \\
& Stylized gaze & \ref{cha:StylizedGaze} \\
& Performative gaze & \ref{cha:StylizedGaze} \\
& Gaze inference from body motion & \ref{cha:GazeAuthoring} \\
& Gaze motion layering & \ref{cha:GazeAuthoring} \\
\hdashline
\multirow{3}{*}{Design} & Gaze editing approach & \ref{cha:GazeAuthoring} \\
& Gaze authoring workflow & \ref{cha:GazeAuthoring} \\
& Nonverbal signals of conversational footing & \ref{cha:GazeFooting} \\
\hdashline
\multirow{10}{*}{Design} & Gaze shift model synthesizes plausible, communicatively accurate gaze shifts & \ref{cha:GazeShiftModel} \\
& Gaze shifts with more torso reorientation are stronger attention cues & \ref{cha:GazeShiftModel} \\
& Virtual agents can use gaze and spatial orientation to manage footing & \ref{cha:GazeFooting} \\
& Nonverbal footing signals are effective only in immersive VR & \ref{cha:GazeFooting} \\
& Gaze adaptation to stylized characters reduces animation artifacts & \ref{cha:StylizedGaze} \\
& Stylized and performative gaze adaptations do not impair communicative accuracy or plausibility & \ref{cha:StylizedGaze} \\
& Human actor's gaze can be reconstructed from their body motion and scene layout & \ref{cha:GazeAuthoring} \\
& Gaze authoring approach allows novices to create and edit gaze animation with less effort & \ref{cha:GazeAuthoring} \\
& Animation obtained from the gaze authoring approach looks plausible & \ref{cha:GazeAuthoring} \\
& Edited gaze animation communicates a different attention distribution & \ref{cha:GazeAuthoring} \\
\hline
\end{tabularx}
\caption{Research contributions from this dissertation.}
\label{tab:Contributions}
\end{table}

In the remainder of this chapter, I discuss potential applications of the methods and findings contributed in this work, as well as their limitations and possible future research directions.

\section{Applications}

Gaze synthesis and authoring methods introduced in this dissertation can be applied to animated characters in a wide range of applications. The gaze shift synthesis model (Chapter~\ref{cha:GazeShiftModel}) is designed primarily for interactive applications, such as non-player characters in games, player avatars, and embodied conversational agents, but we have also integrated it into an animation authoring workflow (Chapter~\ref{cha:GazeAuthoring}).
With its parametric control over head, torso, and whole-body orientation, the gaze shift model is also uniquely applicable to synthesis of nuanced attending behaviors that occur in multiparty interactions. One example are the nonverbal footing signals introduced in Chapter~\ref{cha:GazeFooting}, which can be utilized by virtual agents engaging in multiparty interactions with users in order to influence users' conversational contributions. Such behaviors may become an essential component in social experiences built for virtual and mixed reality media, where users are more sensitive to the spatial arrangement and nonverbal cues of interaction participants.

Stylized gaze methods (Chapter~\ref{cha:StylizedGaze}) have the potential to expand the range of character designs that can be animated by existing gaze models. There are many reasons why authors might want to employ characters with stylized features: such characters might be seen as more appealing than realistic, humanlike characters; they might connect better with the target demographic (e.g., children); their design might be exaggerated to emphasize specific personality traits, etc. While online motion retargeting to such characters is still an open problem, the methods introduced in the current work allow transfer of a specific subset of humanlike motion---directed gaze.

Finally, one application domain that has hitherto received almost no attention in computer animation research on gaze has been animation authoring. Even though much of the facial and body animation seen in today's computer-generated films is captured, eye movements still tend to be hand-authored from scratch. Moreover, editing the characters' gaze animation is still accomplished using labor- and skill-intensive tools. Chapter~\ref{cha:GazeAuthoring} introduces an authoring approach designed to assist in the production of gaze animation in non-interactive contexts, such as game cutscenes, television, and film animation. In addition to the ability to automatically add plausible eye animation to captured scenes, the approach enables convenient authoring and editing of gaze animation by novices. As such, it can serve as a labor-saving tool for experienced animators to give them a starting point for gaze animation editing, but it can also enable novices to author gaze animation at a scale and level of quality that would be beyond their reach using traditional methods.

\section{Limitations and Future Directions}

\subsection{Increasing Gaze Expressiveness}

Major advantages of directed gaze as a model of humanlike attending behaviors are its simple parametrization and uniform nature---the entire behavior is modeled as a sequence of gaze shifts parametrized by timing, target, and head and torso alignments. These advantages come with a downside---the resulting behavior is relatively predictable and lacks expressive variation in eye, head, and body movements. The lack of variation in head and body movements is less of a problem in gaze authoring (Chapter~\ref{cha:GazeAuthoring}), where much of the expressiveness is already encoded in the body motion, but it becomes more noticeable when animating virtual agents, which can appear static and robot-like as a result.
On the other hand, the often sparse and predictable eye movements yielded by our gaze inference model, while plausible, do not approach the rich expressiveness seen in professionally animated characters, where ``each new thought triggers an eye movement''~\citep{maestri2001digital}.

Directed is by no means an all-encompassing model of human gaze movements. For example, it excludes gaze movements such as eye blinks, smooth pursuit and pupil dilation. Animators may wish to incorporate such movements to make their characters look more expressive. While some of these movements are simulated in prior work (e.g., smooth pursuit~\citep{yeo2012eyecatch}), integrating them into our models would be far from straightforward. For example, to support smooth pursuit in our gaze authoring approach would require not just new synthesis methods, but also new movement parametrizations and a gaze inference model that can automatically detect gaze at moving objects.

To improve the expressiveness of the gaze behaviors generated by our models, we enrich them with probabilistically generated eye blinks and saccades added in a post-process. We generate the blinks as described by~\citet{peters2010animating}, while for saccadic gaze we use the Eyes Alive model~\citep{lee2002eyes}. These behaviors make the character look more lively, but they do not improve its communicative expressiveness, as they are generated without regard for context or high-level state. Saccades in humans reflect changes in cognitive state and speech content, while blinks correlate with emotion, personality, and alertness. Our models do not capture these high-level contingencies; they introduce saccades and blinks based only on the low-level gaze shift sequence. As I discuss further below, more sophisticated modeling may be required to capture the communicative variety of real human gaze.

\subsection{Building Better Gaze Models}

The focus of this dissertation is on enabling the use of directed gaze shifts as building blocks of more sophisticated gaze behaviors and as a primitive for motion editing. Automatic production of communicatively effective gaze patterns is the task of high-level gaze models. Although such models are not the primary focus of the current work, two are nonetheless contributed: a gaze model for virtual agents engaging in multiparty interactions (Chapter~\ref{cha:GazeFooting}) and a model for gaze behavior inference from a captured body motion and scene (Chapter~\ref{cha:GazeAuthoring}). Future work can explore building other models that use directed gaze shifts as units of behavior. Example use cases where directed gaze is a good abstraction are various conversational mechanisms (e.g., configuring the conversational formation, turn-taking, intimacy regulation), attention in visually-guided tasks, attending behaviors in crowds, and idle gaze.

Many prior gaze models---including the ones in this dissertation---generate gaze sequences using relatively simple stochastic-heuristic methods. These methods do not always adequately capture the complexity of human attending behaviors and may give rise to gaze behaviors that look implausible or communicate the wrong thing. Part of the problem is that humans do not produce gaze cues in isolation, but in coordination with other aspects of behavior, such as speech, facial expressions, hand gestures, proxemics, locomotion, etc. In order to look plausible and communicate effectively, an animated character must display gaze cues that match other aspects of its visible and audible behavior. The multitude of behavioral contingencies are not adequately modeled by heuristic rules and low-order statistics utilized in many current models.

Future work should investigate probabilistic models learned from time-series data for gaze behavior synthesis. In particular, Markov Decision Processes (MDP) and reinforcement learning may be suitable approaches when the character's high-level state is fully or partially known (observable). This is usually the case with autonomous agents, whose attention, communicative intent, and cognitive state are computationally modeled. Models such as Hidden Markov Models (HMMs) and Dynamic Bayesian Networks (DBN) are appropriate for cases where the high-level state is unknown, such as when the character is animated using motion capture data or controlled by a human player using various tracking devices. Since such models can capture temporal contingencies among different behavioral modalities, as well as their relationships to the current communicative context, I hypothesize they will enable synthesis of more plausible and communicatively effective gaze behaviors.

\subsection{Gaze Model Generality}

This dissertation has proposed directed gaze as model of attending behaviors that generalizes well across different contexts. We have used directed gaze shifts as building blocks of joint attention signals, multiparty-conversational gaze behaviors, and an abstraction for gaze authoring. However, there are many factors within those contexts that our models do not account for, which should affect the gaze patterns produced by the models. One major factor is \emph{user gender}. There are significant cross-gender differences in gaze perception, which we also detected in our own studies, e.g., in Chapter~\ref{cha:GazeFooting}. Other user-characteristic factors include \emph{personality}---e.g., extroverts are known to engage in significantly more eye contact than introverts~\citep{rutter1972visual}---and \emph{culture}---e.g., multiple studies suggest that people from Western cultures initiate more eye contact than those from Eastern cultures~\citep{mccarthy2006cultural,mccarthy2008gaze}. Furthermore, our models do not account for scenario variations. For example, the gaze model in Chapter~\ref{cha:GazeFooting} is not a universal model for signalling footing in multiparty interactions. It is based on data from scenarios involving a lot of verbal interaction and mutual gaze~\citep{mutlu2012conversational} and it may not generalize to scenarios involving (for example) more referential gaze and object manipulation. Finally, attributes of the virtual character, such as gender, realism, desired personality, and even species, may also affect how people respond to its gaze. While we have introduced methods that account for variations in character design, we did not extensively evaluate how these variations might modify the social and cognitive effects of gaze, even though prior research indicates that effects of embodiment on user behavior may be substantial~\citep{parise1996my}.

To optimize the effects of the character's gaze behavior on the current user, high-level gaze models should incorporate the user's attributes, scenario type, and character attributes as parameters, and they should adapt the output behavior based on their values. Since these effects are often subtle, accounting for them may require extensive data collection from an appropriately chosen participant pool. However, the benefits could be significant. For example, the agent in Chapter~\ref{cha:GazeFooting} could decrease the amount of mutual gaze with female users to avoid making them uncomfortable, while the gaze inference model (Chapter~\ref{cha:GazeAuthoring}) could make a character appear more introverted by decreasing the amount of gaze toward other characters.
% TODO: discuss other limitations:
% - Simplicity of model tasks/scenarios in our evaluations

\subsection{Adapting Gaze to User Behavior}

Directed gaze is an effective model for realizing several conversational gaze behaviors. Chapter~\ref{cha:GazeFooting} describes a set of gaze and spatial reorientation cues for virtual agents allowing them to manage conversational footing in interactions with one or more users. While still preliminary in many ways, this work motivates the need for virtual agent systems that can effectively engage with users in multiparty interactions and exposes a number of research challenges. To be effective at managing multiparty interactions, agent systems need to use the information about the users' engagement intents, attention, and environment layout, to make decisions about the agent's positioning, reorientation, and gaze patterns, such that desired communicative goals are achieved. Possible goals include: (1) establish and reconfigure the conversational formation to make optimal use of available space; (2) use appropriate gaze behaviors and proxemics to maximize users' comfort level; (3) manage conversational footing to optimize users' level of participation and sustain their interest; (4) manage turn-taking to avoid ambiguities and breakdowns in communication.

A particularly salient lesson from our project is one of the need to make inferences about users' intents in order to avoid communication failures and keep them engaged. Some of the most egregious mistakes committed by the agent involved cutting off the participant due to an erroneous belief that they were finished with their utterance. This could have been avoided if the system possessed a more comprehensive model of turn-taking behaviors, including the ability to sense the participant's floor-holding cues such as averted gaze. Similarly, the experience of female participants may have been negatively affected when the agent engaged in too much mutual gaze. To avoid such effects, embodied agent systems should be more sensitive to users' intimacy-regulating signals---gaze aversions and interpersonal distance---and adjust their own behavior in response. Future work will need to take better advantage of the sensing capabilities afforded by modern VR devices---e.g., head and eye tracking---to characterize user behavior and make appropriate decisions about the agent's behavior based on those inferences.

\subsection{Scalable Authoring of Attending Behaviors}

Chapter~\ref{cha:GazeAuthoring} of this dissertation has introduced the first animation authoring approach designed specifically for gaze, built around directed gaze as the underlying model. While the approach succeeds in its goal of reducing authoring effort while producing motion at an acceptable level of quality, it can still be significantly improved upon. First, the computational efficiency of gaze motion synthesis is still insufficient for truly interactive editing, due to the alignment parametrization and feed-forward design of the gaze shift model, which requires the entire motion to be resynthesized to see the exact editing result. Second, the current authoring approach is not yet comprehensive enough to allow universal editing of characters' attending behaviors, as it lacks support for whole-body orientation. Editing whole-body orientation as well as eye gaze is challenging since it requires editing the entire underlying body motion, which involves root movements and complex patterns of end-effector constraints. Existing methods such as motion path editing~\citep{gleicher2001path} may provide a good starting point for implementing such features.

With improved computational efficiency and ability to edit whole-body movements, the approach could be used for editing attending behaviors of groups and crowds of characters in a scalable manner. Attending behaviors in such contexts are more complex than simple gaze and spatial orientation shifts. Groups of interacting people arrange themselves into conversational formations~\citep{kendon2010spacing}, they maintain mutual gaze and interpersonal distance appropriate to their situation and intimacy level~\citep{argyle1965eyecontact}, and their gaze patterns reflect a number of joint perceptual, attentional, and social-cognitive processes. To synthesize such behaviors automatically and enable their convenient editing entails a slew of research challenges. These include computational modeling of the principles that govern human social behavior, designing editing handles that map intuitively to parameters of large-scale social interactions, and developing new motion synthesis and editing methods that can handle complex, multi-character, spatial and temporal constraints. The methods and designs presented in the current work represent but the first steps toward a more comprehensive editing approach. 