\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Discussion}
\label{cha:Discussion}

Human attending behaviors involve intricate, coordinated movements of the eyes, head, and torso, and producing convincing animation of such movements on humanlike characters is a challenging problem. The challenge lies not just in animator skill and effort required to author such movements, but also in the difficulty of doing so in a scalable way. A method of authoring or synthesizing animation is scalable if the motion can be created once and then reused across many different characters and scenarios. Gaze animation authored using traditional methods, such as keyframing and IK, cannot be easily transferred to a new character, let alone adapted to a whole new scenario involving different character activities (encoded in their body motion) and scene layout. Models for automatic gaze motion synthesis offer a suitable alternative to hand-authoring in specific scenarios, such as idle gaze or conversational gaze, but they do not scale beyond those scenarios and they also typically offer little control over the output gaze motion. Moreover, there are major subsets of human attending behaviors that are not supported by any existing synthesis models, most notably coordinated gaze and body reorientation.

This works posits that the problem of creating effective attending behaviors on animated characters in a scalable way can be solved more easily by modeling those behaviors as \emph{directed gaze}---sequences of coordinated, intentional movements of the eyes, head, torso, and the whole body toward target objects and information in the environment. To prove this thesis, we contributed several methods for synthesis and authoring of directed gaze behaviors, as well as a set of evaluations of those methods from the standpoint of their scalability and the communicative effectiveness of the resulting behaviors. The specific contributions from this work are the following:

\begin{enumerate}
\item \textbf{Gaze shift synthesis}. Chapter~\ref{cha:GazeShiftModel} has introduced a procedural, neurophysiologically model for synthesis of directed gaze shifts. The model affords intuitive parametric control over coordinated eye, head, and body movements and it abstracts away details of timing and spatial relationships, thus reducing the amount of skill and effort needed to create biologically plausible gaze movements. It is the first model that can produce attention shifts that include parametrically controllable, coordinated movements of the torso and whole body. The naturalness and communicative effectiveness of the model are evaluated in two studies with human participants.
\item \textbf{Gaze and footing in multiparty conversations with a virtual agent} Chapter~\ref{cha:GazeFooting} has sought to provide further demonstration of the importance of coordinated eye gaze and body orientation shifts in implementing effective attending behaviors on virtual characters. It has introduced new directed gaze behaviors for virtual agents engaged in multiparty interaction with avatar-embodied users, that enable them to influence users' level of participation in the interaction by signaling their conversational footing. An experiment with human participants has shown that a virtual agent using the proposed behaviors indeed has that capability, which manifests itself specifically in high-fidelity, immersive VR.
\item \textbf{Stylized and performative gaze}. Chapter~\ref{cha:StylizedGaze} has introduced methods for automatic adaptation of gaze movements to variations in character design and camera position, with the express goal of supporting stylized and non-human characters with exaggerated eye shape and proportions. A set of evaluations have shown that these methods allow synthesis of gaze shifts that are scalable with respect to character design, while preserving their communicative effectiveness.
\item \textbf{Gaze animation authoring}. Chapter~\ref{cha:GazeAuthoring} has proposed an approach for scalable authoring of gaze animation in non-interactive scenarios. Given inputs consisting of a character animated using motion capture and a 3D model of the virtual scene, the proposed approach can automatically synthesize a directed gaze behavior that matches these inputs. The approach also includes methods and a tool for convenient editing of the gaze behavior. Evaluations are provided of the utility of the approach in reducing authoring effort and producing plausible, communicatively effective gaze across a wide range of scenarios.
\end{enumerate}

In the remainder of this chapter, I discuss potential applications of the methods and findings contributed in this work, as well as their limitations and possible future research directions.

\section{Applications}

Gaze synthesis and authoring methods introduced in this dissertation can be applied to animated characters in a wide range of applications. The gaze shift synthesis model (Chapter~\ref{cha:GazeShiftModel}) in particular is designed for characters in interactive applications, such as non-player characters in games, player avatars, and embodied conversational agents, but we have also integrated it into an animation authoring pipeline for offline scenarios (Chapter~\ref{cha:GazeAuthoring}).
With its parametric control over torso and whole-body orientation, the gaze shift model offers unique capabilities for implementing complex attending behaviors needed to support multiparty interactions. Such interactions will become more prevalent in the nascent virtual and mixed reality media, where users are more sensitive to the spatial arrangement of the interaction participants and nonverbal cues they demonstrate (as shown in Chapter~\ref{cha:GazeFooting}).

While preceding methods expand the range of gaze behaviors that can be synthesized on virtual characters, stylized gaze methods (Chapter~\ref{cha:StylizedGaze}) have the potential to expand the range of character designs that can be animated by existing gaze models. There are many scenarios where authors might want to employ characters with stylized features; such characters might be seen as more appealing than realistic, humanlike characters, they might connect better with the target demographic (e.g., children), their design might be exaggerated to emphasize specific personality traits, etc. While online motion retargeting to such characters is still an open problem, the methods introduced in the current work are a major step in allowing reuse of a specific subset of humanlike motion---directed gaze.

Finally, one application domain that has hitherto received almost no attention in computer graphics research on gaze has been animation authoring. Even though much of the facial and body animation seen in today's computer-generated films is captured, eye movements still tend to be hand-authored from scratch. Moreover, editing the characters' attending behaviors is still accomplished using labor- and skill-intensive traditional animation tools. The approach introduced in Chapter~\ref{cha:GazeAuthoring} is meant to assisting in production of gaze animation in non-interactive contexts, such as game cutscenes or television and film animation. In addition to the ability to automatically add plausible eye animation to captured scenes, the approach enables convenient authoring and editing of gaze animation by novices. As such, it can serve as a labor-saving tool for experienced animators to give them a starting point for gaze animation editing, but it can also enable novices to author gaze animation at a scale and level of quality that would be beyond their reach using traditional methods.

\section{Future Directions}

\subsection{Supporting Expressive Gaze Behaviors}

This dissertation has proposed directed gaze as an effective model for synthesis of many kinds of humanlike communicative behaviors. However, it by no means an all-encompassing model of human gaze, as it excludes gaze movements such as smooth pursuit, probabilistic saccades, and minor eye movements such as tremors and pupil dilation. Some of these movements are simulated in prior work (e.g., smooth pursuit~\citep{yeo2012eyecatch}) and a possible future extension of our own work would be to integrate them into our gaze authoring approach. This task would be far from straightforward, however, as these other movements require not just new synthesis methods, but also new parametrizations and new motion analysis and inference models.

Major advantages of directed gaze as a model of attending behavior are its simple parametrization and uniform nature---the entire behavior is modeled as a sequence of gaze shifts parametrized by timing, target, and head and torso alignments. These advantages come with a downside---the outputs of our gaze synthesis methods are relatively predictable and lack expressiveness in head and body movements as well as eye movements. The lack of variation in body movements is less of a problem in gaze authoring, where much of the expressiveness is already encoded in the body motion, but becomes more noticeable when animating virtual agents, which can appear static and robot-like as a result.
On the other hand, the often sparse and predictable eye movements yielded by our gaze inference model~\ref{sec:GazeInference}, while plausible, do not approach the rich expressivity seen in professionally animated characters, where ``each new thought triggers an eye movement''~\citep{maestri2001digital}.

Adding expressivity to our methods is difficult to do without sacrificing generality. Rich patterns found in human gaze are very context-dependent---e.g., gaze aversions and referential gaze that occur in coordination with cognition and speech production---and any model that produces such patterns much consider high-level state beyond a simple gaze shift sequence.
Regardless, future work should explore integration of high-level gaze models with our gaze authoring workflow to enrich the output in specific scenarios. The gaze mechanisms proposed in Chapter~\ref{cha:GazeFooting} are a step in that direction, unique in that they represent the first high-level gaze model to incorporate body orientation cues.

\subsection{Nonverbal Behaviors in Multiparty Interactions}

Directed gaze is an effective model for realizing several nonverbal behaviors that occur in multiparty interactions. Chapter~\ref{cha:GazeFooting} describes a set of gaze and body reorientation mechanisms for virtual agents allowing them to manage conversational footing in interactions with one or more users. It gives empirical evidence of the importance of such behaviors for virtual agent interaction in immersive VR. While still preliminary in many ways, this work motivates the need for virtual agent systems that can effectively engage with users in multiparty interactions and exposes a number of research challenges. To be effective at managing multiparty interactions, agent systems need to use information about user engagement intent, attention, and environment layout, in combination with the agent's movement, reorientation, and gaze cuing capabilities, to achieve a variety of task goals. For example: (1) establish and reconfigure conversational formations that make optimal use of available space; (2) use appropriate gaze behaviors and proxemics to maximize users' comfort level; (3) manage conversational footing to optimize users' level of participation and sustain their interest; (4) manage turn-taking to avoid ambiguities and breakdowns in communication.

A particularly salient lesson from our project is one of the need to make inferences about users' intents in order to avoid communication failures and keep them engaged. Some of the most grating mistakes committed by the agent involved cutting off the user due to erroneous belief that they were finished with their utterance, which could have been avoided if the agent had possessed awareness of the user's floor-holding behaviors, such as averted gaze. Similarly, the experience of some users may have been negatively affected when the agent engaged in too much mutual gaze. To avoid such effects, embodied agent systems should be more sensitive to users' intimacy-regulating signals---gaze aversions and interpersonal distance---and adjust their own behavior in response. Future work will need to take better advantage of the sensing capabilities afforded by modern VR devices---e.g., head and hand tracking---to characterize user behavior and make appropriate decisions about the agent's behavior based on those inferences.

\subsection{Scalable Authoring of Attending Behaviors}

Chapter~\ref{cha:GazeAuthoring} of this dissertation has introduced the first animation authoring approach designed specifically for gaze, built around directed gaze as the underlying model. While the approach succeeds in its goal of reducing authoring effort while producing motion at an acceptable level of quality, it can still be improved upon in many ways, from the accuracy of initial inference to the computational efficiency of editing. The latter limitation is particularly salient: the current implementation of the approach cannot provide truly interactive editing due to the alignment parametrization and feed-forward design of the gaze shift model, which requires the entire motion to be resynthesized to see the exact editing result.

The current authoring approach is not yet comprehensive enough to allow universal editing of characters' attending behaviors. One limitation is the lack of support for whole-body orientation. Editing whole-body orientation as well as eye gaze is challenging since it requires editing the entire underlying body motion, which involves root movements and complex patterns of end-effector constraints. Existing methods such as motion path editing~\citep{gleicher2001path} may provide a good starting point for implementing such features.

The gaze authoring approach has the capability to generate a large amount of gaze motion automatically, adding editable gaze behaviors to an entire scene at the click of a button. This makes it an appealing option for animating the gaze of groups and crowds of interacting characters. However, attending behaviors in such contexts are more complex than simple gaze shifts. Groups of interacting people arrange themselves into conversational formations~\citep{kendon2010spacing}, they maintain mutual gaze and interpersonal distance appropriate to their situation and intimacy level~\citep{argyle1965eyecontact}, and their gaze patterns reflect a number of perceptual, attentional, and social-cognitive processes that go on in parallel. To synthesize such behaviors automatically and enable their convenient editing entails a slew of research challenges. These include computational modeling of the principles that govern human social behavior, designing editing handles that map intuitively to parameters of large-scale social interaction, and developing new motion synthesis and editing methods that can handle complex, multi-character, spatial and temporal constraints. The methods presented in the current work represent but one step toward a more ambitious goal---supporting computational authoring of large-scale attending behaviors for interacting characters. 