\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Discussion}
\label{cha:Discussion}

Human attending behaviors involve movements of the eyes, head, and body, and they play a number of important communicative roles in human perception, social interaction, and cognition. The communicative effectiveness of these behaviors comes from their remarkable kinematic variability: quick eye saccades, eye-head shifts, upper-body and whole-body reorientations are movements that occur in tight coodination with one another and yield very different effects in communication. Producing convincing animation of such movements on virtual characters is a challenging problem, both in interactive and non-interactive contexts. In a non-interactive context, the challenge lies in animator skill and effort required to author such movements, due to the inherent complexity of their spatial and timing relationships. A further challenge is posed by the poor scalability of manual authoring with respect to scenario complexity, scene changes, and character design. Traditional methods used for gaze animation authoring, such as keyframing and IK, do not ensure good scalability on any of those dimensions. In an interactive context, gaze motion synthesis is accomplished by automatic methods, that can dynamically adapt to changes in scenario state and user inputs. To realize the full range of human attending behaviors, the methods must be capable of synthesizing biologically plausible, coordinated movements of the eyes, head, torso, and whole body toward targets in the environment. Moreover, the methods must afford parametric control over how these movements coordinate, such that a subtle saccade can be produced just as easily as a whole-body reorientation.

This works posits that the problem of creating effective attending behaviors on animated characters in a scalable way can be solved more easily by modeling those behaviors as \emph{directed gaze}---sequences of coordinated, intentional movements of the eyes, head, torso, and the whole body toward target objects and information in the environment. To prove this thesis, we contributed several methods for synthesis and authoring of directed gaze behaviors, as well as a set of evaluations of those methods from the standpoint of their scalability and the communicative effectiveness of the resulting behaviors. The specific contributions from this work are the following:

\begin{enumerate}
\item \textbf{Gaze shift synthesis}. Chapter~\ref{cha:GazeShiftModel} has introduced a procedural, neurophysiologically model for synthesis of directed gaze shifts. The model affords parametric control over coordinated eye, head, and body movements and it simulated their spatial and timing relationships using a set of kinematic laws derived from neurophysiology research. Thus it eliminates the need to use hand-authoring or motion capture to obtain biologically plausible gaze movements. It is also the first model that can produce paramerically controllable shifts in torso and whole-body orientation. The naturalness and communicative effectiveness of the model are evaluated in two studies with human participants.
\item \textbf{Gaze and footing in multiparty conversations with a virtual agent} Chapter~\ref{cha:GazeFooting} has sought to provide further evaluation of the communicative effectiveness of our directed gaze model, as well as motivation for the use of directed gaze behaviors on embodied conversational agents. It has enabled virtual agents engaged in multiparty interaction with avatar-embodied users to influence users' level of participation by signaling their conversational footing through gaze and body orientation cues. An experiment with human participants has shown that a virtual agent using the proposed behaviors indeed has that capability, which manifests itself specifically in high-fidelity, immersive VR.
\item \textbf{Stylized and performative gaze}. Chapter~\ref{cha:StylizedGaze} has introduced methods for automatic adaptation of gaze movements to variations in character design and camera position, with the goal of making gaze motion synthesis methods scale better to stylized and non-human characters with exaggerated eye shape and proportions. A set of evaluations have shown that these methods allow synthesis of gaze shifts across a range of character designs, while preserving their communicative effectiveness.
\item \textbf{Gaze animation authoring}. Chapter~\ref{cha:GazeAuthoring} has proposed an approach for scalable authoring of gaze animation in non-interactive scenarios. Given inputs consisting of a character animated using motion capture and a 3D model of the virtual scene, the proposed approach can automatically synthesize a directed gaze behavior that matches these inputs. The approach also includes methods and a tool for convenient editing of the gaze behavior. Evaluations are provided of the utility of the approach in reducing required authoring effort and skill, and producing plausible, communicatively effective gaze in a wide range of scenarios.
\end{enumerate}

In the remainder of this chapter, I discuss potential applications of the methods and findings contributed in this work, as well as their limitations and possible future research directions.

\section{Applications}

Gaze synthesis and authoring methods introduced in this dissertation can be applied to animated characters in a wide range of applications. The gaze shift synthesis model (Chapter~\ref{cha:GazeShiftModel}) is designed primarily for interactive applications, such as non-player characters in games, player avatars, and embodied conversational agents, but we have also integrated it into an animation authoring pipeline for offline scenarios (Chapter~\ref{cha:GazeAuthoring}).
With its parametric control over head, torso, and whole-body orientation, the gaze shift model is also uniquely applicable to synthesis of nuanced attending behaviors that occur in multiparty interactions. One example are the nonverbal footing signals introduced in Chapter~\ref{cha:GazeFooting}, which can be utilized by virtual agents engaging in multiparty interactions with users in order to influence users' conversational contributions. Such behaviors may become an essential component in social experiences built for virtual and mixed reality media, where users are more sensitive to the spatial arrangement and nonverbal cues of interaction participants.

Stylized gaze methods (Chapter~\ref{cha:StylizedGaze}) have the potential to expand the range of character designs that can be animated by existing gaze models. There are many reasons why authors might want to employ characters with stylized features: such characters might be seen as more appealing than realistic, humanlike characters; they might connect better with the target demographic (e.g., children); their design might be exaggerated to emphasize specific personality traits, etc. While online motion retargeting to such characters is still an open problem, the methods introduced in the current work allow transfer of a specific subset of humanlike motion---directed gaze.

Finally, one application domain that has hitherto received almost no attention in computer animation research on gaze has been animation authoring. Even though much of the facial and body animation seen in today's computer-generated films is captured, eye movements still tend to be hand-authored from scratch. Moreover, editing the characters' gaze animation is still accomplished using labor- and skill-intensive tools. Chapter~\ref{cha:GazeAuthoring} introduces an authoring approach designed to assist in the production of gaze animation in non-interactive contexts, such as game cutscenes, television, and film animation. In addition to the ability to automatically add plausible eye animation to captured scenes, the approach enables convenient authoring and editing of gaze animation by novices. As such, it can serve as a labor-saving tool for experienced animators to give them a starting point for gaze animation editing, but it can also enable novices to author gaze animation at a scale and level of quality that would be beyond their reach using traditional methods.

\section{Future Directions}

\subsection{Supporting Expressive Gaze Behaviors}

This dissertation has proposed directed gaze as a model for synthesis of many kinds of humanlike communicative behaviors. However, it by no means an all-encompassing model of human gaze, as it excludes gaze movements such as smooth pursuit, probabilistic saccades, eye blinks, and minor eye movements such as tremors and pupil dilation. Some of these movements are simulated in prior work (e.g., smooth pursuit~\citep{yeo2012eyecatch}) and a possible future extension of our own work would be to integrate them into our gaze authoring approach. This task would be far from straightforward, however, as these other movements require not just new synthesis methods, but also new movement parametrizations and new motion analysis and inference models.

Major advantages of directed gaze as a model of attending behavior are its simple parametrization and uniform nature---the entire behavior is modeled as a sequence of gaze shifts parametrized by timing, target, and head and torso alignments. These advantages come with a downside---the outputs of our gaze synthesis methods are relatively predictable and lack expressiveness in head and body movements as well as eye movements. The lack of variation in body movements is less of a problem in gaze authoring, where much of the expressiveness is already encoded in the body motion, but becomes more noticeable when animating virtual agents, which can appear static and robot-like as a result.
On the other hand, the often sparse and predictable eye movements yielded by our gaze inference model~\ref{sec:GazeInference}, while plausible, do not approach the rich expressivity seen in professionally animated characters, where ``each new thought triggers an eye movement''~\citep{maestri2001digital}.

Adding expressivity to our methods is difficult to do without sacrificing generality. Rich patterns found in human gaze are very context-dependent---e.g., gaze aversions and referential gaze that occur in coordination with cognition and speech production---and any model that produces such patterns much consider high-level state beyond a simple gaze shift sequence.
Regardless, future work should explore integration of high-level gaze models with our gaze authoring workflow to enrich the output in specific scenarios. We have taken first steps toward that goal in the current implementation: we add probabilistically generated eye blinks and saccades to the output motion in a post-process. We generate the blinks as described by~\citet{peters2010animating}, while for saccadic gaze we use the Eyes Alive model~\citep{lee2002eyes}.

\subsection{Nonverbal Behaviors in Multiparty Interactions}

Directed gaze is an effective model for realizing several nonverbal behaviors that occur in multiparty interactions. Chapter~\ref{cha:GazeFooting} describes a set of gaze and body reorientation mechanisms for virtual agents allowing them to manage conversational footing in interactions with one or more users. It gives empirical evidence of the importance of such behaviors for virtual agent interaction in immersive VR. While still preliminary in many ways, this work motivates the need for virtual agent systems that can effectively engage with users in multiparty interactions and exposes a number of research challenges. To be effective at managing multiparty interactions, agent systems need to use information about users' engagement intents, attention, and environment layout, to make decisions about the agent's positioning, reorientation, and gaze patters, such that desired communicative goals are achieved. Possible goals include: (1) establish and reconfigure the conversational formation to make optimal use of available space; (2) use appropriate gaze behaviors and proxemics to maximize users' comfort level; (3) manage conversational footing to optimize users' level of participation and sustain their interest; (4) manage turn-taking to avoid ambiguities and breakdowns in communication.

A particularly salient lesson from our project is one of the need to make inferences about users' intents in order to avoid communication failures and keep them engaged. Some of the most egregious mistakes committed by the agent involved cutting off the participant due to an erroneous belief that they were finished with their utterance, which could have been avoided if the agent had possessed an awareness of the participant's floor-holding behaviors, such as averted gaze. Similarly, the experience of female participants may have been negatively affected when the agent engaged in too much mutual gaze. To avoid such effects, embodied agent systems should be more sensitive to users' intimacy-regulating signals---gaze aversions and interpersonal distance---and adjust their own behavior in response. Future work will need to take better advantage of the sensing capabilities afforded by modern VR devices---e.g., head and eye tracking---to characterize user behavior and make appropriate decisions about the agent's behavior based on those inferences.

\subsection{Scalable Authoring of Attending Behaviors}

Chapter~\ref{cha:GazeAuthoring} of this dissertation has introduced the first animation authoring approach designed specifically for gaze, built around directed gaze as the underlying model. While the approach succeeds in its goal of reducing authoring effort while producing motion at an acceptable level of quality, it can still be improved upon in many ways, from the accuracy of initial inference to the computational efficiency of editing. The latter limitation is particularly salient: the current implementation of the approach cannot provide truly interactive editing due to the alignment parametrization and feed-forward design of the gaze shift model, which requires the entire motion to be resynthesized to see the exact editing result.

The current authoring approach is not yet comprehensive enough to allow universal editing of characters' attending behaviors. One limitation is the lack of support for whole-body orientation. Editing whole-body orientation as well as eye gaze is challenging since it requires editing the entire underlying body motion, which involves root movements and complex patterns of end-effector constraints. Existing methods such as motion path editing~\citep{gleicher2001path} may provide a good starting point for implementing such features.

The gaze authoring approach has the capability to generate a large amount of gaze motion automatically, adding editable gaze behaviors to an entire scene at the click of a button. This makes it an appealing option for animating the gaze of groups and crowds of interacting characters. However, attending behaviors in such contexts are more complex than simple gaze shifts. Groups of interacting people arrange themselves into conversational formations~\citep{kendon2010spacing}, they maintain mutual gaze and interpersonal distance appropriate to their situation and intimacy level~\citep{argyle1965eyecontact}, and their gaze patterns reflect a number of perceptual, attentional, and social-cognitive processes that go on in parallel. To synthesize such behaviors automatically and enable their convenient editing entails a slew of research challenges. These include computational modeling of the principles that govern human social behavior, designing editing handles that map intuitively to parameters of large-scale social interactions, and developing new motion synthesis and editing methods that can handle complex, multi-character, spatial and temporal constraints. The methods presented in the current work represent but one step toward a more ambitious goal---supporting authoring of large-scale attending behaviors for interacting characters. 