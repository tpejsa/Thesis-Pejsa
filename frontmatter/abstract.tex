Gaze is a fundamental component of communicatively effective character animation. In animated film and other non-interactive media, a character's gaze can capture the viewer's attention and convey the character's personality and intent, while interactive virtual characters can use gaze cues to enhance their social interactions with users. In all these contexts, the character's nonverbal attending behavior is realized through coordinated movements of the eyes, head, torso, and the whole body. Producing such richly articulated movements in a traditional animation workflow requires considerable effort and expertise. The current work introduces a set of methods that simplify and automate the process of synthesizing biologically plausible, communicatively effective gaze for both interactive and non-interactive scenarios. The key idea is to model the character's gaze behavior as \emph{directed gaze shifts}---coordinated, rotational movements of the eyes, head, and body toward targets in the scene. A model is proposed for computational synthesis of directed gaze shifts, that allows parametric control over head and body coordination. Furthermore, methods are introduced for low-cost authoring of gaze animation for a wide range of characters and scenarios. These include methods for adapting gaze shift kinematics to non-human and stylized character designs, as well as a comprehensive animation authoring approach, which uses directed gaze shifts as building blocks. Experiments are presented showing how these methods facilitate the creation of communicatively effective attending behaviors of animated characters, while scaling well across a wide range of scenarios and character designs.
