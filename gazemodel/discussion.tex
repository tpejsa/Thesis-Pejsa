Directed gaze shifts are a fundamental building block of effective attending behaviors, without which many aspects of human cognition and social interaction would be impossible. Understanding a person's gaze cues is achieved by integrating information about their eye gaze, but also their head and body orientation. To make animated character communicate effectively using gaze, we require methods for synthesizing shifts in gaze direction and body orientation in a coordinated, biologically plausible and---most importantly---controllable fashion. This chapter of my dissertation has introduced a computational model for synthesis of such gaze shifts. The model enables an animator, designer, or high-level behavior model to parametrically control how the eyes, head, and body coordinate during the gaze shift. The model exposes head, torso, and whole-body alignment parameters, which specify how much these body parts will move during the gaze shift. A study with human participants showed that the model generates gaze that conveys attention direction as accurately as real human gaze and achieves improvements in naturalness thanks to the use of kinematics derived from human neurophysiology. A second study evaluated one of the model's unique features---parametric control over upper body alignment---and showed that people are sensitive to variations in body orientations produced by its manipulation, such that gaze shifts toward the same target are interpreted significantly differently depending on the degree of body reorientation.

The proposed model has a wide applicability, which I demonstrate throughout this dissertation. It is a suitable building block for the gaze behaviors of embodied conversational agents and game characters. Designers can compose these behaviors manually to accompany a predefined interaction, as we did in the preceding studies. They could also be generated automatically by a high-level behavior model, which would output gaze targets and gaze shift timings and supply them to the gaze shift model; one such application are the conversational gaze mechanisms built for the study in Chapter~\ref{cha:GazeFooting}. Finally, the gaze shift model is also suited for animation authoring, due to its intuitive parametrization and the ability to produce biologically plausible gaze shifts across the entire parametric range. For those reasons, it is employed as a building block of the gaze authoring approach presented in Chapter~\ref{cha:GazeAuthoring}.

The model does have several limitations. As described in the current chapter, it is only applicable to characters with realistic, humanlike designs---a limitation addressed in the research presented in Chapter~\ref{cha:StylizedGaze}. Furthermore, the head and body movements, while based on fairly accurate approximations of measurements acquired from real humans, can look somewhat robotic and repetitive compared to biological human motion. Human motion contains variations resulting from personality differences, emotional state, and other, unaccountable factors, which the proposed model makes no attempt to simulate. One possible strategy to achieve more expressive movements is to use the approach by~\citet{lance2010expressive}. They encode variations found in motion capture data in displacement maps~\citep{witkin1995motion} and apply them to procedurally synthesized gaze shifts. Such an extension is outside the scope of the current work, but certainly presents a viable option for future implementers. 