Embodied conversational agents have great potential as user interfaces across a range of application domains including, education, healthcare, service industries, and entertainment. This potential stems from their ability to facilitate natural interaction with humans through the use of communicative cues, such as speech, facial expressions, gaze, and gestures. By employing such cues in interaction, agents can trigger in people social and cognitive processes that lead to improved task outcomes. It has been shown that an agent employing a range of communicative cues can motivate people \cite{mumm2011motivational}, help them learn better \cite{lusk2007animated}, and teach them social skills \cite{tartaro2007authorable}. For this reason, we aim to construct computational models of human communicative behaviors and study how these models might be employed to build more effective ECAs.

In this work, we focus on modeling \emph{gaze shifts}: coordinated movements of the eyes, head, and body towards objects and information in the environment \cite{zangemeister1982types,mccluskey2007monkeys}. Gaze shifts serve as fundamental units of gaze behavior, which itself plays a key role in human communication. In particular, gaze is used to convey and manage attention in social interactions between people and enable beneficial social and cognitive processes. The goal of the current work is to enable designers to compose agent's gaze shifts into larger gaze behaviors that effectively manage people's attention and thus trigger these social and cognitive processes.

To illustrate the types of effects we seek to enable in human interactions with ECAs, we consider a scenario during which two humans engage in a conversation. As they talk, they are likely to gaze into each other's eyes, a state referred to as \emph{mutual gaze} or eye contact. Mutual gaze creates a sense of being attended to and triggers positive social and cognitive effects: the two individuals engaging in eye contact will see each other as more likeable and truthful \cite{goldberg1969visual,argyle1976gaze}, and they will also be more likely to recall information discussed during the conversation \cite{otteson1980effect,sherwood1987facilitative}. As their interaction progresses, one person might make verbal references to an object in their shared environment, e.g., a map. While doing so, they will gaze at the map, signalling to the other person that their attention has shifted to the map. The other person's own gaze will then shift to the map in a seemingly automatic fashion \cite{frischen2007gaze}. These two individuals are said to be engaging in \emph{joint attention} \cite{dentremont2007early} via a shared focus on the map, which facilitates the learning of information by building associations between verbal references and the environment \cite{woodward2005infants}---in our example, the second person may recall locations from the map as a result of joint attention.

The work presented in this paper consists of two components. The first component is a novel computational model for synthesis of an agent's gaze shifts, which we describe in Section~\ref{sec:GazeModel}. The model is characterized by two key properties. First, it is a procedural animation model informed by neurophysiological measurements of gaze that enable it to synthesize natural-looking gaze shifts without the use of motion capture data. Second, the model can generate not only eye motion but also coordinated motion of the eyes, head, and upper body. Much of the prior research on gaze modeling for ECAs has focused only on eye gaze; however, research in neurophysiology and the social sciences has shown that a person tends to also reorient their head and body when shifting their attention towards an object or person of interest and that an observer infers the direction of this attention not only from the eyes but also by aggregating information about the eye, head, and body orientation of the other individual \cite{perrett1994understanding,hietanen1999does,hietanen2002social,pomianowska2011socialcues}. For example, if a person gazes at an observer ``out of the corner of the eye'' while their head remains turned in another direction, the observer will respond to this attention cue differently than if the person completely turned their eyes and head in the observer's direction \cite{hietanen1999does}. Our model supports a broad range of attention cues by incorporating coordinated movements of the eyes, head, and upper body. These movements are parametrically controllable---the model exposes a set of \emph{alignment parameters}, which the designer can use to define how much the agent's head and trunk will participate in a gaze shift. Figure~\ref{fig:Examples} illustrates the usage of these parameters to produce significantly different gaze shifts. We validated the communicative accuracy and naturalness of the model in a study with human participants, which we label Study 1 and present in Section~\ref{sec:Study1} of the paper.

\begin{figure*}
\centering
\includegraphics[width=1\textwidth,page=1]{Figures/tiis14-pejsa.pdf}
\caption{Examples of gaze shifts synthesized using our model: (1) Initially the agent maintains eye contact with the observer.  (2) Gaze shift to the side with low value of the head alignment parameter. (3) Gaze shift in the same direction, but with high head alignment value. (4) Gaze shift in the same direction with a small amount of trunk motion. (5) As before, but with a high amount of trunk motion.}
\label{fig:Examples}
\end{figure*}

The second component of this work is a pair of studies with human participants (labeled Study 2 and Study 3, respectively) in which we examined how control over eye, head, and upper body coordination in gaze shifts enables agents to communicate more effectively. In these studies, the agent used gaze shifts generated by our gaze model to manage the participant's attention in order to better convey certain information or influence the participant's judgments of the agent. Study 2 is an educational task in which we manipulated the eye-head coordination of a virtual teaching agent to produce two distinct patterns of gaze behavior. One of these patterns emphasized information in the environment, thus facilitating establishment of reference and joint attention, which led to improved information recall, while the second pattern emphasized eye contact, which facilitated feelings of affiliation and led to more positive perceptions of the agent. The setting for Study 3 was a virtual art gallery, and we manipulated the amount of upper body movement in the agent's gaze. Gaze shifts with more upper body reorientation would signal a major shift in attention to an art exhibit in the environment and would lead to a stronger perception of interest in that exhibit. The common feature of both studies is the use of alignment parameters to control the amount of head and upper body movement in gaze. Increased movement of the head and upper body signaled a greater shift in attention and augmented the social and cognitive effects produced by the gaze cue.

In summary, our work makes the following contributions:
\begin{enumerate}
\item A procedural, empirically validated model for gaze shift synthesis that enables parametric control over the coordination of eye, head, and upper body movements.
\item A demonstration, in a study with human participants (Study 2), of how control over the agent's eye-head coordination enables the agent to convey information more effectively or establish better affiliation with people interacting with the agent.
\item A demonstration, in a study with human participants (Study 3), of how control of the agent's upper body movements enables the agent to better communicate interest in nearby objects to people interacting with the agent.
\end{enumerate} 

We emphasize that the latter two contributions are distinct from the model that makes up the first contribution. The studies in question were designed not merely to evaluate the current model but to explore the design space of ECA's gaze. Their value is in showing how more effective attention management mechanisms can be constructed from gaze shifts with varying coordination properties. In these studies, we utilized our own gaze model to synthesize such shifts, but we expect their results to generalize to other models that might offer a comparable degree of parametric control.