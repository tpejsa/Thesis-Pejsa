Development of the gaze shift model was followed by an empirical evaluation of its communicative effectiveness and biological plausibility. We conducted a study with human participants where we had participants view videos of an animated character or living human gazing at colored object on a table. The character would first gaze at the camera, shift its gaze toward an object, and then look back at the camera. Gaze shifts were synthesized using our model or a state-of-the-art model~\citep{peters2010animating} with a similar parametrization. Upon watching a gaze shift, the participants would guess the target object and rate the character on a set of scales. We expected that by gazing at an objects and back, the character would initiate joint attention with the participant and thus allow them to infer what object the character is looking at. The study had two objectives. First, we needed to confirm that our model is able to accurately communicate the character's attentional focus. Second, we tested if the gaze shifts generated by our neurophysiologically-based model were seen as more natural than those generated by a state-of-the-art procedural model.

Study results show that our model achieves the communicative accuracy of real human gaze and of the state-of-the-art model. Furthermore, the results partially support our expectation that the model would achieve better naturalness ratings than the state-of-the-art model---the gaze shifts generated by our model were rated as marginally more natural overall, while a subset of the gaze shifts with full head alignment were rated as significantly more natural. In addition, we explored the effect that participant gender and the gender of the virtual character had on our measures, as gender is known to influence gaze perception in artificial agents~\citep{mutlu2006storytelling}.
%The evaluation involved only gaze shifts with eye-head movements but no upper body movements. We do not believe that the current evaluation could be meaningfully extended to include upper body movements, as there is no comparable state-of-the-art gaze model with support for upper body movements that we could compare our model against. A more meaningful evaluation would focus on determining whether the magnitude and mere presence of body movements have effects on gaze direction cueing, which is implicitly confirmed in the results of the third study (Section~\ref{sec:Study3}).

\subsection{Setup and Task}

\begin{figure}
  \centering
  \includegraphics[width=0.85\textwidth,page=7]{gazemodel/Figures/tiis14-pejsa.pdf}
  \caption{Experimental conditions in the study of gaze shift accuracy and naturalness.}
  \label{fig:ModelEvalSetup}
\end{figure}

Participants watched a series of videos in which either a virtual character or a human confederate shifted their gaze toward one of sixteen objects arranged on a desk. This simplified scenario allowed us to focus our evaluation on the effectiveness and naturalness of gaze shifts while minimizing contextual and interactional factors. It also facilitated the matching of animated and real-world conditions. Participants experienced the videos from the perspective of a partner who would be seated across from the character. The objects on the desk were separated into four groups and distinguished by color and shape. Still images from the videos are shown in Figure~\ref{fig:ModelEvalSetup}.

The faces of the virtual characters used in the study were modeled using Singular Inversions FaceGen\footnote{http://www.facegen.com/}. The faces and the eyes of both characters had approximately the same dimensions. The characters were scaled and positioned such that their size and location on the screen were as close as possible to those of the human confederates. The gaze shift model and the experimental task were implemented in C++; we used OGRE engine for rendering.

\subsection{Study Design}

We conducted a 2x2x8 factorial experiment with a split-plot design. Our factors were:

\begin{enumerate}
\item \emph{Participant gender} -- Two levels varying between participants.
\item \emph{Character's gender} -- Two levels varying between participants.
\item \emph{Gaze model type} -- Eight levels varying within participants.
\end{enumerate}

The model type factor consisted of \emph{six} comparison conditions for gaze shifts generated by the state-of-the-art head propensity model~\citep{peters2010animating} and those produced by our model, as well as \emph{two} control conditions. The comparison conditions were created as follows. For each model, we manipulated the model parameters to create \emph{three} distinct comparison conditions with different head alignment/propensity levels---0\%, 50\%, or 100\%---adding up to a total of six such conditions in the experiment. This manipulation enabled us to determine how changes in the model parameters affected the communicative accuracy and perceived naturalness of the gaze shifts. In addition, the model type factor included two control conditions. In the first control condition, the virtual character was replaced with a male or female human confederate, who presented gaze shifts toward the object on a desk in front of him/her. The second control condition involved a virtual character maintaining gaze toward the participant without producing any gaze shifts.

\subsection{Procedure}

Each participant was shown 32 videos of a virtual character or human confederate. In the videos, the character (confederate) gazed toward the participant, announced that they were about to look toward an object with a specific color on the table, shifted their gaze toward the object, and moved their gaze back toward the participant. Following each video, the participants guessed which object the character had gazed at, upon which they filled out a questionnaire for subjective evaluation. Participants observed each gaze model generating gaze shifts toward all object types, colors, and positions. We randomized the order in which the participants observed the videos to minimize transfer effects. Each video was 10 seconds long, with the overall study lasting approximately 20 minutes.

\subsection{Participants}

Ninety-six participants (50 males and 46 females) took part in the study. The participants were recruited through Amazon.com's Mechanical Turk online marketplace, following crowdsourcing best practices to minimize the risk of abuse and to achieve a wide range of demographic representation~\citep{kittur2008crowdsourcing}. The participants received $\$$2.50 for their participation.

\subsection{Measures}

The study used two dependent variables: \emph{communicative accuracy} and \emph{perceived naturalness}. Communicative accuracy was measured by capturing whether participants correctly identified the object toward which the gaze shift of the human confederates or the virtual characters was directed. To measure perceived naturalness, we constructed a scale using four items that measured \emph{naturalness}, \emph{humanlikeness}, \emph{lifelikeness}, and \emph{realism}. These items were highly correlated (Cronbach's $\alpha = .94$). Participants rated each video for each item using a seven-point rating scale.

\subsection{Results}

We analyzed our data using a mixed-model analysis of variance (ANOVA). Overall, model type had a significant effect on communicative accuracy, $F(7, 47.21) = 20.074, p < .0001$, and perceived naturalness, $F(7, 47.21) = 204.88, p < .0001$. In addition, for each measure we performed a total of \emph{six} comparisons across conditions between our model and each control as well as our model against the baseline. We controlled familywise error rate using Dunn-\v{S}idak test, with adjusted significant alpha of .00851 and marginal alpha of .0174. The results of these comparisons are presented below.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth,page=8]{gazemodel/Figures/tiis14-pejsa.pdf}
  \caption{Results of Study 1, aggregated across participant and character gender. The baseline model refers to the head propensity model we used for comparison. From left to right: communicative accuracy, perceived naturalness aggregated across gaze shifts with different head alignments/propensity values, and perceived naturalness for the subset of gaze shifts with 100\% head alignment/propensity.}
  \label{fig:ModelEvalResults}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.85\textwidth,page=9]{gazemodel/Figures/tiis14-pejsa.pdf}
  \caption{Communicative accuracy and perceived naturalness across characters with male and female features.}
  \label{fig:ModelEvalResultsGender}
\end{figure}

\emph{Communicative Accuracy} -- Comparisons across conditions found no significant differences in the communicative accuracy of the gaze shifts produced by our model, aggregated across all levels of head alignment, and those produced by human confederates. Similarly, no differences in accuracy were found between our model and the head propensity model, neither when aggregating across all levels of head alignment nor when comparing corresponding head alignment levels. The results suggest no significant difference in accuracy of the gaze shifts generated by our model, those performed by human confederates, and those generated by the state-of-the-art model. Unsurprisingly, our model achieves significantly higher accuracy than the character control (virtual character producing no gaze shifts), $F(1, 122.9) = 11.23, p = .0055$.

\emph{Perceived Naturalness} -- Comparisons across conditions showed that participants rated the gaze shifts generated by our model, aggregated across all levels of head alignment, as marginally more natural than those generated by the head propensity model, aggregated across all levels of head propensity, $F(1, 2829) = 6.037, p = .014$. Pairwise comparisons across the two models with corresponding head alignment/propensity values showed that, at 100\% alignment/propensity, participants rated gaze shifts produced by our model to be significantly more natural than those generated by the Peters model, $F(1, 2826) = 7.88, p = .005$. Furthermore, our model was rated as significantly less natural than the human control, $F(1, 122.3) = 1108.247, p < .0001$, and significantly more natural than the virtual character control, $F(1, 122.9) = 193.21, p < .0001$.

Results from the communicative accuracy and perceived naturalness measures are illustrated in Figure \ref{fig:ModelEvalResults}.

\emph{Gender} -- Participants rated gaze shifts performed by the character with female features as significantly more natural than those performed by the character with male features, $F(1, 1958) = 17.17, p < .001$. On the other hand, communicative accuracy of the gaze shifts performed by the character with male features was significantly higher than that of the gaze shifts performed by the character with female features, $F(1, 1958) = 4.85, p = .028$. Finally, we found a marginal interaction between participant gender and the gender features of the virtual character on the naturalness measure, $F(1, 1958) = 3.20, p = .074$. Figure \ref{fig:ModelEvalResultsGender} illustrates these results.

\subsection{Discussion}

Study results show that gaze shifts generated by our model communicate attention direction as accurately as do human gaze shifts and those generated by the state-of-the-art model. The mean accuracy for both models and the human control is below 65\%, indicating that discriminating the correct gaze target using gaze as a visual cue is a difficult task for people. Comparable results are obtained by~\citet{bailly2010gaze}, who utilized a similar experimental protocol. In their study, participants inferred the correct object with 85\% accuracy when aided by virtual character's gaze cues---the higher accuracy is likely due to lower spatial density of gaze targets in their setup.

Furthermore, the results of the current study show that our model produces marginally more natural gaze shifts than the state-of-the-art model across the range of the head alignment parameter, as well as significantly more natural gaze shifts at 100\% head alignment. We attribute the difference in naturalness to the neurophysiological basis of our model, which should result in more realistic motions. However, significant increase in naturalness is only achieved for gaze shifts at full head alignment; we believe this is because those gaze shifts are dominated by head motion, which is slower and more visually prominent than the quick, subtle eye movements.

Finally, we found significant differences in naturalness and communicative accuracy between the two levels of character gender, as well as marginal interaction between character and participant gender. We believe these effects are caused by stylistic differences in character design. For example, the female character's gaze may be perceived as more natural because the character is more aesthetically appealing than the male character. Moreover, small variations in eye size and shape between the two characters could also affect people's perceptions of their gaze. We hypothesize that more consistent character design would reduce the effects of gender on experimental results. 