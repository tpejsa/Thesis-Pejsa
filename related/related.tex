\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Related Work}

Gaze has been a subject of research in several academic disciplines. Social sciences have focused on studying the functions of gaze in human perception, communication, and cognition. In neurophysiology, there has been a focus on describing gaze in terms of its kinematic properties, such as gaze shift amplitudes and velocities. These two bodies of work are complementary and provide a solid basis for computational modeling of gaze. Gaze synthesis methods presented in this work are derived from quantitative measurements of gaze in neurophysiology and perceptual psychology, while the hypotheses about social and perceptual effects of gaze tested in my studies are informed by research in the social sciences.

In this chapter, I review the social sciences and neurophysiology research on gaze. In addition, I discuss prior work on computational synthesis of gaze motion for animated characters and embodied conversational agents.

\section{Gaze in Human Communication}

Humans have an innate ability to track the gaze of others and infer the focus of their social attention, which emerges in the first year of life~\cite{scaife1975infant,vecera1995detection}. Numerous studies (surveyed in ~\cite{langton2000eyes}) have shown that, when observing the gaze others, humans reflexively reorient their visual attention in the same direction. This automated reorienting has a neurological basis---human brains have specialized structures for processing gaze direction~\cite{emery2000eyes}. Studies have shown that gaze direction is inferred not just from the eyes, but by integrating information about head and body pose~\cite{hietanen1999does,langton2000eyes,hietanen2002social,pomianowska2011socialcues}.

People are particularly sensitive to being gazed at by others~\cite{argyle1976gaze}. Being gazed at can make a person feel uncomfortable, but it can also lead to positive social and cognitive effects. Increased gaze contributes to immediacy---the feeling of physical or psychological closeness between people~\cite{mehrabian1966immediacy}. Immediacy correlates positively with information recall~\cite{otteson1980effect,sherwood1987facilitative,fullwood2006effect,kelley1988effects}---e.g., a lecturer looking at a student can help them remember more information from the lesson.
\emph{Mutual gaze} is the state where two people engage in eye contact~\cite{argyle1976gaze}. People who make a lot of eye contact in social interactions are seen as more likeable and credible~\cite{beebe1976effects,argyle1976gaze}. Sometimes a person might break eye contact in order to look at an object of information in the environment, before reestablishing mutual gaze. The viewer's attentional focus is then reflexively redirected toward the same object. The two individuals are said to be engaging in \emph{joint attention}~\cite{dentremont2007early}, which is an important process for the grounding of linguistic references and communication of intent~\cite{Hanna and Brennan 2007, Preissler and Carey 2005,mumme2007actions}---e.g., people might look at an object as they talk about it.

Attending behaviors, realized through shifts in gaze and body orientation, underpin many important regulatory mechanisms in social interactions between two or more parties~\cite{heylen2006head}. The speaker looks at listeners to indicate the addressees of their utterance, to request backchannels (e.g., a nod or verbal confirmation of understanding), and to signal the end of their conversational turn. Listeners look back at the speaker to indicate they are paying attention, to signal their understanding, or to request the conversational floor. Intimacy regulation is achieved using gaze in conjunction with proxemics~\cite{argyle1965eyecontact}: more intimate conversations are characterized by increased mutual gaze and lower physical distance between the participants. Finally, gaze and body orientation also serve as nonverbal cues of participants' conversational roles (what Goffman termed as ``footing''~\cite{goffman1979footing}), which is particularly important in multiparty interactions, where participants' roles can range from speakers or addressees, to bystanders, to overhearers and eavesdroppers. In general, increased gaze and body orientation towards a participant signals a higher degree of participation in the interaction.

Body reorientation is an important, but often overlooked behavior in multiparty interactions. Changes in body orientation occur relatively rarely during interaction compared to gaze shifts involving eye and head movements and indicate larger shifts in attention and mutual involvement~\cite{kendon1973visible,schegloff1998bodytorque}. Body orientation shifts are likely to occur at the start of an interaction, or when a new participants joins in. One reason for body reorientation is physical comfort---people like to avoid head rotations greater than 90$^{\circ}$ when gazing at someone~\cite{kendon1973visible}. Another one is to signal a shift in participants' conversational roles or effect a reconfiguration of the conversational formation---e.g., when a new party joins the conversation, the current participants might turn towards them~\cite{kendon2010spacing}.

The above findings have several implications for character animation. First, they attest to the important role of gaze behaviors in human communication and thus motivate the need for computational synthesis of natural, controllable gaze shifts. Second, they demonstrate that coordinated control over gaze direction and body orientation is needed to articulate the full range of attending behaviors on animated characters. Third, they suggest that camera position matters when designing the character's gaze. How an animated character directs its gaze and orients its body relative to the camera will have significant effects on the viewer's experience.

\section{Gaze Modeling in Neurophysiology}

Research in neurophysiology has studied how humans and other primates carry out gaze shifts in a tightly connected dynamic process by coordinating eye, head, and body movements  ~\cite{zangemeister1982types,andredeshays1988eyehead1,barnes1979vor,freedman2000coordination,uemura1980eyehead,mccluskey2007monkeys}. Researchers measured these movements in highly controlled experiments, obtaining numeric data about kinematic properties such as movement range~\cite{guitton1987gaze} and eye and head velocities~\cite{guitton1987gaze,freedman2000coordination,barnes1979vor,uemura1980eyehead}. The model for gaze shift synthesis described in Chapter~\ref{cha:GazeShiftModel} is based on their results.

Kinematics of coordinated eye and head movements are reported by~\citet{guitton1987gaze},~\citet{freedman2000coordination},~\citet{barnes1979vor}, \citet{uemura1980eyehead}, and others. Eye and head movements in gaze are tightly coupled and significantly affect each other. There exists a linear relationship between head velocity and eye movement amplitude in gaze shifts~\cite{barnes1979vor,uemura1980eyehead}. Furthermore, head movement amplitude decreases as eye movements start at increasingly contralateral positions (i.e., oriented away from the target in relation to head direction). Shifts that start at such positions require that the eyes contribute more to the shift~\cite{mccluskey2007monkeys}. The degree to which individuals use their heads in performing a gaze shift is highly idiosyncratic. Neurophysiological research literature describes some people as ``head-movers'', i.e., individuals who move their head fully to align with the gaze target every time, and some as ``non-head-movers''~\cite{fuller1992head}. From a biomechanical standpoint, humans should universally be ``non-head-movers'', as fully moving the head---which is almost a hundred times heavier than the eyes---is not an economical solution~\cite{kim2007head}; therefore, the human tendency to move the head more than necessary during gaze shifts can be attributed to the role of head orientation in signalling attention.

Shifts in attention direction are accomplished not only through movements of the eyes and the head but also through movements of the body. Unfortunately, studies that have attempted to measure the kinematics of these movements are few. \citet{mccluskey2007monkeys} measured upper body movements in primate gaze, finding that body contribution to gaze shifts increased with gaze shift amplitude and that the upper body trailed eye and head movements with substantial latency. Based on these results, they concluded that body movements in gaze shifts are part of a series of coordinated motor events triggered when primates reorient their gaze. \citet{hollands2004wholebody} obtained similar findings for coordination of the eyes, head, and feet during whole-body reorientation.

\section{Gaze Motion Synthesis}

Gaze behaviors for animated characters are synthesized using computational models of human gaze. I distinguish between low-level and high-level models. \emph{Low-level} gaze models synthesize atomic gaze movements (such as gaze shifts) by simulating the kinematics of real human gaze, whereas \emph{high-level} models synthesize complex gaze behaviors (composed of many low-level movements) that match the current situation. Put more simply, high-level models determine \emph{where} the character should look, while low-level models determine \emph{how} to look there. The gaze shift synthesis model presented in Chapter~\ref{cha:GazeShiftModel} is an example of a low-level model, whereas the gaze inference model in Section~\ref{sec:GazeInferenceg} can be classed as high-level.

In the remainder of the section, I give an overview of gaze synthesis models for animated characters and embodied conversational agents. I focus on models that are the most relevant to the current work---a more thorough survey is provided by~\citet{ruhland2015gazereview}.

\subsection{Low-level Gaze Models}

Low-level gaze models synthesize atomic movements that comprise the overall gaze behavior. These movements are:

\begin{enumerate}
\item \textbf{Saccades} -- Saccades are rapid eyeball movements toward targets of interest. Saccades are what people most commonly associate with gaze.
\item \textbf{Vestibulo-ocular reflex (VOR)} -- The VOR stabilizes the eyes during head motion, ensuring they remain fixated on the target and the image remains stable.
\item \textbf{Vergence} -- In almost all gaze movements, both eyes move in the same direction. Vergence is the exception---when the eyes focus on an object that lies along the visual midline, they need to move in the opposite direction.
\item \textbf{Smooth pursuit} -- Smooth pursuit executes a series of saccades to visually track a slow-moving object or read a line text, while ensuring the image remains stable.
\item \textbf{Gaze shifts} -- Gaze shifts are coordinated, rotational movements of the eyes, head, and body toward targets of interest.
\item \textbf{Eye blinks} -- Eye blinks are rapid eyelid-closing and opening movements. They typically occur spontaneously and provide lubrication to the eye, though they also occur reflexively (e.g., to shield the eye from injury) and voluntarily (as a form of nonverbal communication, e.g., winks).
\end{enumerate}

Low-level gaze models generally focus on simulating saccades and gaze shifts. Since saccades can be thought of as gaze shifts comprised only of eye movements, models for the latter implicitly support the former. The VOR is fairly trivial to implement, so almost all gaze models incorporate it. Likewise, all models implicitly simulate vergence, because they compute each eye's orientation separately. Smooth pursuit is a relatively infrequent movement, so it is rarely simulated---one notable exception is the smooth pursuit implementation by~\citet{yeo2012eyecatch}. Finally, almost all gaze models also integrate eye blinks, which are essential for believable animation---e.g., ~\citet{peters2010animating} integrate a statistical model of gaze-evoked blinks.

Gaze shift and saccade models can be classified into data-driven models~\cite{heck2007automated,lance2010expressive,lee2002eyes,deng2005automated,ma2009natural,le2012live}, which use recorded, annotated, or motion-captured data to synthesize novel gaze movements, and procedural models~\cite{peters2010animating,thiebaux2009realtime}, which synthesize gaze movements using equations that describe their kinematics (often derived from neurophysiology literature). Data-driven models typically produce more natural gaze motions, while procedural models offer a greater degree of parametric control. Saccadic gaze models are generally data-driven; they use statistical~\cite{lee2002eyes,ma2009natural,le2012live} or sampling-based~\cite{deng2005automated} models of gaze data to synthesize saccadic sequences given inputs such as head motion and speech. Among gaze shift models, the Expressive Gaze Model (EGM)~\cite{lance2010expressive} is an example of a hybrid model, which produces procedurally generated, neurophysiologically-based eye movements in conjunction with motion-captured head and torso movements.

Gaze shift models can also be classified into eye-head models, that support only coordinated eye and head movements, and upper-body models, that also support torso movements. The latter include: the EGM~\cite{lance2010expressive}, which animates the torso movements using motion capture data; the Parametric Gaze Map by~\citet{heck2007automated} synthesizes gaze shifts by interpolating motion captured poses; the model by~\citet{grillon2009crowds}, which uses an IK solver to turn the upper body towards the gaze target. To my knowledge, there are no prior gaze models that explicitly support whole-body movements, although both~\cite{heck2007automated} and~\cite{grillon2009crowds} can adapt existing body motion with upper-body turns.

Parametric control is an important characteristic of gaze models that enables the specification of gaze movement properties. For gaze shift models, a minimal parametrization allows the specification of where to look, either as the gaze target location (3D point) or direction (3D vector or a pair of angles). In addition, some models~\cite{peters2010animating,lance2010expressive,thiebaux2009realtime} allow parametric control over head movements via a parameter typically dubbed ``head alignment'' or ``head propensity'', which specifies how much the head contributes to the gaze shift. There is evidence that more head movement in the gaze shift yields a stronger attention cue; e.g., one study~\cite{andrist2012designing} has shown that a virtual agent looking at information in the environment with more head alignment facilitates greater recall of that information by the participant. Only one prior model allows equivalent parametric control over torso alignment---the SmartBody Gaze Controller~\cite{thiebaux2009realtime}.

The gaze shift model presented in the current work is a procedural, neurophysiologically-based model. To my knowledge, it is the first model that supports coordinated eye, head, torso, and whole-body movements. It integrates a turn-in-place controller to support whole-body turns, and it exposes parameters for controlling head, torso, and whole-body alignment in gaze shifts. As such, it is currently the most complete computational implementation of movements that comprise human attending behaviors.

\subsection{High-level Gaze Models}

High-level gaze models determine where and when the character should look given the current situation in the scenario. They generate complex gaze behaviors comprised of atomic gaze movements, which may be synthesized by low-level gaze models. More formally, high-level gaze models work by mapping high-level state inputs to timings and spatial targets of gaze movements using some internal logic, which usually takes the form of heuristic rules (e.g., always look at an object before picking it up) or statistical models derived from real-world data (e.g., if you are the speaker, look at the addressee's face 26\% of the time). Elements of high-level state might include the character's goals and intentions (e.g., whether the character is about to pick up an object), their emotional state, the state of the virtual environment (e.g., what are the most visually salient objects), and interaction state (e.g., whether the character is the speaker or listener in a conversation.) High-levels models are necessarily context-specific: there are models for conversational gaze~\cite{pelachaud2003modelling,masuko2007headeye,gratch2007rapport,andrist2013aversion,lee2007rickel}, autonomous speaker gaze~\cite{bee2010gaze,zoric2011oncreating,marsella2013virtual}, emotionally expressive gaze~\cite{queiroz2007automatic,lance2010expressive,li2012emotional}, gaze for locomotion and physical tasks~\cite{khullar2001look,mitake2007reactive,huang16planning}, idle gaze~\cite{khullar2001look,peters2003bottomup,mitake2007reactive,peters2008applying,cafaro2009animating,grillon2009crowds,kokkinara2011modelling}, etc. The multitude of high-level gaze models are extensively surveyed in~\cite{ruhland2015gazereview}; here I focus on a subset of gaze models most closely related to the current research.

The gaze authoring approach proposed in the current work (Chapter~\ref{cha:GazeAuthoring}) relies on an automated gaze inference model to generate a best-guess representation of the character's attending behavior. As such, it falls in the category of high-level gaze models. To infer the gaze behavior, the model analyzes the kinematic properties of the body motion, the geometry of the virtual scene, and author-supplied semantic annotations. As such, the model bears similarity to prior gaze models that try to automate the gaze behavior based on scene and task information.

Research on gaze control in cognitive science, surveyed in~\cite{henderson2003human}, has shown that people's gaze is influenced by two mechanisms of attention: the spontaneous \emph{bottom-up} attention, which directs their gaze toward visually salient environment features, and the intentional \emph{top-down} attention, which directs it toward task-relevant objects and other people in the interaction.
Many prior gaze models have focused on automating bottom-up attention, typically to synthesize believable idle gaze of virtual agents and avatars. This is often accomplished by computing a saliency map of the virtual scene from features such as color, contrast, orientation, and motion~\cite{peters2003bottomup,peters2008applying} and using it to predict where the character should look. My gaze inference model (Section~\ref{sec:GazeInference}) performs gaze target inference using a similar representation.
Also related are approaches such as~\cite{cafaro2009animating,grillon2009crowds,kokkinara2011modelling}, which determine the gaze target from spatial and kinematic properties of scene objects and other characters, such as proximity, velocity, and orientation. In contrast, models of top-down attention generate gaze based on the character's goals, intentions, and knowledge. Examples include the Rickel Gaze Model~\cite{lee2007rickel} and models of gaze for virtual demonstrators~\cite{huang16planning}. Hybrid approaches~\cite{khullar2001look,mitake2007reactive} integrate bottom-up gaze with top-down, behavior- and task-driven patterns.

My gaze inference approach differs from the above in that its main objective is to support editing gaze in motion capture data. As such, it seeks to discover an editable specification of the gaze behavior already encoded in the body motion, rather than generate a novel gaze behavior that potentially overwrites the body motion's gaze component. The approach is more closely related to top-down models, since it assumes the body motion encodes the actor's intent (expressed in gaze), and it tries to infer that intent by analyzing available data: head and body kinematics, hand contacts, and author-supplied annotations.

\section{Gaze Motion Editing}

My gaze authoring approach operates by inferring an editable specification of the gaze behavior as a series of gaze shift-fixation pairs, and providing intuitive tools for editing that specification. To my knowledge, there are no prior motion editing methods designed specifically for gaze. In the area of multimodal interaction, there are systems for behavior synthesis which rely on editable representations of the agent's behavior. These systems allow implementers to specify the agent's behavior using text annotations. Examples include BodyChat~\cite{vilhjalmsson1998bodychat}, BEAT~\cite{cassell1999fully}, Spark~\cite{vilhjalmsson2004animating}, and systems based on Behavior Markup Language (BML)~\cite{vilhjalmsson2007bml}. However, these systems are designed for specifying the behaviors of interactive agents rather than animation authoring, so they typically lack the controls and graphical tools needed for intuitive and expressive motion editing.

The gaze authoring approach is related to other model-based editing methods. These methods use an abstracted model to specify what the motion should be and produce the final motion that expresses the author's intent, using techniques such as spacetime optimization, physics-based modeling, and synthesis from priors. Examples include physics-based spacetime methods~\cite{popovic99physically}, motion path editing~\cite{gleicher2001path}, spatial relationship preserving interaction editing~\cite{ho2010spatial}, data-driven hand animation for guitar playing~\cite{elkoura2003handrix}, etc. Furthermore, mine is the first approach for automatically adding gaze animation to motion capture data. Other researchers have considered the analogous problem of inferring missing hand animation, notably \citet{jorg2012finger} and \citet{ye2012hand}.
