\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Related Work}

Gaze has been a subject of research in several academic disciplines. Social sciences have focused on studying the functions of gaze in human perception, communication, and cognition. In neurophysiology, there has been a focus on describing gaze in terms of its kinematic properties, such as gaze shift amplitudes and velocities. These two bodies of work are complementary and provide a solid basis for computational modeling of gaze. Gaze synthesis methods presented in this work are derived from quantitative measurements of gaze in neurophysiology and perceptual psychology, while the hypotheses about social and perceptual effects of gaze tested in my studies are informed by research in the social sciences.

In this chapter, I review the social sciences and neurophysiology research on gaze. In addition, I discuss prior work on computational synthesis of gaze motion for animated characters and embodied conversational agents.

\subsection{Gaze in Human Communication}

Humans have an innate ability to track the gaze of others and infer the focus of their social attention, which emerges in the first year of life~\cite{scaife1975infant,vecera1995detection}. Numerous studies (surveyed in ~\cite{langton2000eyes}) have shown that, when observing the gaze others, humans reflexively reorient their visual attention in the same direction. This automated reorienting has a neurological basis---human brains have specialized structures for processing gaze direction~\cite{emery2000eyes}. Studies have shown that gaze direction is inferred not just from the eyes, but by integrating information about head and body pose~\cite{hietanen1999does,langton2000eyes,hietanen2002social,pomianowska2011socialcues}.

People are particularly sensitive to being gazed at by others~\cite{argyle1976gaze}. Being gazed at can make a person feel uncomfortable, but it can also lead to positive social and cognitive effects. Increased gaze contributes to immediacy---the feeling of physical or psychological closeness between people~\cite{mehrabian1966immediacy}. Immediacy correlates positively with information recall~\cite{otteson1980effect,sherwood1987facilitative,fullwood2006effect,kelley1988effects}---e.g., a lecturer looking at a student can help them remember more information from the lesson.
\emph{Mutual gaze} is the state where two people engage in eye contact~\cite{argyle1976gaze}. People who make a lot of eye contact in social interactions are seen as more likeable and credible~\cite{beebe1976effects,argyle1976gaze}. Sometimes a person might break eye contact in order to look at an object of information in the environment, before reestablishing mutual gaze. The viewer's attentional focus is then reflexively redirected toward the same object. The two individuals are said to be engaging in \emph{joint attention}~\cite{dentremont2007early}, which is an important process for the grounding of linguistic references and communication of intent~\cite{Hanna and Brennan 2007, Preissler and Carey 2005,mumme2007actions}---e.g., people might look at an object as they talk about it.

Attending behaviors, realized through shifts in gaze and body orientation, underpin many important regulatory mechanisms in social interactions between two or more parties~\cite{heylen2006head}. The speaker looks at listeners to indicate the addressees of their utterance, to request backchannels (e.g., a nod or verbal confirmation of understanding), and to signal the end of their conversational turn. Listeners look back at the speaker to indicate they are paying attention, to signal their understanding, or to request the conversational floor. Intimacy regulation is achieved using gaze in conjunction with proxemics~\cite{argyle1965eyecontact}: more intimate conversations are characterized by increased mutual gaze and lower physical distance between the participants. Finally, gaze and body orientation also serve as nonverbal cues of participants' conversational roles (what Goffman termed as ``footing''~\cite{goffman1979footing}), which is particularly important in multiparty interactions, where participants' roles can range from speakers or addressees, to bystanders, to overhearers and eavesdroppers. In general, increased gaze and body orientation towards a participant signals a higher degree of participation in the interaction.

Body reorientation is an important, but often overlooked behavior in multiparty interactions. Changes in body orientation occur relatively rarely during interaction compared to gaze shifts involving eye and head movements and indicate larger shifts in attention and mutual involvement~\cite{kendon1973visible,schegloff1998bodytorque}. Body orientation shifts are likely to occur at the start of an interaction, or when a new participants joins in. One reason for body reorientation is physical comfort---people like to avoid head rotations greater than 90$^{\circ}$ when gazing at someone~\cite{kendon1973visible}. Another one is to signal a shift in participants' conversational roles or effect a reconfiguration of the conversational formation---e.g., when a new party joins the conversation, the current participants might turn towards them~\cite{kendon2010spacing}.

The above findings have several implications for character animation. First, they attest to the important role of gaze behaviors in human communication and thus motivate the need for computational synthesis of natural, controllable gaze shifts. Second, they demonstrate that coordinated control over gaze direction and body orientation is needed to articulate the full range of attending behaviors on animated characters. Third, they suggest that camera position matters when designing the character's gaze. How an animated character directs its gaze and orients its body relative to the camera will have significant effects on the viewer's experience.

\subsection{Gaze Modeling in Neurophysiology}

Research in neurophysiology has studied how humans and other primates carry out gaze shifts in a tightly connected dynamic process by coordinating eye, head, and body movements  ~\cite{zangemeister1982types,andredeshays1988eyehead1,barnes1979vor,freedman2000coordination,uemura1980eyehead,mccluskey2007monkeys}. Researchers measured these movements in highly controlled experiments, obtaining numeric data about kinematic properties such as movement range~\cite{guitton1987gaze} and eye and head velocities~\cite{guitton1987gaze,freedman2000coordination,barnes1979vor,uemura1980eyehead}. The model for gaze shift synthesis described in Chapter~\ref{cha:GazeShiftModel} is based on their results.

Kinematics of coordinated eye and head movements are reported by~\citet{guitton1987gaze},~\citet{freedman2000coordination},~\citet{barnes1979vor}, \citet{uemura1980eyehead}, and others. Eye and head movements in gaze are tightly coupled and significantly affect each other. There exists a linear relationship between head velocity and eye movement amplitude in gaze shifts~\cite{barnes1979vor,uemura1980eyehead}. Furthermore, head movement amplitude decreases as eye movements start at increasingly contralateral positions (i.e., oriented away from the target in relation to head direction). Shifts that start at such positions require that the eyes contribute more to the shift~\cite{mccluskey2007monkeys}. The degree to which individuals use their heads in performing a gaze shift is highly idiosyncratic. Neurophysiological research literature describes some people as ``head-movers'', i.e., individuals who move their head fully to align with the gaze target every time, and some as ``non-head-movers''~\cite{fuller1992head}. From a biomechanical standpoint, humans should universally be ``non-head-movers'', as fully moving the head---which is almost a hundred times heavier than the eyes---is not an economical solution~\cite{kim2007head}; therefore, the human tendency to move the head more than necessary during gaze shifts can be attributed to the role of head orientation in signalling attention.

Shifts in attention direction are accomplished not only through movements of the eyes and the head but also through movements of the body. Unfortunately, studies that have attempted to measure the kinematics of these movements are few. \citet{mccluskey2007monkeys} measured upper body movements in primate gaze, finding that body contribution to gaze shifts increased with gaze shift amplitude and that the upper body trailed eye and head movements with substantial latency. Based on these results, they concluded that body movements in gaze shifts are part of a series of coordinated motor events triggered when primates reorient their gaze. \citet{hollands2004wholebody} obtained similar findings for coordination of the eyes, head, and feet during whole-body reorientation.

\subsection{Gaze Motion Synthesis}

Gaze behaviors for animated characters are synthesized using computational models of human gaze. I distinguish between low-level and high-level models. \emph{Low-level} gaze models synthesize atomic gaze movements (such as gaze shifts) by simulating the kinematics of real human gaze, whereas \emph{high-level} models synthesize complex gaze behaviors (composed of many low-level movements) that fit the current scenario. The gaze shift synthesis model presented in Chapter~\ref{cha:GazeShiftModel} is an example of a low-level model, whereas the gaze inference model in Chapter~\ref{cha:GazeAuthoring} can be classed as high-level.

In the remainder of the section, I give an overview of gaze synthesis models for animated characters and embodied conversational agents. I focus on models that are most relevant to the current work---a more thorough survey is provided by~\citet{ruhland2015gazereview}.

\subsubsection{Low-level Gaze Models}

Low-level gaze models synthesize atomic movements that comprise the overall gaze behavior. These movements are:

\begin{enumerate}
\item \textbf{Saccades} -- Saccades are rapid eyeball movements toward targets of interest. Saccades are what people most commonly associate with gaze.
\item \textbf{Vestibulo-ocular reflex (VOR)} -- The VOR stabilizes the eyes during head motion, ensuring they remain fixated on the target and the image remains stable.
\item \textbf{Vergence} -- In almost all gaze movements, both eyes move in the same direction. Vergence is the exception---when the eyes focus on an object that lies along the visual midline, they need to move in the opposite direction.
\item \textbf{Smooth pursuit} -- Smooth pursuit executes a series of saccades to visually track a slow-moving object or read a line text, while ensuring the image remains stable.
\item \textbf{Gaze shifts} -- Gaze shifts are coordinated, rotational movements of the eyes, head, and body toward targets of interest.
\item \textbf{Eye blinks} -- Eye blinks are rapid eyelid-closing and opening movements. They typically occur spontaneously and provide lubrication to the eye, though they also occur reflexively (e.g., to shield the eye from injury) and voluntarily (as a form of nonverbal communication, e.g., winks).
\end{enumerate}

Low-level gaze models generally focus on simulating saccades and gaze shifts. Since saccades can be thought of as gaze shifts comprised only of eye movements, models for the latter implicitly support the former. The VOR is fairly trivial to implement, so almost all gaze models incorporate it. Likewise, all models implicitly simulate vergence, because they compute each eye's orientation separately. Smooth pursuit is a relatively infrequent movement, so it is rarely simulated---one notable exception is the smooth pursuit implementation by~\citet{yeo2012eyecatch}.

TODO: explain the idea of parametric control

Gaze shift and saccade models can be classified into data-driven models, which use recorded, annotated, or motion-captured data to synthesize novel gaze behaviors~\cite{heck2007automated,lance2010expressive,lee2002eyes,deng2005automated,le2012live}, and procedural models, which synthesize gaze movements using equations that describe their kinematics~\cite{peters2010animating,thiebaux2009realtime}. Data-driven models typically produce more natural gaze motions, while procedural models offer a greater degree of parametric control. Saccadic gaze models are generally data-driven; they use statistical~\cite{le2012live,lee2002eyes} or sampling-based~\cite{deng2005automated} models of gaze data to synthesize saccadic sequences given inputs such as head motion and speech. Among gaze shift models, the Expressive Gaze Model (EGM)~\cite{lance2010expressive} is an example of a hybrid model, which produces procedurally generated, neurophysiologically-based eye movements in conjunction with motion-captured head and torso movements. It also supports parametric control over the head movements. The model by~\citet{peters2010animating} is procedural, eye-head model that supports parametric control via a head propensity parameter.

The gaze shift model presented in the current work


% \subsection{Gaze Motion Synthesis}
\noindent\textbf{Gaze Synthesis} -- Ruhland et al. \shortcite{ruhland2015gazereview} provide a survey of prior work on gaze in computer animation. Researchers have proposed a number of synthesis models of gaze shifts. While some work (e.g. \cite{deng2005automated,lee2002eyes}) focuses on saccades (rapid eyeball movements in gaze), a number of models synthesize coordinated movements of the eyes, head, and body towards targets. For example, \cite{lance2010expressive} and \cite{heck2007automated} use a data-driven approach, while \cite{peters2010animating} and \cite{andrist2012headeye} introduce procedural models inspired by neurophysiological observations. We have chosen to build from the model by Pejsa et al.~\shortcite{pejsa2015gaze} as it provides neurophysiologically plausible movements with parameterized control over both head and torso posture. While our work focuses on directed gaze to fixed targets, there are also computational models of other types of gaze movements, such as smooth pursuit \cite{yeo2012eyecatch} and conversational aversion \cite{andrist2013aversion}.
%Prior works on gaze in computer animation are concerned with computational modeling of kinematics of directed gaze movements to enable their synthesis. This body of research is more extensively surveyed in~\cite{ruhland2015gazereview}; here we highlight only the most relevant papers.
%One of the most important gaze movements is the \emph{saccade}---rapid eyeball movement between fixations. Much of the prior work in gaze animation has focused on synthesis of natural saccadic gaze using statistical models learned from recorded eye movements~\cite{deng2005automated,le2012live,lee2002eyes}. Saccades are integral components of larger gaze shifts, where they occur in coordination with head and upper body movements. Researchers have also proposed several models of eye-head coordination in gaze ~\cite{peters2010animating,lance2010expressive,andrist2012headeye}. These models enable control over head posture using parameters that determine how much the head turns toward a target during a gaze shift. They achieve motion naturalness in different ways. The Expressive Gaze Model~\cite{lance2010expressive} makes use of recorded motion data to add expressiveness to gaze movements, while the model by Andrist et al.~\shortcite{andrist2012headeye} is procedural but uses kinematics derived from neurophysiological measurements of gaze. Pejsa et al.~\shortcite{pejsa2013stylized} demonstrate how these kinematics can be adapted to characters with non-humanlike eye designs. Another notable gaze model is the data-driven model by Ma et al.~\shortcite{ma2009natural}, which infers natural eye movements from motion-captured head movements. Our gaze inference approach differs in that we do not infer eye movements directly; rather, we infer an abstracted specification from which the overall gaze movement can be synthesized. Several gaze synthesis models also incorporate coordinated torso movements~\cite{pejsa2015gaze,thiebaux2009realtime,lance2010expressive,heck2007automated,grillon2009crowds}. We adapt the model by Pejsa et al.~\shortcite{pejsa2015gaze} as it provides parameterized control over torso posture.

Don't forget:
- Badler - Where To Look, Eyes Alive
- When talking about eye-head coordination, cite andrist2012designing
- Eye blinks
- Talk about gaze in VR?

%\subsection{Gaze Inference}
\noindent\textbf{Gaze Inference} -- We use the term ``gaze inference'' to describe methods that analytically determine when and where a person should look, and use that information to generate a virtual character's gaze behavior. Research on gaze control in cognitive science, surveyed in~\cite{henderson2003human}, has shown that people's gaze is influenced by two mechanisms of attention: the spontaneous \emph{bottom-up} attention, which directs their gaze toward visually salient environment features, and the deliberate \emph{top-down} attention, which directs it toward task-relevant objects. Much prior work has focused on simulating bottom-up attention, typically to synthesize believable idle gaze of virtual agents and avatars. This is often accomplished by computing a saliency map of the virtual scene from features such as color, contrast, orientation, and motion~\cite{peters2003bottomup,peters2008applying} and using it to predict where the character should look. We perform gaze target inference using a similar representation. Also related are approaches such as~\cite{cafaro2009animating,grillon2009crowds,kokkinara2011modelling}, which determine the gaze target from spatial and kinematic properties of scene objects and other characters, such as proximity, velocity, and orientation. In contrast, models of top-down attention generate gaze based on the character's goals, intentions, and knowledge. Examples include the Rickel Gaze Model~\cite{lee2007rickel} and models of gaze for virtual demonstrators~\cite{huang16planning}. Hybrid approaches~\cite{khullar2001look,mitake2007reactive} integrate bottom-up gaze with top-down, behavior- and task-driven patterns.

Our gaze inference approach is different in that its main objective is to support motion editing. As such, it seeks to discover an editable specification of the gaze behavior already encoded in the body motion, rather than generate a novel gaze behavior that potentially overwrites the body motion's gaze component. The approach is more closely related to top-down models, since it assumes the body motion encodes the actor's intent (expressed in gaze), and it tries to infer that intent by analyzing available data: head and torso kinematics, hand contacts, and author-supplied labels. In relying on head movement kinematics to infer eye movements, our approach also bears similarity to the data-driven model by Ma et al.~\shortcite{ma2009natural}.

\noindent\textbf{Motion Synthesis and Editing} -- To our knowledge, ours is the first approach for model-based editing of gaze motion. Model-based editing uses an abstracted model to specify what the motion should be and produce the final motion that expresses the author's intent, using techniques such as spacetime optimization, physics-based modeling, and synthesis from priors. Examples include physics-based spacetime methods~\cite{popovic99physically}, motion path editing~\cite{gleicher2001path}, spatial relationship preserving interaction editing~\cite{ho2010spatial}, data-driven hand animation for guitar playing~\cite{elkoura2003handrix}, etc. Furthermore, ours is the first approach for automatically adding gaze animation to motion capture data. Other researchers have considered the analogous problem of inferring missing hand animation, notably \cite{jorg2012finger} and \cite{ye2012hand}.


