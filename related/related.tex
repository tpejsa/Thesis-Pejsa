\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Related Work}

Gaze has been a subject of research in several academic disciplines. Social sciences have focused on studying the functions of gaze in human perception, communication, and cognition. Human-computer and human-robot interaction have studied those same functions on virtual agents, avatars, and humanlike robots. In neurophysiology, there has been a focus on describing gaze in terms of its kinematic properties, such as gaze shift amplitudes and velocities. These bodies of work are complementary and provide a solid foundation for computational modeling of gaze. Gaze synthesis methods presented in this work largely derive from quantitative measurements of gaze in neurophysiology and perceptual psychology, while the hypotheses about its social and perceptual effects are informed by research in the social sciences, HCI, and HRI.

In this chapter, I review the social sciences, HCI, and neurophysiology research on gaze, followed by a review of related work on computational synthesis of gaze motion for animated characters and embodied conversational agents. In addition, I discuss prior work relevant to the problem of animating stylized characters.

\section{Gaze in Human Communication}
\label{sec:GazeHumanCommunication}

\subsection{Human-Human Interactions}

Humans have an innate ability to track the gaze of others and infer the focus of their social attention, which emerges in the first year of life~\citep{scaife1975infant,vecera1995detection}. Numerous studies (surveyed in ~\citep{langton2000eyes}) have shown that, when observing the gaze others, humans reflexively reorient their visual attention in the same direction. This automated reorienting has a neurological basis---human brains have specialized structures for processing gaze direction~\citep{emery2000eyes}. Studies have shown that gaze direction is inferred not just from the eyes, but by integrating information about head and body pose~\citep{hietanen1999does,langton2000eyes,hietanen2002social,pomianowska2011socialcues}.

People are particularly sensitive to being gazed at by others~\citep{argyle1976gaze}. Being gazed at can make a person feel uncomfortable, but it can also lead to positive social and cognitive effects. Increased gaze contributes to immediacy---the feeling of physical or psychological closeness between people~\citep{mehrabian1966immediacy}. Immediacy correlates positively with information recall~\citep{otteson1980effect,sherwood1987facilitative,fullwood2006effect,kelley1988effects}---e.g., a lecturer looking at a student can help them remember more information from the lesson.
\emph{Mutual gaze} is the state where two people engage in eye contact~\citep{argyle1976gaze}. People who make a lot of eye contact in social interactions are seen as more likeable and credible~\citep{beebe1976effects,argyle1976gaze}. Sometimes a person might break eye contact in order to look at an object of information in the environment, before reestablishing mutual gaze. The viewer's attentional focus is then reflexively redirected toward the same object. The two individuals are said to be engaging in \emph{joint attention}~\citep{moore2014joint}, which is a vital process for the grounding of speech references and early acquisition of language.

Attending behaviors, realized through shifts in gaze and body orientation, underpin many important regulatory mechanisms in social interactions between two or more parties~\citep{kendon1967some,heylen2006head}. The speaker looks at listeners to indicate the addressees of their utterance, to request backchannels (e.g., a nod or verbal confirmation of understanding), and to signal the end of their conversational turn. Listeners look back at the speaker to indicate they are paying attention, to signal their understanding, or to request the conversational floor. Intimacy regulation is achieved using gaze in conjunction with proxemics~\citep{argyle1965eyecontact}: more intimate conversations are characterized by increased mutual gaze and lower physical distance between the participants. Finally, gaze and body orientation also serve as nonverbal cues of participants' conversational roles (what Goffman termed ``footing''~\citep{goffman1979footing}), which is particularly important in multiparty interactions, where participants' roles can range from speakers or addressees, to bystanders, to overhearers and eavesdroppers. In general, increased gaze and body orientation toward a participant signals a higher degree of participation in the interaction~\citep{mutlu2012conversational}.

Body reorientation is an important, but often overlooked behavior in human interactions. Changes in body orientation occur relatively rarely during interaction compared to gaze shifts involving eye and head movements and indicate larger shifts in attention and mutual involvement~\citep{kendon1990conducting,schegloff1998bodytorque}.
Body orientation shifts are likely to occur at the start of an interaction, or when a new participants joins in. Conversational participants generally position and orient themselves in a roughly circular fashion, creating what Kendon has termed the ``o-space''~\citep{kendon1990conducting}. One characteristic of this arrangement is that people do not need to gaze more than 30$^{\circ}$ off-center~\citep{kendon1990conducting}. \citet{schegloff1998bodytorque} coined the term ``body torque'' to describe the state where the head, torso, pelvis, and feet are out of alignment with one another. He hypothesized that such a pose suggests engagement with multiple targets with an implicit importance ranking and signals an impending change in the activity---e.g., a reconfiguration of the conversational formation to involve a new participant~\citep{kendon1990conducting}.

The above findings have several implications for character animation. First, they attest to the important role of gaze behaviors in human communication and thus motivate the need for computational synthesis of natural, controllable gaze shifts. Second, they demonstrate that coordinated control over gaze direction and body orientation is needed to articulate the full range of attending behaviors on animated characters. Third, they suggest that camera position matters when designing the character's gaze. How an animated character directs its gaze and orients its body relative to the camera will have significant effects on the viewer's experience.

\subsection{Human-Avatar and Human-Agent Interactions}

The communicative effects of gaze and body orientation have also been studied in interactions with virtual humans, avatars, and humanlike robots, providing ample evidence of the importance of well-designed gaze mechanisms in these contexts. Increased gaze from a storytelling robot facilitates better information recall in listeners~\citep{mutlu2006storytelling}, and similar effects have been observed in interaction with avatars in immersive virtual reality~\citep{bailenson2005transformed}. A virtual agent can also employ gaze cues to assist people in localizing task-relevant objects in the environment \cite{bailly2010gaze}. The effectiveness of gaze and body orientation cues in supporting conversational processes has been demonstrated on virtual embodiments: gaze and body orientation of an avatar affects interpersonal distance~\citep{bailenson2003interpersonal}, while well-designed gaze cues of a virtual agent can facilitate more efficient turn-taking~\citep{heylen2005experimenting,andrist2013aversion}.

A few works have studied the effects of eye, head, and body coordination on attention cues of virtual agents and humanlike robots.~\citet{andrist2012designing} have shown that a virtual agent looking at information in the environment with more head alignment facilitates greater recall of that information by the participant. Furthermore, two works have shown that a humanlike robot can use its gaze distribution and body orientation shifts to manage the footing~\citep{mutlu2012conversational} and conversational formation~\citep{kuzuoka2010reconfiguring} in interactions with human participants, thus effectively shaping human conversational behavior with their own. These findings provide strong motivation for endowing embodied conversational agents with the ability to shift their gaze and body orientation in a coordinated manner in order to support new communicative capabilities. Chapter~\ref{cha:GazeFooting} presents a study examining similar effects in interactions with an agent in virtual reality.

\section{Gaze Modeling in Neurophysiology}

Research in neurophysiology has studied how humans and other primates carry out gaze shifts in a tightly connected dynamic process by coordinating eye, head, and body movements~\citep{zangemeister1982types,andredeshays1988eyehead1,barnes1979vor,freedman2000coordination,uemura1980eyehead,mccluskey2007monkeys}. Researchers measured these movements in highly controlled experiments, obtaining numeric data about kinematic properties such as movement range~\citep{guitton1987gaze} and eye and head velocities~\citep{guitton1987gaze,freedman2000coordination,barnes1979vor,uemura1980eyehead}. The model for gaze shift synthesis described in Chapter~\ref{cha:GazeShiftModel} is based on their results.

Kinematics of coordinated eye and head movements are reported by~\citet{guitton1987gaze},~\citet{freedman2000coordination},~\citet{barnes1979vor}, \citet{uemura1980eyehead}, and others. Eye and head movements in gaze are tightly coupled and significantly affect each other. There exists a linear relationship between head velocity and eye movement amplitude in gaze shifts~\citep{barnes1979vor,uemura1980eyehead}. Furthermore, head movement amplitude decreases as eye movements start at increasingly contralateral positions (i.e., oriented away from the target in relation to head direction). Shifts that start at such positions require that the eyes contribute more to the shift~\citep{mccluskey2007monkeys}. The degree to which individuals use their heads in performing a gaze shift is highly idiosyncratic. Neurophysiological research literature describes some people as ``head-movers'', i.e., individuals who move their head fully to align with the gaze target every time, and some as ``non-head-movers''~\citep{fuller1992head}. From a biomechanical standpoint, humans should universally be ``non-head-movers'', as fully moving the head---which is almost a hundred times heavier than the eyes---is not an economical solution~\citep{kim2007head}; therefore, the human tendency to move the head more than necessary during gaze shifts can be attributed to the role of head orientation in signalling attention.

Shifts in attention direction are accomplished not only through movements of the eyes and the head but also through movements of the body. Unfortunately, studies that have attempted to measure the kinematics of these movements are few. \citet{mccluskey2007monkeys} measured upper body movements in primate gaze, finding that body contribution to gaze shifts increased with gaze shift amplitude and that the upper body trailed eye and head movements with substantial latency. Based on these results, they concluded that body movements in gaze shifts are part of a series of coordinated motor events triggered when primates reorient their gaze. \citet{hollands2004wholebody} obtained similar findings for coordination of the eyes, head, and feet during whole-body reorientation.

\section{Gaze Motion Synthesis}

Gaze behaviors for animated characters are synthesized using computational models of human gaze. I distinguish between low-level and high-level models. \emph{Low-level} models synthesize atomic gaze movements (such as gaze shifts) by simulating the kinematics of real human gaze, whereas \emph{high-level} models synthesize complex gaze behaviors (composed of many low-level movements) that match the current situation. Put more simply, high-level models determine \emph{when} and \emph{where} the character should look, while low-level models determine \emph{how} to look there. The gaze shift synthesis model presented in Chapter~\ref{cha:GazeShiftModel} is an example of a low-level model, whereas the gaze inference model in Section~\ref{sec:GazeInference} can be classed as high-level.

In the remainder of the section, I give an overview of gaze synthesis models for animated characters and embodied conversational agents. I focus on models that are the most relevant to the current work---a more thorough survey is provided by~\citet{ruhland2015gazereview}.

\subsection{Low-level Gaze Models}

Low-level gaze models synthesize atomic movements that comprise the overall gaze behavior. These movements are:

\begin{enumerate}
\item \textbf{Saccades} -- Saccades are rapid eyeball movements toward targets of interest. Saccades are what people most commonly associate with gaze.
\item \textbf{Vestibulo-ocular reflex (VOR)} -- The VOR stabilizes the eyes during head motion, ensuring they remain fixated on the target and the image remains stable.
\item \textbf{Vergence} -- In almost all gaze movements, both eyes move in the same direction. Vergence is the exception---when the eyes focus on an object that lies along the visual midline, they need to move in the opposite direction.
\item \textbf{Smooth pursuit} -- Smooth pursuit executes a series of saccades to visually track a slow-moving object or read a line text, while ensuring the image remains stable.
\item \textbf{Gaze shifts} -- Gaze shifts are coordinated, rotational movements of the eyes, head, and body toward targets of interest.
\item \textbf{Eye blinks} -- Eye blinks are rapid eyelid-closing and opening movements. They typically occur spontaneously and provide lubrication to the eye, though they also occur reflexively (e.g., to shield the eye from injury) and voluntarily (as a form of nonverbal communication, e.g., winks).
\end{enumerate}

Low-level gaze models generally focus on simulating saccades and gaze shifts. Since saccades can be thought of as gaze shifts comprised only of eye movements, models for the latter implicitly support the former. The VOR is fairly trivial to implement, so almost all gaze models incorporate it. Likewise, most models implicitly simulate vergence, because they compute each eye's orientation separately. Smooth pursuit is a relatively infrequent movement, so it is rarely simulated---one notable exception is the smooth pursuit implementation by~\citet{yeo2012eyecatch}. Finally, almost all gaze models also integrate eye blinks, which are essential for believable animation---e.g., ~\citet{peters2010animating} integrate a statistical model of gaze-evoked blinks.

Gaze shift and saccade models can be classified into data-driven models~\citep{heck2007automated,lance2010expressive,lee2002eyes,deng2005automated,ma2009natural,le2012live}, which use motion-captured or annotated data to synthesize novel gaze movements, and procedural models~\citep{peters2010animating,thiebaux2009realtime}, which synthesize gaze movements using equations describing their kinematics (often derived from neurophysiology literature). Data-driven models typically produce more natural gaze motions, while procedural models offer a greater degree of parametric control. Many data-driven models focus on saccadic gaze; they use statistical~\citep{lee2002eyes,ma2009natural,le2012live} or sampling-based~\citep{deng2005automated} models of gaze data to synthesize saccadic sequences given inputs such as head motion and speech. Among gaze shift models, the Expressive Gaze Model (EGM)~\citep{lance2010expressive} is an example of a hybrid model, which produces procedurally generated, neurophysiologically-based eye movements in conjunction with motion-captured head and torso movements.

Gaze shift models can also be classified into eye-head models, that support only coordinated eye and head movements, and upper-body models, that also support torso movements. The latter include: the EGM~\citep{lance2010expressive}, which animates the torso movements using motion capture data; the Parametric Gaze Map by~\citet{heck2007automated} synthesizes gaze shifts by interpolating motion captured poses; the model by~\citet{grillon2009crowds}, which uses an IK solver to turn the upper body toward the gaze target. To my knowledge, there are no prior gaze models that explicitly support whole-body movements, although both~\citep{heck2007automated} and~\citep{grillon2009crowds} can adapt existing body motions with upper-body turns.

Parametric control is an important characteristic of gaze models that enables specification of gaze movement properties. For gaze shift models, a minimal parametrization allows the specification of where to look, either as the gaze target location (3D point) or direction (3D vector or a pair of angles). In addition, some models~\citep{peters2010animating,lance2010expressive,thiebaux2009realtime} allow parametric control over head movements via a parameter typically dubbed ``head alignment'' or ``head propensity'', which specifies how much the head contributes to the gaze shift. One study has shown that more head movement in a gaze shift yields a stronger attention cue~\citep{andrist2012designing}. Only one prior model allows equivalent parametric control over torso alignment---the SmartBody Gaze Controller~\citep{thiebaux2009realtime}.

The gaze shift model presented in the current work is a procedural, neurophysiologically-based model. To my knowledge, it is the first model that supports coordinated eye, head, torso, and whole-body movements. It exposes parameters for controlling head, torso, and whole-body alignment in gaze shifts, and it integrates with an inverse kinematics solver and a turn-in-place controller to enable upper-body and whole-body reorientation. As such, it is currently the most complete computational implementation of movements that comprise human attending behaviors.

\subsection{High-level Gaze Models}

High-level gaze models determine where and when the character should look given the current situation in the scenario. They generate complex gaze behaviors comprised of atomic gaze movements, which may be synthesized by low-level gaze models. More formally, high-level gaze models work by mapping high-level state inputs to timings and spatial targets of gaze movements using some internal logic, which usually takes the form of heuristic rules (e.g., always look at an object before picking it up) or statistical models derived from real-world data (e.g., if you are the speaker, look at the addressee's face 26\% of the time). Elements of high-level state might include the character's goals and intentions (e.g., whether the character is about to pick up an object), their emotional state, the state of the virtual environment (e.g., what are the most visually salient objects), and interaction state (e.g., whether the character is the speaker or listener in a conversation.) High-levels models are necessarily context-specific: there are models for conversational gaze~\citep{pelachaud2003modelling,masuko2007headeye,gratch2007rapport,andrist2013aversion,lee2007rickel}, autonomous speaker gaze~\citep{bee2010gaze,zoric2011oncreating,marsella2013virtual}, emotionally expressive gaze~\citep{queiroz2007automatic,lance2010expressive,li2012emotional}, gaze for locomotion and physical tasks~\citep{khullar2001look,mitake2007reactive,huang2016planning}, idle gaze~\citep{khullar2001look,peters2003bottomup,mitake2007reactive,peters2008applying,cafaro2009animating,grillon2009crowds,kokkinara2011modelling}, etc. The multitude of high-level gaze models are extensively surveyed in~\citep{ruhland2015gazereview}; here I focus on a subset of gaze models related to the current research.

The gaze authoring approach proposed in the current work (Chapter~\ref{cha:GazeAuthoring}) relies on an automated gaze inference model to generate a best-guess representation of the character's attending behavior. The gaze inference model falls into the category of high-level gaze models. To infer the gaze behavior, the model analyzes the kinematic properties of the body motion, the geometry of the virtual scene, and author-supplied semantic annotations. As such, the model bears similarity to prior gaze models that try to automate the gaze behavior based on scene and task information. It is also related to behavior synthesis systems for embodied conversational agents, which use editable representations of the agent's behavior in the form of scripts and XML markup. Examples include BodyChat~\citep{vilhjalmsson1998bodychat}, BEAT~\citep{cassell1999fully}, Spark~\citep{vilhjalmsson2004animating}, and systems based on the Behavior Markup Language (BML)~\citep{vilhjalmsson2007bml}. However, these systems are designed for specifying the behaviors of interactive agents rather than animation authoring, so they typically lack the controls and graphical tools needed for intuitive and expressive motion editing.

Research on gaze control in cognitive science, surveyed in~\citep{henderson2003human}, has shown that people's gaze is influenced by two mechanisms of attention: the spontaneous \emph{bottom-up} attention, which directs their gaze toward visually salient environment features, and the intentional \emph{top-down} attention, which directs it toward task-relevant objects and other people in the interaction.
Many prior gaze models have focused on automating bottom-up attention, typically to synthesize believable idle gaze of virtual agents and avatars. This is often accomplished by computing a saliency map of the virtual scene from features such as color, contrast, orientation, and motion~\citep{peters2003bottomup,peters2008applying} and using it to predict where the character should look. Our gaze inference model (Section~\ref{sec:GazeInference}) performs gaze target inference using a similar representation.
Also related are approaches such as~\citep{cafaro2009animating,grillon2009crowds,kokkinara2011modelling}, which determine the gaze target from spatial and kinematic properties of scene objects and other characters, such as proximity, velocity, and orientation. In contrast, models of top-down attention generate gaze based on the character's goals, intentions, and knowledge. Examples include the Rickel Gaze Model~\citep{lee2007rickel}, models of gaze for virtual demonstrators~\citep{huang2016planning}, and the gaze model for object manipulation tasks by~\citet{bai2012synthesis}. Hybrid approaches~\citep{khullar2001look,mitake2007reactive} integrate bottom-up gaze with top-down, behavior- and task-driven patterns.

Our gaze inference approach differs from the above in that its main objective is to support editing gaze in motion capture data. As such, it seeks to discover an editable specification of the gaze behavior already encoded in the body motion, rather than generate a novel gaze behavior that potentially overwrites the body motion's gaze component. The approach is more closely related to top-down models, since it assumes the body motion encodes the actor's intent (expressed in gaze), and it tries to infer that intent by analyzing available data: head and body kinematics, hand contacts, and author-supplied annotations.
The approach is also related to methods that automatically add missing movements to motion capture data. Several works~\citep{jorg2012finger} and \citep{ye2012hand} solve the analogous problem of adding hand animation to captured body motions; however, I am unaware of prior work that considers the problem of inferring gaze animation.
% TODO: The gaze authoring approach falls under the umbrella of \emph{model-based} editing methods. These methods use an abstracted model to specify what the motion should be and produce the final motion that expresses the author's intent, using techniques such as spacetime optimization, physics-based modeling, and synthesis from priors. Examples include physics-based spacetime methods~\citep{popovic99physically}, motion path editing~\citep{gleicher2001path}, spatial relationship preserving interaction editing~\citep{ho2010spatial}, data-driven hand animation for guitar playing~\citep{elkoura2003handrix}, etc.
% TODO: consider also discussing more research on gaze on avatars and ECAs, esp. in IVEs

\section{Animating Stylized Characters}
\label{sec:AnimatingStylizedCharacters}

Stylized characters with simplified, exaggerated, or non-human features are a standard element of animated storytelling and they are also commonplace in the roles of embodied conversational agents. The causes for their popularity lie in their aesthetic appeal, familiarity, communicative expressiveness, and robustness to uncanny valley effects~\citep{mori2012uncanny}. However, while most of the character animation research in computer graphics has focused on simulating and reproducing realistic human movements, the challenges of animating stylized characters have received less attention---especially with respect to gaze animation.

\subsection{Principles of Stylized Motion}

Stylized characters have been a standard feature of traditional animation since its inception. Decades of evolution in the animation craft have given rise to a number of animation principles---guidelines and techniques that trained animators apply in order to create appealing, engaging, and believable animation, perhaps best exemplified in the famous thirteen principles utilized at Disney Animation Studios~\citep{thomas1981illusion}. Much of the relevant research on motion synthesis has focused on computational modeling of these principles in order to exaggerate and stylize character motion. Perhaps the earliest treatment of traditional animation principles in computer graphics is the seminal paper by~\citet{lasseter1987principles}. Since then, there have been a number of efforts to devise general methods for ``cartoonifying'' motion, automatically introducing effects such as squash-and-stretch deformations~\citep{chenney2002simulating,wang2006filter,kwon2012squash}, joint breaking~\citep{noble2007deform}, rubber-hose animation~\citep{kwon2008exaggerating}, anticipation and follow-through~\citep{choi2004anticipation,wang2006filter,kim2006anticipation}, and slow-in, slow-out~\citep{kwon2011slowin}. What these methods have in common is that they alter the kinematics---or even dynamics---of the character's movement in order to produce some aesthetic, perceptual, or communicative effect.

\subsection{Adapting Motion to Stylized Characters}

Stylized characters tend to depart from human anatomic proportions and morphology and applying realistic human motion to such characters can result in unappealing and distracting animation artifacts, such as footskating, limb penetrations, etc. Adding to the challenge is the fact that stylized characters often have simplified designs and lack many human facial or body features, so important details of the original motion can be lost when applied to such characters. This problem is particularly relevant to gaze animation, as stylized characters often have simplified or no eyes. Traditional animation has dealt with such challenges by expressing the character's attention and personality through other cues---e.g., Winnie Pooh's eyes are simple black dots, so exaggerated head movements and body attitudes were used in place of eye gaze~\cite{thomas1981illusion}.

In computer graphics, researchers have proposed \emph{motion adaptation/retargeting} methods to enable the transfer of human motion data to characters with varied designs. The problem of body motion retargeting~\citep{gleicher1998retargetting} can be stated as follows: compute the adapted motion such that all the physical constraints are met, the motion is as similar as possible to the original, and there are no frame-to-frame discontinuities. Researchers have tried to solve this problem using spacetime optimization or per-frame inverse kinematics with filtering---an early review is given by~\citet{gleicher2001comparing}---while others have proposed online retargeting methods that implicitly ensure pose continuity in their IK formulation~\citep{shin2001puppetry}. Much of the difficulty in motion retargeting comes from the character pose representation, which is typically based on joint orientations. Such a representation requires numerical optimization to compute the pose and makes it more difficult to support highly varied character morphologies. Retargeting approaches have been proposed which rely on alternative pose parametrizations and allow for simpler, often analytical formulations, as well as more diverse character designs---e.g., ~\citep{multon2008mkm,hecker2008real,ho2010spatial}.
% TODO: facial motion retargeting

The current work aims to devise gaze animation methods that are applicable to a wide range of characters, including highly stylized ones. To achieve this aim, we drew upon the ideas from prior work in traditional animation, motion stylization, and motion retargeting. The gaze shift model, presented in Chapter~\ref{cha:GazeShiftModel}, can support varied character morphologies because it decouples gaze shift kinematics from pose computation. It is also coupled with an IK solver to preserve end-effector constraints that could get violated during body orientation shifts. However, this approach is insufficient to animate the gaze of stylized characters, because such characters exhibit animation artifacts even with biologically valid gaze shift kinematics. The stylized gaze methods proposed in Chapter~\ref{cha:StylizedGaze}, which attempt to synthesize artifact-free gaze motion, perform motion adaptation with grounding in traditional animation principles as they apply to eye and head animation---e.g., the large, exaggerated eyes of a stylized character should move more slowly to impart a sense of weight~\citep{williams2009animator}. 