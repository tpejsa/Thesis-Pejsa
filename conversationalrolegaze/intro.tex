Shifts in eye gaze direction and body orientation are building blocks of nonverbal behaviors humans rely on to initiate and conduct conversations. When two participants initiate a conversation, they position and orient their bodies in an \emph{F-formation}~\citep{kendon1990conducting}, defined as a spatial arrangement creating a space between them to which they have equal, direct, and exclusive access. When another participant joins the conversation, the others reorient their bodies in order to reconfigure the F-formation and include the newcomer. As the conversation progresses, the participants use gaze cues to indicate they are paying attention to the speaker, address of their utterances, hold or release the conversational floor, and regulate intimacy~\citep{heylen2006head}. As demonstrated by~\citet{mutlu2012conversational}, the participants' gaze patterns reflect their conversational roles or \emph{footing}~\citep{goffman1979footing}---e.g., active participants (speakers and addressees) receive more gaze than bystanders or overhearers.

The objective of this research is to demonstrate that well-designed directed gaze behaviors can achieve the same effects in multiparty interactions between virtual agents and avatar-embodied humans in an immersive virtual environment, and thus motivate the need for such behaviors.
A common scenario in virtual worlds (e.g., Second Life\footnote{Linden Lab, http://secondlife.com/}, AltspaceVR\footnote{AltspaceVR, http://altvr.com/}) and online games (e.g., World of Warcraft\footnote{Blizzard Entertainment, https://worldofwarcraft.com/}) involves multiple user-controlled avatars engaging in conversation with an autonomous, computer-controlled character. Depending on the application, such characters are known as non-player characters (NPCs), bots, or virtual agents---I use the latter term in the current chapter. The agents may provide information or services to users of the virtual world, or players may interact with them as part of the game's story. Yet virtual agents seldom possess the ability to converse with multiple users in a realistic fashion. Typically each user interacts with the agent independently, so there are multiple one-on-one conversations going on simultaneously, or one user interacts with the agent while the others must wait. However, in a real multiparty interaction, all present parties can make contributions. Participants rely on nonverbal signals to address their utterances and regulate who should speak at any given time, thus ensuring the conversation proceeds smoothly, without misunderstandings, awkward silences, or people talking over one another. This research introduces directed gaze behaviors enabling a virtual agent to more effectively manage social interactions with multiple users.

This research is motivated by the observations that social interactions in virtual environments have to a large extent been inhibited by traditional display and input technology. Interactions have typically been experienced on a 2D screen and with a low field of view, thereby reducing the user's awareness of other participants' gaze cues and spatial arrangement. Moreover, the established mouse and keyboard inputs are not the most natural way of looking and moving around the virtual environment. Modern virtual reality devices such as the Oculus Rift\footnote{Oculus VR, https://www.oculus.com/} and HTC Vive\footnote{HTC, http://www.htcvive.com/} bring with them a high field of view and enhanced immersion, as well as more natural inputs based on high-fidelity, rotational and positional head tracking. Wide adoption of these devices creates an opportunity and incentive to build VR applications centered on realistic social experiences. Our goal is to investigate whether users in immersive VR are responsive to nonverbal signals of conversational footing, and in the process also demonstrate how such signals can be produced by our gaze shift synthesis model (Chapter~\ref{cha:GazeShiftModel}).

In this chapter, I give an overview of directed gaze behaviors designed for a virtual agent engaging in interaction with one or more parties in a virtual environment. The behaviors principally include body orientations shift for reconfiguring the conversational formation and gaze patterns for establishing footing. Both kinds of behaviors use directed gaze shifts synthesized by our model (Chapter~\ref{cha:GazeShiftModel}) as building blocks. I present the results of an experiment with human participants demonstrating how a virtual agent using such behaviors can establish participants' footing in a multiparty interaction, thus shaping their participation behavior.
