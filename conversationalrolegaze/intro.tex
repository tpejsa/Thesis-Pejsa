A common scenario in virtual worlds (e.g., Second Life\footnote{Linden Lab, http://secondlife.com/}, AltspaceVR\footnote{AltspaceVR, http://altvr.com/}) and online games (e.g., World of Warcraft\footnote{Blizzard Entertainment, https://worldofwarcraft.com/}) involves multiple user-controlled avatars engaging in conversation with an autonomous, computer-controlled character. Depending on the application, such characters are known as non-player characters (NPCs), bots, or virtual agents---I use the latter term in the current chapter. The agents may provide information or services to users of the virtual world, or players may interact with them as part of the game's story. Yet virtual agents seldom possess the ability to converse with multiple users in a realistic fashion. Typically each user interacts with the agent independently, so there are multiple one-on-one conversations going on simultaneously, or one user interacts with the agent while the others must wait. However, in a real multiparty interaction, all present parties can make contributions. Participants rely on nonverbal signals to address their utterances and regulate who should speak at any given time, thus ensuring the conversation proceeds smoothly, without misunderstandings, awkward silences, or people talking over one another.
%This research introduces directed gaze behaviors enabling a virtual agent to more effectively manage social interactions with multiple users.

The amount of one's participation in a multiparty interaction varies depending on their conversational role---what Goffman has termed ``footing''~\citep{goffman1979footing}. These roles include: speakers and addressees, who are the core participants in the interaction; bystanders, who listen to the conversation without making their own contributions and are acknowledged by the speakers and addressees; overhearers and eavesdroppers, who are unacknowledged listeners to the conversation. Participants' footing is reflected in their nonverbal behavior---principally gaze and spatial orientation. Speakers gaze at addressees of their utterances, while listeners gaze at the speaker to indicate they are paying attention~\citep{heylen2006head}. Speakers and listeners also use gaze as a turn-taking signal: speakers release the conversational floor by looking at listeners or avert their gaze to hold it~\citep{kendon1967some}, while listeners may gaze at speakers to request the floor~\citep{wiemann1975turn}.
%Frequent gaze aversions also regulate intimacy, thus avoiding the discomfort caused by prolonged eye contact~\cite{argyle1965eyecontact}.
Overall, speakers and addressees receive much more gaze than bystanders~\citep{mutlu2012conversational}. Spatial arrangement of participants serves as another footing signal. Participants position and orient themselves in an F-formation~\citep{kendon1990conducting}---a spatial arrangement creating a space between them to which they have equal, direct, and exclusive access. When another participant joins the conversation, the others reorient themselves to reconfigure the F-formation and include the newcomer. On the other hand, bystanders are not a part of the F-formation and are thus excluded from the conversation.

This research has two objectives. The first objective is to enable an agent, who is engaged in social interaction with human participants in a virtual environment, to use its nonverbal behaviors---gaze and spatial orientation---to regulate participants' footing and thus influence the amount of their contribution. The second objective is to demonstrate that such nonverbal behaviors can be produced by our gaze shift synthesis model (Chapter~\ref{cha:GazeShiftModel}).
The motivation for the first objective comes from observations that social interactions in virtual environments have to a large extent been inhibited by traditional display and input technology. Interactions have typically been experienced on a 2D screen and with a low field of view, thereby reducing the user's awareness of other participants' gaze cues and spatial arrangement. Moreover, the established mouse and keyboard inputs are not the most natural way of looking and moving around the virtual environment. Modern virtual reality devices such as the Oculus Rift\footnote{Oculus VR, https://www.oculus.com/} and HTC Vive\footnote{HTC, http://www.htcvive.com/} bring with them a high field of view and enhanced immersion, as well as more natural inputs based on high-fidelity, rotational and positional head tracking. Wide adoption of these devices creates an opportunity and incentive to build VR applications centered on realistic social experiences. This research will demonstrate that users in immersive VR are responsive to nonverbal signals of conversational footing, which comprise one component of such experiences.

In this chapter, I give an overview of directed gaze behaviors designed for a virtual agent engaging in interaction with one or more parties in a virtual environment. The behaviors principally include body orientations shifts for reconfiguring the conversational formation and gaze patterns for establishing footing. Both kinds of behaviors use directed gaze shifts synthesized by our model (Chapter~\ref{cha:GazeShiftModel}) as building blocks. I present the results of an experiment with human participants demonstrating how a virtual agent using such behaviors can establish participants' footing in a multiparty interaction, thus shaping their participation behavior.
