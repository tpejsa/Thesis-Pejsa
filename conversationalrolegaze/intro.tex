Many applications envisioned for virtual reality (VR) will involve multiparty conversations with virtual agents and avatars of other human participants.
Depending on the application, such characters are known as non-player characters (NPCs), bots, or virtual agents---I use the latter term in the current chapter.
Multiparty interactions centered on agents could form an integral part of applications such as joint, interactive learning (with the agent as a tutor), multiplayer social games, and interactive narratives experienced jointly by friends.

In order to engage in multiparty interactions with humans, virtual agents need to produce humanlike nonverbal signals.
People use nonverbal signals, such as body orientation and gaze, to regulate who is allowed to speak and to coordinate the production of speech utterances. Such signals are important as they help prevent misunderstandings, awkward silences, and people talking over one another. Conversational participants' nonverbal signals establish their roles---also known as \emph{footing}--- which greatly determine their conversational behavior.
Mutually clear conversational roles are vital for smooth, effective multiparty interactions.
However, there are currently no computational models that provide virtual agents with behaviors designed to signal footing, and no studies that assess whether such behaviors can enable agents to shape the roles (footing) of human participants.

This chapter introduces computational models of body reorientation and gaze behaviors for signaling footing. Prior work~\citep{mutlu2012conversational} has introduced a footing gaze model for humanlike robots. We generalize this model to larger groups of participants and supplement it with a body reorientation model to support a broader range of scenarios.
As demonstrated in Chapter~\ref{cha:GazeShiftModel}, shifts in body orientation and gaze can be realized as parametric variations of the same basic movement.
We utilize our two models in a study with human participants. Study results demonstrate that a virtual agent with appropriately designed gaze and spatial orientation cues can shape the footing of human participants and thus influence their conversational behavior.

While the models can be applied to virtual characters independently of how they are presented, our study also examines the influence of display type (Virtual Reality vs. traditional on-screen) on the effectiveness of footing signals. We expect that the improved affordances of a modern VR display, such as wide field of view, stereo, and natural viewpoint control, may enhance the effects of character behaviors on participants. Our results suggest that agents' footing-management abilities are strengthened in a VR setting.

In the remainder of this chapter, I introduce computational models of two nonverbal behaviors for signaling conversational footing---body reorientation and gaze---and I describe their implementation in an embodied dialog system (Section~\ref{sec:GazeFootingBehaviors}). I present the results of a study demonstrating that human participants tend to conform to conversational roles signaled by a virtual agent, but only when the interaction is experienced in immersive VR  (Section~\ref{sec:GazeFootingExperiment}). Finally, I discuss the limitations of this work and potential avenues for future research\footnote{This chapter will be published in~\citet{pejsa2017whome}.}. 