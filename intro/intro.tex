\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Introduction}

Character animation---the process of bringing drawn, sculpted, or computer-generated characters to life---is the foundation of several popular forms of art and entertainment, such as animated film and video games. With their proven ability to engage and entertain people irrespective of age and culture, animated characters hold potential that extends beyond storytelling media: the last two decades have also witnessed a growing research and commercial interest in embodied conversational agents~\citep{cassell2000embodied}, animated characters that can converse with users in real time and serve in educational, marketing, and socially assistive roles.
However, the criteria for good character animation are fairly uniform across different media and application domains. One criterion is that animation must be communicatively \emph{effective}---it must clearly convey the character's intents, emotions, and personality through their movements.
The effectiveness of animation very much depends on the quality of the movements crafted by the animator. While the art of animation allows for much stylistic variation in how the character moves, mainstream film and game animation displays a strong propensity toward physically and biologically \emph{plausible} movements, that closely resemble those of living creatures.

A key component of plausible, communicatively effective character animation is \emph{directed gaze}---the behavior of looking toward people and objects in the environment.
The importance of gaze in human communication is well-documented. By observing the gaze of others, people infer the focus of their attention and understand high-level meaning behind their actions. Human attending behaviors play vital roles in many aspects of communication, from grounding references to objects by looking at them~\citep{hanna2007speakers,preissler2005role}, to supporting conversational processes such as turn-taking~\citep{wiemann1975turn}.
%A person can look at objects and information in the environment and the viewer's attention will be automatically redirected toward the same objects through the mechanism of joint attention~\citep{moore2014joint}.
Importantly, attending behaviors consist of more than just eye movements. When we attend to an object or another person, we do not just look at them with our eyes---we may also turn our heads and even our entire bodies toward them~\citep{langton2000eyes}. Variations in head and body attitude relative to the eyes produce different communicative effects---e.g., we are less involved with someone if we are glancing at them out of the corner of the eye than if we are facing them head-on~\citep{kendon1990conducting,schegloff1998bodytorque}.
Shifts in human attention from one target to another are then accomplished not just through eye movements, but through coordinated redirection of the eyes, head, and body. Intentional, coordinated movements of the eyes, head, and body toward a target are known as \emph{directed gaze shifts}.
Studies of human social behavior have shown that directed gaze is particularly important in multiparty interactions, where participants use their eyes and bodies to signal changes in spatial and social organization of the interaction~\citep{kendon1990conducting,mutlu2012conversational}---e.g., participants can reorient their bodies toward a newcomer to involve them as an equal partner in the interaction, or they can continue facing each other and relegate the newcomer to a bystander role.

It is well-known to professional animators that animated characters with appropriately crafted gaze behaviors can achieve communicative effects comparable to real humans. In the pioneering years of animation, an oft-repeated rule of thumb at Walt Disney Animation Studios was: ``If you're short of time, spend it on the eyes. The eyes are what people watch.''~\citep{thomas1981illusion} Animators expend much effort into animating believable gaze movements that capture the audience's attention and direct it toward the most important thing on the screen.
However, in the context of computer animation research and virtual agent interaction, gaze is understudied and undersupported. There is a lack of specialized methods for synthesis and authoring of eye gaze behaviors, let alone those that support coordinated head and body movements.

In non-interactive contexts (e.g., television and film animation), gaze is typically hand-authored using general methods such as inverse kinematics (IK) and keyframing. Gaze IK methods constrain the eyes to point in a specific direction even as the body moves, while keyframing specifies how the gaze direction changes over time. Body posture---integral to directed gaze behaviors---needs to be edited separately using keyframing and layering. To create biologically plausible and communicatively effective gaze, animators must get the timing and spatial relationships of coordinated eye, head, and body movements right, which requires great expertise and effort. I use the term \emph{high-cost} to describe an animation authoring approach that requires a lot of expertise and effort to use. The high cost of manual gaze authoring is further compounded by its lack of \emph{scalability} with respect to scenario complexity, scene changes, and character design. I define an authoring method as scalable if an acceptable-quality animation can be created reasonably quickly regardless of scenario complexity (number of characters and scenario length). However, hand-authoring time generally can be expected to increase at least linearly with the amount of content in the scenario. Furthermore, hand-authored gaze is valid only for the current character and scene. If we want to apply the same gaze behavior to a character with a different design (e.g., different eye size or spacing) or change the scene layout (thus altering gaze target locations), we must hand-edit the gaze animation or recreate it from scratch. Evidently, the established methods and workflows for creating gaze in non-interactive 3D animation impose a high cost in terms of required expertise and effort, compounded by their lack of scalability. One of the objectives of this dissertation is to introduce more scalable, lower-cost methods for gaze animation authoring.

Gaze synthesis for characters in interactive scenarios (game characters, avatars, and virtual agents) brings further challenges. Scalability constraints of hand-authoring, already costly in non-interactive animation, are often prohibitive when interactivity is required. Characters' gaze behaviors need to dynamically adapt to scenario events and inputs---e.g., a virtual agent needs to look toward the user as the latter moves around. This largely precludes the use of hand-authored gaze animation, because the latter is generally immutable at interaction time. Instead, we must employ computational gaze models, which automatically synthesize the character's gaze behavior given the current scenario state. These models are necessarily context-specific: people display different gaze patterns when engaging in face-to-face conversation (conversational gaze), visually scanning their environment (idle gaze), engaging in visually-guided tasks (e.g., locomotion, physical object manipulation), etc.  However, all of these behaviors are comprised of atomic movements---saccades, eye gaze shifts, body orientation shifts---and their communicative effectiveness stems from their kinematic variability---e.g., gaze shifts that involve a lot of head movement are perceived differently from those that only engage the eyes~\citep{langton2000eyes}. Another objective of this dissertation is to introduce new methods for synthesis of these types of gaze movements, that allow parametric control over their kinematic properties and thus their communicative effects. In particular, we focus on providing parametric control over the head and body movements that occur in coordination with eye gaze---a feature that has hitherto received limited study in computer animation, yet it is needed to realize the full spectrum of attending behaviors.

\textbf{The thesis of this dissertation is that directed gaze is an effective and scalable model for synthesis and low-cost authoring of attending behaviors on animated characters.} The idea of directed gaze is to represent the gaze behavior as a sequence of \emph{directed gaze shifts}. A directed gaze shift consists of coordinated, rotational movements of the eyes, head, and potentially torso, pelvis, and feet toward a target in the environment. It signals a shift in attention toward the target, while the amount of head and body reorientation in the gaze shift signals how the character's attention is distributed among the previous target and the new one. Gaze shift targets can be objects, information, other characters, or visually salient features. This dissertation introduces methods for synthesis of directed gaze shifts based on the kinematics of human movements (Chapter~\ref{cha:GazeShiftModel}, also Section~\ref{sec:GazeSynthesis}). The communicative effectiveness of the proposed methods lies in the kinematic variability of directed gaze shifts they can synthesize---a subtle saccade toward a target can be synthesized as easily as a whole-body turn. As demonstrated in empirical studies(Sections~\ref{sec:GazeShiftModelEval1} and~\ref{sec:GazeShiftModelEval2}, Chapter~\ref{cha:GazeFooting}), the methods can be utilized for synthesis of gaze behaviors to fulfill a variety of communicative roles: signaling attention, regulating various conversational processes, and effecting spatial reorganization of social interactions. Directed gaze is thus also a scalable model of gaze behavior that works in many scenarios, such as two-party and multiparty conversations, object manipulations, locomotion, and idle gaze. It also scales well across characters; any character with eyes, a head, and a body can engage in directed gaze shifts, while morphological and stylistic differences can be accounted for using principled motion adaptation methods (introduced in Chapter~\ref{cha:StylizedGaze}).

Directed gaze as a model of gaze behavior is also advantageous from the standpoint of animation authoring, which we exploit in a novel authoring approach (Chapter~\ref{cha:GazeAuthoring}). The authoring process begins with a body motion, created using either motion capture or keyframing. Since directed gaze shifts are characterized by tightly coordinated head and body movements, we can analyze the body motion and infer the directed gaze sequence that matches the character's motion. From this sequence, we can automatically synthesize the character's gaze animation. The effort of adding eye gaze to an animated scenario is thus potentially constant with respect to scenario complexity. The inferred representation also lends itself to convenient animation editing. It is abstract, allowing the animator to describe the gaze behavior in terms of targets to be looked at, while the kinematics of eye, head, and body movements are synthesized automatically. It is also uniform, since the behavior is comprised entirely of gaze shifts, so its content can be edited with simple operations such as adding and removing gaze shifts. The properties of automatic inference and convenient editing significantly reduce the cost of animation authoring. Moreover, the high level of abstraction afforded by the directed gaze representation makes our approach more scalable with respect to scene changes. As gaze targets move, our methods automatically synthesize the updated gaze motion, without requiring the animator to edit the gaze behavior content.

\section{Methodology}

The research presented in this dissertation was conducted using two main methods: model development and empirical evaluation.

This dissertation introduces computational models of gaze motion, which include a gaze shift synthesis model, methods for adapting gaze shift motion to stylized and non-human character designs, methods for gaze behavior inference from motion capture data, methods for layering gaze motion onto a captured body motion, and a nonverbal behavior model for signalling conversational footing. \emph{Model development} was largely guided by existing literature on human gaze in neurophysiology and social sciences, as well as artistic animation principles and traditions. The findings from literature were integrated into computational models as kinematic equations, heuristic decision-making rules, and probabilistic models built from human behavior data. The constructed models are typically parametric---they afford a set of parameters, which animators or implementers can manipulate to control the properties of resulting gaze motion.

Empirical evaluations predominantly took the form of studies with human participants, although computational simulations and comparisons with ground-truth, hand-annotated human behavior data were also utilized when appropriate. The studies had a two-fold purpose of demonstrating that our constructed models of gaze motion possessed desired perceptual and communicative properties, as well as garnering more general insights about the effects animated gaze behaviors had on people viewing and interacting with the character. The studies were designed to test one or more hypotheses about the effects of synthesized gaze behaviors on human participants viewing or interacting with the virtual character. They were conducted either in-person in a controlled laboratory setting, or online using the Amazon Mechanical Turk crowd-sourcing service. They involved a variety of measures: objective (e.g., participants' accuracy in predicting the character's gaze direction), behavioral (e.g., how much participants spoke with the character), and subjective (e.g., how positively participants rated the character.) The collected data were analyzed using appropriate statistical tests, such as ANOVA and chi-square.

Both the models and evaluations were implemented in a custom character animation and behavior toolkit built using the Unity game engine~\citep{unity3d}. All animation, behavior, and task logic was implemented as Unity C\# scripts. These included our gaze shift model, an eyeblink controller, several high-level gaze behavior models (such as our model for signalling footing), task logic for all the human subjects' experiments, etc. All the preprocessing on character models and animations, as well as all the animation editing tools and operations were implemented as Unity Editor scripts. Character models were exported from 3D modeling applications such as Autodesk 3ds Max~\citep{max} and DAZ Studio~\citep{daz3d} as FBX assets, upon which they were imported into Unity and preprocessed in preparation for animation using our toolkit. When utilized, recorded motion data was either obtained from publicly available datasets such as the CMU database~\citep{cmumocap} or captured at a local motion capture studio. The captured data was processed using Autodesk MotionBuilder~\citep{mobu} and then exported into Unity. Some studies featured virtual agent speech, which was either prerecorded (Chapters~\ref{cha:GazeShiftModel} and~\ref{cha:GazeFooting}) or synthesized using Microsoft Speech SDK (Chapter~\ref{cha:GazeFooting}). One exception to the above is an early version of the gaze shift model utilized in Section~\ref{sec:GazeShiftModelEval1}, which was implemented in C++ using the OGRE engine~\citep{ogre3d} and subsequently ported to Unity.

\section{Contributions}

%This research contributes methods for synthesis and editing of gaze motion based on the representation as a sequence of directed gaze shifts.
%It also contributes demonstrations---in studies with human participants---of the effectiveness of directed gaze shifts as building blocks of behavioral mechanisms for attention signalling.
%The main contributions are the following:
Contributions from the current research can be split into three categories: new animation methods, new interaction designs, and new insights from empirical studies. The animation methods are the following:

\begin{enumerate}
\item \textbf{Gaze shift synthesis model} (Chapter~\ref{cha:GazeShiftModel}). A new model is introduced for synthesis of individual directed gaze shifts that affords parametric control over coordinated eye, head, and body movements and abstracts away details of timing and spatial relationships. The model serves as a fundamental building block of complex gaze behaviors. It is fully procedural and does not rely on motion capture data. Instead, it synthesizes biologically plausible gaze movements using a set of kinematic laws derived from neurophysiological studies of gaze.
\item \textbf{Stylized gaze} (Chapter~\ref{cha:StylizedGaze}. Stylized and non-human characters, such as cartoon characters with large or asymmetric eyes, cannot be animated using human gaze kinematics due to anatomic differences. We introduce a set of methods for automatic adaptation of gaze movements to characters with varied eye and head designs, thereby enhancing the scalability of gaze synthesis methods with respect to character design. We collectively refer to these methods as \emph{stylized gaze}.
\item \textbf{Performative gaze} (Chapter~\ref{cha:StylizedGaze}). When stage actors and animated characters in film and television use directed gaze, they often do not fully align their line of sight with the target and instead maintain partial facing toward the viewer. The viewer is thus able to better see what the character is attending to, as well as feel more engaged by the character. We refer to such partial gaze shifts as \emph{performative gaze}. To extend the communicative range of gaze shift models, we introduce a motion adaptation method that can automatically adapt the target direction of a gaze shift relative to the camera such that partial alignment with the viewer is maintained.
\item \textbf{Gaze inference from body motion} (Chapter~\ref{cha:GazeAuthoring}). As the first component of our approach for gaze animation authoring, we introduce methods for automated inference of the character's directed gaze sequence from their body motion and scene layout. Our approach can then automatically synthesize the eye animation that matches the body motion and scene layout from the inferred sequence, or allow the animator to conveniently edit the gaze animation by editing the sequence.
\item \textbf{Gaze motion layering} (Chapter~\ref{cha:GazeAuthoring}). As the second component of the gaze authoring approach, we introduce methods for layering gaze shift movements synthesized by our gaze shift model (Chapter~\ref{cha:GazeShiftModel}) onto the character's body motion. The methods ensure that the resulting gaze motion has plausible kinematics and does not override the expressive content of the original body motion.
\end{enumerate}

This dissertation also introduces several design contributions. These include:

\begin{enumerate}
\item \textbf{Gaze editing approach} (Chapter~\ref{cha:GazeAuthoring}). The third and final component of the gaze authoring approach is the approach for convenient editing of the gaze behavior. We have designed a set of convenient operations for editing the gaze behavior by adding and removing gaze shifts, as well as modifying their target and timing properties. The edited gaze motion is automatically synthesized and layered onto the character's body motion. The editing operations are implemented as controls in a graphical animation editing tool.
\item \textbf{Gaze authoring workflow} (Chapter~\ref{cha:GazeAuthoring}). We have designed a workflow for gaze animation authoring that is based on an abstract representation of the gaze behavior as a sequence of directed gaze shifts. The workflow begins with automatic inference of the gaze sequence from the character's body motion and scene, followed by convenient manual editing of the sequence using a graphical tool. The gaze motion is continually resynthesized from the sequence representation and layered onto the body motion.
\item \textbf{Nonverbal signals of conversational footing} (Chapter~\ref{cha:GazeFooting}). When two or more people engage in conversation, their degree of participation is tied to their \emph{footing}---speakers and addressees contribute more to the discourse than bystanders and overhearers. Participants' spatial orientations and gaze patterns serve as nonverbal signals of footing---speakers and addressees face each other and gaze at each other more than bystanders and overhearers. We describe spatial orientation and gaze mechanisms for embodied conversational agents enabling them to signal the footing of users engaging in multiparty interaction with the agent. These mechanisms are designed to allow an agent to shape users' conversational behaviors and subjective experiences of the interaction.
\end{enumerate}

The animation methods and designs proposed in this dissertation were extensively evaluated in empirical studies. Gaze shift synthesis methods were evaluated with respect to the biological plausibility and communicative effectiveness of resulting gaze shifts, as well as scalability across different characters. Gaze behavior mechanisms for signalling footing were evaluated on their communicative effectiveness. The gaze authoring approach was evaluated with respect to its scalability across different scenarios, reduction in authoring cost (effort and expertise required), and the ability to produce plausible and communicatively effective gaze animation. The evaluations took the form of empirical studies, the findings from which constitute distinct contributions:

\begin{enumerate}
\item \textbf{A procedural, neurophysiologically-based gaze shift model synthesizes plausible gaze shifts that accurately convey attention direction} (Chapter~\ref{cha:GazeShiftModel}). The first study in Chapter~\ref{cha:GazeShiftModel} has shown that participants are able to infer the targets of gaze shifts generated by our gaze shift model as accurately as those from a state-of-the-art model or real human gaze. They are also rated as natural as those from the state-of-the-art model.
\item \textbf{Gaze shifts with more torso reorientation are perceived as stronger attention cues} (Chapter~\ref{cha:GazeShiftModel}). The second study in Chapter~\ref{cha:GazeShiftModel} has shown that the more a virtual agent reorients its torso toward a gaze shift target, the more interested they are perceived to be in that target.
\item \textbf{Virtual agents can use nonverbal cues (gaze and spatial orientation) to manage conversational footing} (Chapter~\ref{cha:GazeFooting}). A study has shown that participants in a multiparty interaction conform to footing signalled by a virtual agent's gaze and spatial orientation; addressees make more conversational contributions than bystanders.
\item \textbf{The nonverbal footing signals on virtual agents are effective only in immersive virtual reality} (Chapter~\ref{cha:GazeFooting}). The same study has shown the effects of the virtual agent's footing signals on participants' conversational behavior occur only when the interaction is in immersive VR rather than a 2D display.
\item \textbf{Principled adaptation of gaze movements to stylized character designs reduces the amount of visual artifacts in the animation} (Chapter~\ref{cha:StylizedGaze}). A simulation has shown that motion adaptation  applied to gaze shifts synthesized by our model and applied to stylized characters reduce the quantitative prevalence of gaze animation artifacts on such characters.
\item \textbf{Stylized and performative adaptations of gaze movements do not impair their accuracy in conveying attention direction or their plausibility} (Chapter~\ref{cha:StylizedGaze}). A study has shown that when stylized and performative adaptations are applied to gaze shifts, participants are able to infer gaze shift targets as accurately as with the original gaze shift kinematics, and their perceived naturalness is also comparable.
\item \textbf{The important aspects of a human actor's gaze behavior can be reconstructed from the kinematic properties of their body motion and the virtual scene layout} (Chapter~\ref{cha:GazeAuthoring}). Our gaze inference model detects gaze shifts and infers their targets from body motion kinematics and scene layout. An evaluation using ground-truth, hand-annotated motion capture and eye tracking data has shown that the model is able to do so with sufficient sensitivity and accuracy to obtain a representation of the original gaze behavior suitable for gaze motion synthesis and editing.
\item \textbf{A gaze authoring approach based on a gaze behavior representation as a sequence of directed gaze shifts allows novice animators to create and edit gaze animation with less effort} (Chapter~\ref{cha:GazeAuthoring}). An evaluation has shown that a novice animator can add eye movements to an animated scenario in substantially less time and with fewer operations using our approach than experienced animators working by hand in a traditional workflow. Likewise, a novice animator can edit a character's gaze behavior in the scenario in substantially less time and with fewer operations using our tool than an experienced animator employing a traditional tool.
\item \textbf{Character animation obtained from the gaze authoring approach is perceived as plausible} (Chapter~\ref{cha:GazeAuthoring}). A study with human participants has shown that animation created using our approach is perceived as more plausible than having no eye movements, and its plausibility may also be superior to animation with recorded eye movements (using eye tracker cameras) and comparable to animation with hand-authored eye movements.
\item \textbf{The attention distribution of characters with edited gaze animation as perceived as different from the original scenario} (Chapter~\ref{cha:GazeAuthoring}). Another study has shown that when a character's gaze animation is edited using our approach, participants describe the character's attention distribution significantly differently compared to the original scenario.
\end{enumerate}
