\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Introduction}

Character animation---the process of bringing drawn, sculpted, or computer-generated characters to life---is the foundation of some of the most popular contemporary forms of art and entertainment. Animated characters in films, video games, and on television have captivated and entertained generations of viewers. Character animation is also finding utility beyond storytelling media; the last two decades have also witnessed a growing research and commercial interest in embodied conversational agents~\citep{cassell2000embodied}, animated characters that can converse with users in real time and serve in educational, marketing, and socially assistive roles.
However, the criteria for good character animation are fairly uniform across different media and application domains. One criterion is that animation must be communicatively \emph{effective}---it must clearly convey the character's intents, emotions, and personality through their movements.
The effectiveness of animation very much depends on the quality of the movements crafted by the animator. While the art of animation allows for much stylistic variation in how the character moves, mainstream film and game animation displays a strong propensity toward physically and biologically \emph{plausible} movements, that closely resemble those of living creatures.

A key component of plausible, communicatively effective character animation is \emph{directed gaze}---the behavior of looking toward people and objects in the environment.
The significance of gaze cues in human communication is well-documented. By observing the gaze of others, people infer the focus of their attention and understand high-level meaning behind their actions. Human attending behaviors play vital roles in many aspects of communication, from grounding references to objects by looking at them~\citep{hanna2007speakers,preissler2005role}, to supporting conversational processes such as turn-taking~\citep{wiemann1975turn}.
%A person can look at objects and information in the environment and the viewer's attention will be automatically redirected toward the same objects through the mechanism of joint attention~\citep{moore2014joint}.
Importantly, attending behaviors consist of more than just eye movements. When we attend to an object or another person, we do not just look at them with our eyes---we may also turn our heads and even our entire bodies toward them~\citep{langton2000eyes}. Variations in head and body attitude relative to the eyes produce different communicative effects---e.g., we are less involved with someone if we are glancing at them out of the corner of the eye than if we are facing them head-on~\citep{kendon1990conducting,schegloff1998bodytorque}.
Shifts in human attention from one target to another are then accomplished not just through eye movements, but through coordinated redirection of the eyes, head, and body. Intentional, coordinated movements of the eyes, head, and body toward a target are known as \emph{directed gaze shifts}.
In particular, coordinated eye gaze and body orientation shifts play important roles in spatial and social organization of multiparty interactions, where they serve as nonverbal signals of participants' conversational roles or \emph{footing}~\citep{goffman1979footing}. For example, when a newcomer wants to join the interaction, the current participants may reorient their bodies to involve them on equal footing, or they may continue facing each other and relegate the newcomer to a bystander role~\citep{kendon1990conducting}.

It is well-known to professional animators that animated characters with appropriately crafted gaze behaviors can achieve communicative effects comparable to real humans. In the pioneering years of animation, an oft-repeated rule of thumb at Walt Disney Animation Studios was: ``If you're short of time, spend it on the eyes. The eyes are what people watch.''~\citep{williams2009animator} Animators expend much effort into animating believable gaze movements that capture the audience's attention and direct it toward the most important thing on the screen.
However, in the context of computer animation research and virtual agent interaction, gaze is understudied and undersupported. There is a lack of specialized methods for synthesis and authoring of directed gaze behaviors, let alone those that support coordinated head and body movements.

Gaze synthesis for characters in interactive scenarios (game characters, avatars, and virtual agents) is performed by computational gaze models, which automatically synthesize the character's gaze behavior given the current scenario state.
These models are necessarily context-specific. People display different gaze patterns when engaging in face-to-face conversation (conversational gaze), visually scanning their environment (idle gaze), engaging in visually-guided tasks (e.g., locomotion, physical object manipulation), etc. Therefore, specialized gaze models must be built for specific roles.
However, irrespective of context, the character's gaze patterns are built from the same primitive---the directed gaze shift.

Communicative effectiveness of gaze behaviors stems from the kinematic variability of directed gaze shifts that comprise them. Gaze shifts can be saccades (which only engage the eyes), sideward glances (which involve minimal head movement), head-on gaze shifts, upper-body shifts (which engage the torso), and whole-body orientation shifts. Different types of gaze shifts communicate different things to observers~\citep{langton2000eyes}. In general, a directed gaze shift signals an attentional shift toward a target, while the amount of head and body reorientation signals how the character's attention is distributed among the previous target and the new one.
However, existing gaze synthesis methods cannot synthesize the full range of directed gaze shifts. Most methods support only saccades and coordinated eye-head movements, but human attending behaviors can also involve changes in body orientation. Moreover, such movements are an integral part of certain communicative behaviors; as a result, these behaviors and their effects on interaction with avatars and virtual agents have received little or no study.

In non-interactive contexts (e.g., television and film animation, game cutscenes), gaze behaviors are also typically comprised of directed gaze shifts, with other types of gaze movements being far less common. However, animation quality requirements are typically higher than in interactive scenarios, so gaze is authored by hand using general methods such as inverse kinematics (IK) and keyframing. Gaze IK methods constrain the eyes to point in a specific direction even as the body moves, while keyframing specifies how the gaze direction changes over time. Body posture---integral to directed gaze behaviors---needs to be edited separately using keyframing and layering. Creating biologically plausible and communicatively effective gaze is difficult and labor-intensive; animators must get the timing and spatial relationships of all the eye, head, and body joints just right.

Hand-authoring of gaze animation thus imposes a high \emph{cost} in terms of expertise and effort required. This cost is further compounded by poor \emph{scalability} of hand-authoring with respect to scenario complexity, scene changes, and character design. I regard an authoring method as scalable if an acceptable-quality animation can be created reasonably quickly regardless of scenario complexity (number of characters and scenario length). However, hand-authoring time can generally be expected to increase at least linearly with the amount of content in the scenario. Furthermore, hand-authored gaze is valid only for the current character and scene. If we want to apply the same gaze behavior to a character with a different design (e.g., different eye size or spacing) or change the scene layout (thus altering gaze target locations), we must hand-edit the gaze animation or recreate it from scratch. Therefore, hand-authoring also scales poorly with respect to character design and scene changes.
%One of the objectives of this dissertation is to introduce a more scalable, lower-cost approach to gaze animation authoring.

\textbf{The thesis of this dissertation is that directed gaze shifts can serve as primitives for synthesis and low-cost authoring of effective attending behaviors on animated characters.} This dissertation introduces new methods for synthesis of directed gaze shifts as building blocks of larger attending behaviors (Chapter~\ref{cha:GazeShiftModel}). The communicative effectiveness of synthesized gaze shifts lies in their kinematic variability---a subtle glance toward a target can be synthesized as easily as a whole-body turn (Figure~\ref{fig:Overview}, 1). Furthermore, since the methods are based on measurements of human gaze kinematics, they synthesize biologically plausible movements across the entire parametric range.
Empirical studies demonstrate how effective attending behaviors on virtual agents can be constructed from directed gaze shifts synthesized by our methods. We show how the broader kinematic range (1) augments an agent's ability to signal its attentional focus and initiate joint attention with a human (Sections~\ref{sec:GazeShiftModelEval1} and~\ref{sec:GazeShiftModelEval2}) and (2) enables an agent to configure the conversational formation and establish footing using its gaze and spatial orientation cues (Chapter~\ref{cha:GazeFooting}; Figure~\ref{fig:Overview}, 2).

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{figures/Overview.pdf}
\caption{This dissertation in pictures: (1) We introduce a model for synthesis of a wide range of directed gaze movements (Chapter~\ref{cha:GazeShiftModel}). (2) We use it to synthesize gaze cues of a virtual agent engaging in multiparty interaction (Chapter~\ref{cha:GazeFooting}). (3) We introduce methods for adapting gaze movements to a wide range of character designs (Chapter~\ref{cha:StylizedGaze}). (4) We introduce an approach for authoring directed gaze animation in non-interactive scenarios (Chapter~\ref{cha:GazeAuthoring}).}
\label{fig:Overview}
\end{figure}

Directed gaze as a model of attending behavior is also advantageous for animation authoring.
First, it scales well across characters; any character with eyes, a head, and a body can engage in directed gaze shifts, while morphological and stylistic differences can be accounted for using principled motion adaptation methods introduced in Chapter~\ref{cha:StylizedGaze} (Figure~\ref{fig:Overview}, 3). Gaze animation can thus be applied to characters with different designs without the need for costly, manual editing.

Second, the use of directed gaze shifts as authoring primitives improves the scalability of authoring with respect to scenario complexity and scene changes, which we exploit in a novel authoring approach (Chapter~\ref{cha:GazeAuthoring}; Figure~\ref{fig:Overview}, 4). The authoring process begins with a body motion, created using either motion capture or keyframing. Since directed gaze shifts are characterized by tightly coordinated head and body movements, we can analyze the body motion and infer the directed gaze sequence that matches the character's motion. From this sequence, we can automatically synthesize the character's eye animation. The effort of adding eye gaze to an animated scenario is thus potentially constant with respect to scenario complexity.
The inferred representation also lends itself to convenient animation editing. It is abstract, allowing the animator to describe the gaze behavior in terms of targets to be looked at, while the kinematics of eye, head, and body movements are synthesized automatically.
It is also uniform; the behavior is comprised entirely of gaze shifts, so its content can be edited with simple operations such as adding and removing gaze shifts.
Automatic inference and convenient editing features significantly reduce the cost of animation authoring.
Moreover, the high level of abstraction afforded by the directed gaze representation makes our approach more scalable with respect to scene changes. As gaze targets move, our methods automatically synthesize the updated gaze motion, without requiring the animator to edit the gaze behavior content.

\section{Methodology}

The research presented in this dissertation was conducted using two main methods: model development and empirical evaluation.

This dissertation introduces computational models of gaze motion, which include a gaze shift synthesis model, methods for adapting gaze shift motion to stylized and non-human character designs, methods for gaze behavior inference from motion capture data, methods for layering gaze motion onto a captured body motion, and a nonverbal behavior model for signalling conversational footing. \emph{Model development} was largely guided by existing literature on human gaze in neurophysiology and social sciences, as well as artistic animation principles and traditions. The findings from literature were integrated into computational models as kinematic equations, heuristic decision-making rules, and probabilistic models built from human behavior data. The constructed models are typically parametric---they afford a set of parameters, which animators or implementers can manipulate to control the properties of resulting gaze motion.

Empirical evaluations predominantly took the form of studies with human participants, although computational simulations and comparisons with ground-truth, hand-annotated human behavior data were also utilized when appropriate. The studies had a two-fold purpose of demonstrating that our constructed models of gaze motion possessed desired perceptual and communicative properties, as well as garnering more general insights about the effects animated gaze behaviors had on people viewing and interacting with the character. The studies were designed to test one or more hypotheses about the effects of synthesized gaze behaviors on human participants viewing or interacting with the virtual character. They were conducted either in-person in a controlled laboratory setting, or online using the Amazon Mechanical Turk crowd-sourcing service. They involved a variety of measures: objective (e.g., participants' accuracy in predicting the character's gaze direction), behavioral (e.g., how much participants spoke with the character), and subjective (e.g., how positively participants rated the character.) The collected data were analyzed using appropriate statistical tests, such as ANOVA and chi-square.

Both the models and evaluations were implemented in a custom character animation and behavior toolkit built using the Unity game engine\footnote{http://unity3d.com/}. All animation, behavior, and task logic was implemented as Unity C\# scripts. These included our gaze shift model, an eyeblink controller, several high-level gaze behavior models (such as our model for signalling footing), task logic for all the human subjects' experiments, etc. All the preprocessing on character models and animations, as well as all the animation editing tools and operations were implemented as Unity Editor scripts. Character models were exported from 3D modeling applications such as Autodesk 3ds Max\footnote{http://www.autodesk.com/3dsMax} and DAZ Studio\footnote{http://www.daz3d.com/} as FBX assets, upon which they were imported into Unity and preprocessed in preparation for animation using our toolkit. When utilized, recorded motion data was either obtained from publicly available datasets such as the CMU database\footnote{http://mocap.cs.cmu.edu/} or captured at a local motion capture studio. The captured data was processed using Autodesk MotionBuilder\footnote{http://www.autodesk.com/motionbuilder} and then exported into Unity. Some studies featured virtual agent speech, which was either prerecorded (Chapters~\ref{cha:GazeShiftModel} and~\ref{cha:GazeFooting}) or synthesized using Microsoft Speech SDK (Chapter~\ref{cha:GazeFooting}). One exception to the above is an early version of the gaze shift model utilized in Section~\ref{sec:GazeShiftModelEval1}, which was implemented in C++ using the OGRE engine\footnote{http://www.ogre3d.org/} and subsequently ported to Unity.

\section{Contributions}

%This research contributes methods for synthesis and editing of gaze motion based on the representation as a sequence of directed gaze shifts.
%It also contributes demonstrations---in studies with human participants---of the effectiveness of directed gaze shifts as building blocks of behavioral mechanisms for attention signalling.
%The main contributions are the following:
Contributions from the current research can be split into three categories: technical contributions, new interaction designs, and empirical contributions.

\subsection{Technical Contributions}

This dissertation contributes several new methods for analysis, synthesis, adaptation, and editing of gaze animation. These methods constitute the technical contributions of the current work:

\begin{enumerate}
\item \textbf{Gaze shift synthesis} (Chapter~\ref{cha:GazeShiftModel}). A new model is introduced for synthesis of individual directed gaze shifts. The model can synthesize coordinated movements of the eyes, head, torso, pelvis, and feet toward targets in the environment. The model is fully procedural and does not rely on motion capture data. Instead, it synthesizes biologically plausible gaze movements using a set of kinematic laws derived from neurophysiological studies of gaze.
\item \textbf{Stylized gaze} (Chapter~\ref{cha:StylizedGaze}). Stylized and non-human characters, such as cartoon characters with large or asymmetric eyes, cannot be animated using human gaze kinematics due to anatomic differences. We introduce a set of methods for automatic adaptation of gaze movements to characters with varied eye and head designs, thereby enhancing the scalability of gaze synthesis methods with respect to character design. We collectively refer to these methods as \emph{stylized gaze}.
\item \textbf{Performative gaze} (Chapter~\ref{cha:StylizedGaze}). When stage actors and animated characters in film and television use directed gaze, they often do not fully align their line of sight with the target and instead maintain partial facing toward the viewer. The viewer is thus able to better see what the character is attending to, as well as feel more engaged by the character. We refer to such partial gaze shifts as \emph{performative gaze}. To extend the communicative range of gaze shift models, we introduce a motion adaptation method that can automatically adapt the target direction of a gaze shift relative to the camera such that partial alignment with the viewer is maintained.
\item \textbf{Gaze inference from body motion} (Chapter~\ref{cha:GazeAuthoring}). Our approach for gaze animation authoring incorporates methods for automated inference of the character's directed gaze sequence from their body motion and scene layout. Our approach can then automatically synthesize the eye animation that matches the body motion and scene layout from the inferred sequence, or allow the animator to conveniently edit the gaze animation by editing the sequence.
\item \textbf{Gaze motion layering} (Chapter~\ref{cha:GazeAuthoring}). Another component of the gaze authoring approach are methods for layering the movements synthesized by our gaze shift model (Chapter~\ref{cha:GazeShiftModel}) onto the character's body motion. The methods ensure that the resulting gaze motion has plausible kinematics and does not override the expressive content of the original body motion.
\end{enumerate}

\subsection{Design Contributions}

This dissertation also introduces several design contributions. These designs include gaze behavior models for virtual agent systems. Designers can implement these models on virtual agents to enable the latter to interact more naturally with users. The models include:

\begin{enumerate}
\item \textbf{Parametric gaze shift model} (Chapter~\ref{cha:GazeShiftModel}). Our gaze shift synthesis model provides a parametrization of gaze movements based on a set of alignment parameters. These parameters specify how much different body parts (head, torso, lower body) partake in the gaze shift, allowing the synthesis of movements ranging from subtle, saccadic glances to whole-body turns. Greater reorientation of a body part signals a greater shift in attention toward the target. The parametric model serves as a building block of complex attending behaviors.
\item \textbf{Nonverbal signals of conversational footing} (Chapter~\ref{cha:GazeFooting}). When two or more people engage in conversation, their degree of participation is tied to their \emph{footing}---speakers and addressees contribute more to the discourse than bystanders and overhearers. Participants' spatial orientations and gaze patterns serve as nonverbal signals of footing---speakers and addressees face each other and gaze at each other more than bystanders and overhearers. We describe spatial orientation and gaze mechanisms for embodied conversational agents enabling them to signal the footing of users engaging in multiparty interaction with the agent. These mechanisms are designed to allow an agent to shape users' conversational behaviors and subjective experiences of the interaction.
\end{enumerate}

The dissertation also introduces designs intended to assist animators in authoring gaze animation:

\begin{enumerate}
\item \textbf{Gaze authoring workflow} (Chapter~\ref{cha:GazeAuthoring}). We have designed a workflow for gaze animation authoring that is based on an abstract representation of the gaze behavior as a sequence of directed gaze shifts. The workflow begins with automatic inference of the gaze sequence from the character's body motion and scene, followed by convenient manual editing of the sequence using a graphical tool. The gaze motion is continually resynthesized from the sequence representation and layered onto the body motion.
\item \textbf{Gaze editing approach} (Chapter~\ref{cha:GazeAuthoring}). A key component of our gaze authoring approach is an approach for gaze motion editing. We have define a set of convenient editing operations such as adding and removing gaze shifts, as well as modifying their target and timing properties. The edited gaze motion is automatically synthesized and layered onto the character's body motion. The editing operations are implemented as controls in a graphical animation editing tool.
\end{enumerate}

\subsection{Empirical Contributions}

The animation methods and designs proposed in this dissertation were extensively evaluated in empirical studies. Gaze shift synthesis methods were evaluated with respect to the biological plausibility and communicative effectiveness of resulting gaze shifts, as well as scalability across different characters. Gaze behavior mechanisms for signalling footing were evaluated on their communicative effectiveness. The gaze authoring approach was evaluated with respect to its scalability across different scenarios, reduction in authoring cost (effort and expertise required), and the ability to produce plausible and communicatively effective gaze animation. The evaluations took the form of empirical studies, the findings from which constitute distinct contributions:

\begin{enumerate}
\item \textbf{Gaze shift model synthesizes plausible, communicatively effective gaze shifts} (Chapter~\ref{cha:GazeShiftModel}). The first study in Chapter~\ref{cha:GazeShiftModel} has shown that participants are able to infer the targets of gaze shifts synthesized by our neurophysiology-based model as accurately as those from a state-of-the-art, procedural model or real human gaze. They are also rated as natural as those from the state-of-the-art model.
\item \textbf{Gaze shifts with more torso reorientation are stronger attention cues} (Chapter~\ref{cha:GazeShiftModel}). The second study in Chapter~\ref{cha:GazeShiftModel} has shown that the more a virtual agent reorients its torso toward a gaze shift target, the more interested they are perceived to be in that target.
\item \textbf{Virtual agents can use gaze and spatial orientation to manage footing} (Chapter~\ref{cha:GazeFooting}). A study has shown that participants in a multiparty interaction conform to conversational roles signalled by a virtual agent's gaze and spatial orientation; addressees make more conversational contributions than bystanders.
\item \textbf{Nonverbal footing signals are effective only in immersive VR} (Chapter~\ref{cha:GazeFooting}). The same study has shown the effects of the virtual agent's footing signals on participants' conversational behavior occur only when the interaction is in immersive VR rather than on a 2D display.
\item \textbf{Gaze adaptation to stylized characters reduces animation artifacts} (Chapter~\ref{cha:StylizedGaze}). A simulation has shown that motion adaptations of gaze shifts synthesized by our model and applied to stylized characters reduce the quantitative prevalence of gaze animation artifacts on such characters.
\item \textbf{Stylized and performative gaze adaptations do not impair communicative accuracy or plausibility} (Chapter~\ref{cha:StylizedGaze}). A study has shown that when stylized and performative adaptations are applied to gaze shifts, participants are able to infer gaze shift targets as accurately as with the original gaze shift kinematics, and their perceived naturalness is also comparable.
\item \textbf{Human actor's gaze can be reconstructed from their body motion and scene layout} (Chapter~\ref{cha:GazeAuthoring}). Our gaze inference model detects gaze shifts and infers their targets from body motion kinematics and scene layout, in order to allow the synthesis of plausible eye movements, as well as editing of the gaze behavior. An evaluation using ground-truth, hand-annotated motion capture and eye tracking data has shown that the model is able to perform the inference with sufficient sensitivity and accuracy to obtain a representation of the original gaze behavior suitable for synthesis and editing purposes.
\item \textbf{Gaze authoring approach allows novices to create and edit gaze animation with less effort} (Chapter~\ref{cha:GazeAuthoring}). An evaluation has shown that a novice animator can add eye movements to an animated scenario in substantially less time and with fewer operations using our approach than experienced animators working by hand in a traditional workflow. Likewise, a novice animator can edit a character's gaze behavior in the scenario in substantially less time and with fewer operations using our tool than an experienced animator employing a traditional tool.
\item \textbf{Animation obtained from the gaze authoring approach looks plausible} (Chapter~\ref{cha:GazeAuthoring}). A study with human participants has shown that animation created using our approach is perceived as more plausible than having no eye movements, and its plausibility may also be superior to animation with recorded eye movements (using eye tracker cameras) and comparable to animation with hand-authored eye movements.
\item \textbf{Edited gaze animation communicates a different attention distribution} (Chapter~\ref{cha:GazeAuthoring}). Another study has shown that when a character's gaze animation is edited using our approach, participants describe the character's attention distribution significantly differently compared to the original scenario.
\end{enumerate}


\section{Summary}

It is the goal of this dissertation to improve computational animation of attending behaviors for virtual characters, by increasing the variability of behaviors, characters, and scenarios supported, as well as lowering the cost of creating the animation at an acceptable level of quality. To facilitate those advances, attending behaviors are modeled as a sequence of directed gaze shifts---coordinated redirections of the eyes, head, and body among targets in the environment. Four research threads have been undertaken: (1) building a computational, parametric model for synthesis of gaze shift movements (Chapter~\ref{cha:GazeShiftModel}); (2) using that model as a building block of virtual agents' nonverbal behaviors for managing conversational footing in multiparty interactions with users (Chapter~\ref{cha:GazeFooting}); (3) introducing methods for adapting gaze shift movements to non-realistic and non-human character designs (Chapter~\ref{cha:StylizedGaze}); (4) introducing a comprehensive approach for low-cost, scalable authoring of directed gaze animation (Chapter~\ref{cha:GazeAuthoring}).
