\chapterstyle{deposit}
\pagestyle{deposit}

\chapter{Introduction}

Character animation---the process of bringing drawn, sculpted, or computer-generated characters to life---forms the backbone of some of the most popular contemporary art forms, such as animated films and video games. With their proven ability to engage and entertain people irrespective of age and culture, animated characters hold potential that extends beyond storytelling media: the last two decades have witnessed a growing research and commercial interest in embodied conversational agents~\citep{cassell2000embodied}, animated characters that can converse with users in real time and serve in educational, marketing, and socially assistive roles. Across all these application domains, one maxim pertaining to character animation holds true: to captivate and engage the viewer, animated characters need to be \emph{believable}. Their visible behaviors must be comprised of physically and biologically plausible movements, and they must clearly convey the characters' intentions, emotions, and personalities. Believable character animation is synonymous with plausible, communicatively effective behavior, and a key component of such behavior is \emph{gaze}. This body of research contributes several new techniques for creating communicatively effective gaze behaviors for computer-generated, animated characters.

The importance of gaze in human communication is well-documented. By observing the gaze of others, people can infer the focus of their attention and understand high-level meanings behind their actions. Human attending behaviors play vital roles in many aspects of human communication, from grounding references to objects by looking at them~\citep{hanna2007speakers,preissler2005role}, to regulating various conversational processes~\citep{kendon1990conducting,heylen2006head,mutlu2012conversational}. %A person can look at objects and information in the environment and the viewer's attention will be automatically redirected toward the same objects through the mechanism of joint attention~\citep{moore2014joint}.
Importantly, attending behaviors consist of more than just eye movements---people convey their attention through intentional, coordinated shifts in eye direction, head, and body orientation toward targets in the environment~\citep{langton2000eyes}, which I refer to as \emph{directed gaze}.
Effective directed gaze is integral to many processes in human communication. Coordinated shifts in gaze and body orientation are how people signal the spatial distribution of their attention in multiparty interactions and through strategic use of these behaviors they can effect changes in the spatial and social organization of multiparty interactions. Two notable functions of directed gaze are reconfiguration of the conversational formation and establishment of footing~\citep{kendon1990conducting,mutlu2012conversational}---participants in a multiparty interaction can use specific gaze patterns and body reorientation to involve a newcomer as an equal partner, or relegate them to a bystander role.

Such behavioral mechanisms are commonplace in human interaction, yet understudied and undersupported in the context of character animation and virtual agent interaction. Yet it is well-known to professional animators that animated characters with appropriately crafted gaze behaviors can achieve communicative effects comparable to real humans. In the pioneering years of animation, an oft-repeated rule of thumb at Walt Disney Animation Studios was: ``If you're short of time, spend it on the eyes. The eyes are what people watch.''~\citep{thomas1981illusion} Animators expend much effort into animating believable gaze movements that capture the audience's attention and direct it toward the most important thing on the screen.

In 3D animation, gaze is typically hand-animated using inverse kinematics and keyframing. A gaze IK rig constrains the eyes to point in a specific direction, while keyframing specifies how this direction changes over time. To create believable gaze, animators must get the timing and spatial relationships of gaze movements right, which requires great expertise and effort. For this reason, manual authoring is costly. Worse yet, it scales poorly: hand-animated gaze is valid only for the current character and scene. If we want to apply the same gaze behavior to a character with a different design (e.g., different eye size or spacing), or change the scene layout (thus altering gaze target locations), we must hand-edit the animation or recreate it from scratch. The scalability issue is even more apparent in interactive scenarios, where the gaze needs to dynamically adapt to scenario events and inputs: e.g., a virtual agent needs to be able to look at the user as they move around.

Automated synthesis is a viable approach to animating gaze when interactive control is desired, but current methods have several limitations. First, they generally use automated, procedural or data-driven models of human gaze to synthesize gaze movements in specific scenarios, such as face-to-face conversations or crowd animation. This makes them unsuitable for animation authoring purposes, which also require scalability across different scenarios and character designs, as well as the ability to edit the resulting behavior. Furthermore, they tend to focus on specific types of gaze movements: saccades, head movements, or smooth pursuit. There is a distinct lack of methods for synthesis of controllable, coordinated gaze and body orientation shifts, which serve as building blocks of important regulatory behaviors for virtual agents in multiparty interaction. Yet such behaviors are growing in importance with the advent of high-fidelity, immersive virtual and mixed reality devices for consumers, which provide the sensing and display capabilities needed to support truly believable social experiences involving virtual agents. Finally, there is often a lack of empirical evidence for the effectiveness of various gaze synthesis methods, both in terms of communicative capability and authoring benefits. This potentially inhibits their adoption---for example, virtual agent implementers have little motivation to explore sophisticated gaze mechanisms if they offer no quantifiable benefit.
%Traditionally, interaction with a virtual agent has taken place on a 2D screen, often with only their face or bust shown. In such a context, behaviors associated with spatial organization of the interaction, such as body orientation, are fairly inconsequential. However, growing use of high-fidelity, virtual and mixed reality technologies is endowing virtual agents with an unprecedented degree of presence, and they will need to utilize the full spectrum of spatial behaviors to engage effectively with users.

\textbf{The thesis of this dissertation is that directed gaze is an effective and scalable model for synthesis and authoring of attending behaviors on animated characters.} The idea of directed gaze is to represent the gaze behavior as a sequence of gaze shifts---coordinated eye, head, and body movements toward targets in the environment. These targets can be objects, information, other characters, or randomly chosen locations. Many types of gaze movements supported by prior gaze synthesis methods can be subsumed under directed gaze, e.g., saccades, gaze aversions, coordinated eye-head shifts. However, directed gaze also involves other types of movements, such as coordinated shifts in torso and whole-body orientation. As a result, directed gaze is a general model of gaze motion that scales across many scenarios, such as two-party and multiparty conversations, environment interactions, locomotion, and idle gaze. It also scales well across characters: any character with eyes, head, and body can engage in directed gaze shifts, and morphological and stylistic differences can be accounted for using principled motion adaptation techniques. Finally, directed gaze representation lends itself to convenient animation editing. It is sufficiently abstract that novice animators can specify the content of the gaze behavior without bothering with the details of timing and spatial relationships, and it is uniform, so content can be edited with a few simple operations, such as adding and removing gaze shifts.

This research contributes techniques for synthesis and editing of gaze motion based on the representation as a sequence of directed gaze shifts. It also contributes demonstrations---in studies with human participants---of the effectiveness of directed gaze shifts as building blocks of behavioral mechanisms for attention signalling. The main contributions of this work are the following:

\begin{enumerate}
\item \textbf{Gaze shift synthesis} (Chapter~\ref{cha:GazeShiftModel}). I propose a model for synthesis of individual directed gaze shifts that affords intuitive parametric control over coordinated eye, head, and body movements and abstracts away details of timing and spatial relationships. The model serves as a fundamental building block of more complex gaze behaviors. It is fully procedural and does not rely on motion capture data; instead, it synthesizes biologically plausible gaze movements using a set of kinematic laws derived from neurophysiological studies of gaze.
\item \textbf{Gaze and footing in multiparty conversations with a virtual agent} (Chapter~\ref{cha:GazeFooting}). I demonstrate in a study with human participants how a virtual agent can use shifts in gaze direction and body orientation to regulate footing in multiparty interactions, and thus manage the participants' conversational behavior. The study provides evidence for the effectiveness of directed gaze as a component of conversational behaviors of interactive characters in the context of high-fidelity, immersive VR.
\item \textbf{Stylized and performative gaze} (Chapter~\ref{cha:StylizedGaze}). Stylized and non-human characters, such as cartoon characters with large or asymmetric eyes, cannot be animated using human gaze kinematics due to anatomic differences. I introduce a set of techniques for automatic adaptation of gaze movements to varied character designs, and I empirically demonstrate that gaze shift kinematics can be adapted based on character morphology and camera position without affecting their communicative function.
\item \textbf{Gaze animation authoring} (Chapter~\ref{cha:GazeAuthoring}). I propose an approach for adding editable gaze animation to characters in a wide range of scenarios. Given inputs consisting of a character animated using motion capture and a 3D model of the virtual scene, the proposed approach will automatically synthesize a directed gaze behavior that matches these inputs. The approach also includes methods and a tool for convenient editing of the gaze behavior. I extensively evaluate the effectiveness of the approach in reducing authoring effort and producing plausible, communicatively effective gaze.
\end{enumerate}

The methods proposed in this research have applications in both interactive and non-interactive contexts. The gaze shift synthesis model and adaptation techniques can be used to construct better gaze mechanisms for embodied conversational agents, game characters, and avatars---one such mechanism is demonstrated in the study of gaze for footing regulation (Chapter~\ref{cha:GazeFooting}). The authoring approach is designed for non-interactive contexts, such as game cutscenes, television, and film animation. It can automatically add plausible gaze animation to a given scenario and it also enables convenient editing of the result. As such, it can be used as a primary authoring tool for novice animators or a labor-saving tool for the more experienced ones. It could be particularly useful when a large amount of gaze animation needs to be produced under strict time constraints, such as animating dozens of midground or background characters in a film, or animating for television.
